# Research Papers: medical multimodal generation unified

Updated: 2026-01-02 19:07
Total: 380 papers

---

## 1. BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays

**Authors:** Yang Zhou, Tan Li Hui Faith, Yanyu Xu, Sicong Leng, Xinxing Xu

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=iMtAjdGh1U) | > Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and...

---

## 2. MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling

**Authors:** Xuzhe Zhang, Yuhao Wu, Elsa Angelini, Ang Li, Jia Guo

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MAPSeg_Unified_Unsupervised_Domain_Adaptation_for_Heterogeneous_Medical_Image_Segmentation_CVPR_2024_paper.pdf) | > Robust segmentation is critical for deriving quantitative measures from large-scale multi-center and longitudinal medical scans. Manually annotating medical scans however is expensive and labor-intensive and may not always be available in every domain. Unsupervised domain adaptation (UDA) is a well-studied technique that alleviates this label-scarcity problem by leveraging available labels from an...

---

## 3. A Unified and Interpretable Emotion Representation and Expression Generation

**Authors:** Reni Paskaleva, Mykyta Holubakha, Andela Ilic, Saman Motamed, Luc Van Gool

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Paskaleva_A_Unified_and_Interpretable_Emotion_Representation_and_Expression_Generation_CVPR_2024_paper.pdf) | > Canonical emotions such as happy sad and fear are easy to understand and annotate. However emotions are often compound e.g. happily surprised and can be mapped to the action units (AUs) used for expressing emotions and trivially to the canonical ones. Intuitively emotions are continuous as represented by the arousal-valence (AV) model. An interpretable unification of these four modalities --namely...

---

## 4. CodeT5Mix: A Pretrained Mixture of Encoder-decoder Transformers for Code Understanding and Generation

**Authors:** Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Junnan Li, Steven Hoi

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.000

> Pretrained language models (LMs) trained on vast source code have achieved prominent progress in a wide range of code intelligence tasks. Despite their success, they either adopt specific types of network architectures (encoder-only or decoder-only) for different downstream tasks or rely on a single architecture (encoder-decoder or UniLM-style encoder) for all tasks. The latter approach usually re...

---

## 5. UniSeMi: Toward Unified Semi-supervised Medical Image Segmentation

**Authors:** Qingjie Zeng, Yutong Xie, Zilin LU, Mengkang Lu, Yong Xia

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.000

> Semi-supervised learning (SSL) for medical image segmentation is put forward to mitigate the scarcity of annotation by leveraging unlabeled data. Recently proposed SSL works focus on designing task-specific models that process different tasks separately. This results in marginal improvement due to inadequate supervision from scarce labels of each single task.
To address this, we advocate learning ...

---

## 6. MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey

**Authors:** Xian Wu, Yutian Zhao, Yunyan Zhang, Jiageng Wu, Zhihong Zhu

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=XXaIoJyYs7) | > Large language models (LLMs) have demonstrated remarkable capabilities in language understanding and generation, leading to their widespread adoption across various fields. Among these, the medical field is particularly well-suited for LLM applications, as many medical tasks can be enhanced by LLMs. Despite the existence of benchmarks for evaluating LLMs in medical question-answering and exams, th...

---

## 7. HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding

**Authors:** Trong-Thuan Nguyen, Pha Nguyen, Khoa Luu

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_HIG_Hierarchical_Interlacement_Graph_Approach_to_Scene_Graph_Generation_in_CVPR_2024_paper.pdf) | > Visual interactivity understanding within visual scenes presents a significant challenge in computer vision. Existing methods focus on complex interactivities while leveraging a simple relationship model. These methods however struggle with a diversity of appearance situation position interaction and relation in videos. This limitation hinders the ability to fully comprehend the interplay within t...

---

## 8. Unified Generative and Discriminative Training for Multi-modal Large Language Models

**Authors:** Wei Chow, Juncheng Li, Qifan Yu, Kaihang Pan, Hao Fei

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=w67vRHZF13) | > In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet...

---

## 9. Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation

**Authors:** Qinghe Ma, Jian Zhang, Lei Qi, Qian Yu, Yinghuan Shi

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Constructing_and_Exploring_Intermediate_Domains_in_Mixed_Domain_Semi-supervised_Medical_CVPR_2024_paper.pdf) | > Both limited annotation and domain shift are prevalent challenges in medical image segmentation. Traditional semi-supervised segmentation and unsupervised domain adaptation methods address one of these issues separately. However the coexistence of limited annotation and domain shift is quite common which motivates us to introduce a novel and challenging scenario: Mixed Domain Semi-supervised medic...

---

## 10. MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making

**Authors:** Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=EKdk4vxKO4) | > Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named **M**edical **D**ecision-making **Agents** (**MDAgents**) that helps to address this gap by automatically assigning a collaboration structure to a team of LLMs...

---

## 11. MedSat: A Public Health Dataset for England Featuring Medical Prescriptions and Satellite Imagery

**Authors:** Sanja Scepanovic, Ivica Obadic, Sagar Joglekar, Laura GIUSTARINI, Cristiano Nattero

**Year:** 2023 | **Venue:** NIPS 2023 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=CSJYz1Zovj) | > As extreme weather events become more frequent, understanding their impact on human health becomes increasingly crucial. However, the utilization of Earth Observation to effectively analyze the environmental context in relation to health remains limited. This limitation is primarily due to the lack of fine-grained spatial and temporal data in public and population health studies, hindering a compr...

---

## 12. Meta-Learning for Bootstrapping Medical Image Segmentation from Imperfect Supervision

**Authors:** Qingyue Wei, Lequan Yu, Xianhang Li, Wei Shao, Cihang Xie

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.000

> Medical imaging has witnessed remarkable progress but usually requires a large amount of high-quality annotated data which is time-consuming and costly to obtain. To alleviate the annotation burden, learning from imperfect supervision (scarce or noisy annotations) has received much attention recently. In this paper, we present Meta-Learning for Bootstrapping Medical Image Segmentation (MLB-Seg), a...

---

## 13. CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor Segmentation

**Authors:** Zhongzhen Huang, Yankai Jiang, Rongzhao Zhang, Shaoting Zhang, Xiaofan Zhang

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=pnmUiVAGnv) | > Existing promptable segmentation methods in the medical imaging field primarily consider either textual or visual prompts to segment relevant objects, yet they often fall short when addressing anomalies in medical images, like tumors, which may vary greatly in shape, size, and appearance. Recognizing the complexity of medical scenarios and the limitations of textual or visual prompts, we propose a...

---

## 14. HarmonyLM: Advancing Unified Large-Scale Language Modeling for Sound and Music Generation

**Authors:** Huadai Liu, Yongqi Wang, Ruofan Hu, Ruiqi Li, Bai Jionghao

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.000

> The fields of sound generation and music generation have seen notable advancements with the development of specialized models tailored to each domain. However, these domains share commonalities, and the use of specialized models can lead to increased hardware resource requirements. On the other hand, recent breakthroughs in large language models, particularly in natural language processing, have s...

---

## 15. Benchmarking Counterfactual Image Generation

**Authors:** Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=0T8xRFrScB) | > Generative AI has revolutionised visual content editing, empowering users to effortlessly modify images and videos. However, not all edits are equal. To perform realistic edits in domains such as natural image or medical imaging, modifications must respect causal relationships inherent to the data generation process. Such image editing falls into the counterfactual image generation regime. Evaluat...

---

## 16. Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization

**Authors:** Zonghan Yang, Xiaoyuan Yi, Peng Li, Yang Liu, Xing Xie

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=FvevdI0aA_h) | > Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing s...

---

## 17. A Unified Framework for Consistency Generative Modeling

**Authors:** Hongkun Dou, Junzhe Lu, Jinyang Du, Chengwei Fu, Wen Yao

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.000

> Consistency modeling, a novel generative paradigm inspired by diffusion models, has gained traction for its capacity to facilitate real-time generation through single-step sampling. While its advantages are evident, the understanding of its underlying principles and effective algorithmic enhancements remain elusive. In response,  we present a unified framework for consistency generative modeling, ...

---

## 18. OmniParser: A Unified Framework for Text Spotting Key Information Extraction and Table Recognition

**Authors:** Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Wan_OmniParser_A_Unified_Framework_for_Text_Spotting_Key_Information_Extraction_CVPR_2024_paper.pdf) | > Recently visually-situated text parsing (VsTP) has experienced notable advancements driven by the increasing demand for automated document understanding and the emergence of Generative Large Language Models (LLMs) capable of processing document-based questions. Various methods have been proposed to address the challenging problem of VsTP. However due to the diversified targets and heterogeneous sc...

---

## 19. G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training

**Authors:** Che Liu, Cheng Ouyang, Sibo Cheng, Anand Shah, Wenjia Bai

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=zsXbGJJ7Oo) | > Medical imaging tasks require an understanding of subtle and localized visual features due to the inherently detailed and area-specific nature of pathological patterns, which are crucial for clinical diagnosis. Although recent advances in medical vision-language pre-training (VLP) enable models to learn clinically relevant visual features by leveraging both medical images and their associated radi...

---

## 20. Meta-Guided Diffusion Models for Zero-Shot Medical Imaging Inverse Problems

**Authors:** Hossein Askari, Fred Roosta, Hongfu Sun

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.000

> In the realm of medical imaging, inverse problems aim to infer high-quality images from incomplete, noisy measurements, with the objective of minimizing expenses and risks to patients in clinical settings. The Diffusion Models have recently emerged as a promising approach to such practical challenges, proving particularly useful for the zero-shot inference of images from partially acquired measure...

---

## 21. A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning

**Authors:** Yuelin Zhang, Pengyu Zheng, Wanquan Yan, Chengyu Fang, Shing Shin Cheng

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_A_Unified_Framework_for_Microscopy_Defocus_Deblur_with_Multi-Pyramid_Transformer_CVPR_2024_paper.pdf) | > Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem a unified framework including the multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer atte...

---

## 22. Understanding and Improving Adversarial Attacks on Latent Diffusion Model

**Authors:** Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.000

> Latent Diffusion Model (LDM) has emerged as a leading tool in image generation, particularly with its capability in few-shot generation. This capability also presents risks, notably in unauthorized artwork replication and misinformation generation. In response, adversarial attacks have been designed to safeguard personal images from being used as reference data. However, existing adversarial attac...

---

## 23. PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications

**Authors:** Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=WvoKwq12x5) | > Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures.
To address the...

---

## 24. KiUT: Knowledge-Injected U-Transformer for Radiology Report Generation

**Authors:** Zhongzhen Huang, Xiaofan Zhang, Shaoting Zhang

**Year:** 2023 | **Venue:** CVPR 2023 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.pdf) | > Radiology report generation aims to automatically generate a clinically accurate and coherent paragraph from the X-ray image, which could relieve radiologists from the heavy burden of report writing. Although various image caption methods have shown remarkable performance in the natural image field, generating accurate reports for medical images requires knowledge of multiple modalities, including...

---

## 25. Chat-UniVi: A Unified Vision-Language Model for Image and Video Understanding

**Authors:** Peng Jin, Ryuichi Takanobu, Cai Wan Zhang, Xiaochun Cao, Li Yuan

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.000

> Large language models have demonstrated impressive universal capabilities across a wide range of open-ended tasks and have extended their utility to encompass multimodal conversations. In this study, we introduce Chat-UniVi, a unified vision-language model capable of comprehending and engaging in conversations involving images and videos. Specifically, Chat-UniVi uniformly represents images and vi...

---

## 26. PTUnifier: Pseudo Tokens as Paradigm Unifiers in Medical Vision-and-Language Pre-training

**Authors:** Zhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, Xiang Wan

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.000

> Medical vision-and-language pre-training (Med-VLP) has shown promising improvements on many downstream medical tasks owing to its applicability to extracting generic representations from medical images and texts. Practically, there exist two typical paradigms, i.e., the \textbf{fusion-encoder paradigm} and the \textbf{dual-encoder paradigm}, depending on whether a heavy fusion module is used. The ...

---

## 27. Uni3D: Exploring Unified 3D Representation at Scale

**Authors:** Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=wcaE4Dfgt8) | > Scaling up representations for images or text has been extensively investigated in the past few years and has led to revolutions in learning vision and language. However, scalable representation for 3D objects and scenes is relatively unexplored. In this work, we present Uni3D, a 3D foundation model to explore the unified 3D representation at scale. Uni3D uses a 2D initialized ViT end-to-end pretr...

---

## 28. CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification

**Authors:** Haoran Lai, Qingsong Yao, Zihang Jiang, Rongsheng Wang, Zhiyang He

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Lai_CARZero_Cross-Attention_Alignment_for_Radiology_Zero-Shot_Classification_CVPR_2024_paper.pdf) | > The advancement of Zero-Shot Learning in the medical domain has been driven forward by using pre-trained models on large-scale image-text pairs focusing on image-text alignment. However existing methods primarily rely on cosine similarity for alignment which may not fully capture the complex relationship between medical images and reports. To address this gap we introduce a novel approach called C...

---

## 29. Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism

**Authors:** Zhijian Zhuo, Yifei Wang, Jinwen Ma, Yisen Wang

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.000

[PDF](https://openreview.net/pdf?id=cIbjyd2Vcy) | > Recently, a variety of methods under the name of non-contrastive learning (like BYOL, SimSiam, SwAV, DINO) show that when equipped with some asymmetric architectural designs, aligning positive pairs alone is sufficient to attain good performance in self-supervised visual learning. Despite some understandings of some specific modules (like the predictor in BYOL), there is yet no unified theoretical...

---

## 30. CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare

**Authors:** Akash Ghosh, Arkadeep Acharya, Raghav Jain, Sriparna Saha, Aman Chadha

**Year:** 2024 | **Venue:** AAAI 2024 | **Citations:** N/A | **Score:** 0.520

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/30206/32143) | > In the era of modern healthcare, swiftly generating medical question summaries is crucial for informed and timely patient care. Despite the increasing complexity and volume of medical data, existing studies have focused solely on text-based summarization, neglecting the integration of visual information. Recognizing the untapped potential of combining textual queries with visual representations of...

---

## 31. Prompt Tuning for Unified Multimodal Pretrained Models

**Authors:** Hao Yang, Junyang Lin, An Yang, Peng Wang, Chang Zhou

**Year:** 2023 | **Venue:** ACL 2023 | **Citations:** N/A | **Score:** 0.447

[PDF](https://aclanthology.org/2023.findings-acl.27.pdf) | > Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low computation costs and almost lossless performance. In this work, we explore the transfer of prompt tun...

---

## 32. UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion

**Authors:** Wei Li, Xue Xu, Jiachen Liu, Xinyan Xiao

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.461

[PDF](https://aclanthology.org/2024.acl-long.335.pdf) | > Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and...

---

## 33. MedJourney: Counterfactual Medical Image Generation by Instruction-Learning from Multimodal Patient Journeys

**Authors:** Yu Gu, Jianwei Yang, Naoto Usuyama, Chunyuan Li, Sheng Zhang

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.463

> Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such counterfactual generation methods can help differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the ...

---

## 34. Harmonizing Visual Text Comprehension and Generation

**Authors:** Zhen Zhao, Jingqun Tang, Binghong Wu, Chunhui Lin, Shu Wei

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.442

[PDF](https://openreview.net/pdf?id=fqjeKsHOVR) | > In this work, we present TextHarmony, a unified and versatile multimodal generative model proficient in comprehending and generating visual text. Simultaneously generating images and texts typically results in performance degradation due to the inherent inconsistency between vision and language modalities. To overcome this challenge, existing approaches resort to modality-specific data for supervi...

---

## 35. Biomedical Visual Instruction Tuning with Clinician Preference Alignment

**Authors:** Hejie Cui, Lingjun Mao, Xin LIANG, Jieyu Zhang, Hui Ren

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.443

[PDF](https://openreview.net/pdf?id=Eogs84mv7N) | > Recent advancements in multimodal foundation models have showcased impressive capabilities in understanding and reasoning with visual and textual information. Adapting these foundation models trained for general usage to specialized domains like biomedicine requires large-scale domain-specific instruction datasets. While existing works have explored curating such datasets automatically, the result...

---

## 36. PointMLLM: Aligning multi-modality with LLM for point cloud understanding, generation and editing

**Authors:** Dingning Liu, Xiaoshui Huang, Zhihui Wang, Zhenfei Yin, Peng Gao

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.435

> We introduce UniPoint-LLM, which integrates point cloud understanding ability
into Image Multimodal Large Language Model(MLLM) and enable more flexible
and controllable natural language-driven 3D generation, realized the unified pro-
cess of point cloud understanding and generation. Unlike traditional text-to-3D
methods with limited prompt inputs or constrained parameters, UniPoint-LLM al-
lows us...

---

## 37. Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale

**Authors:** Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.495

[PDF](https://aclanthology.org/2024.emnlp-main.418.pdf) | > The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed’s large-scale, de-i...

---

## 38. Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration

**Authors:** Kaihang Pan, Zhaoyu Fan, Juncheng Li, Qifan Yu, Hao Fei

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.434

[PDF](https://openreview.net/pdf?id=kf80ZS3fVy) | > The swift advancement in Multimodal LLMs (MLLMs) also presents significant challenges for effective knowledge editing. Current methods, including intrinsic knowledge editing and external knowledge resorting, each possess strengths and weaknesses, struggling to balance the desired properties of reliability, generality, and locality when applied to MLLMs. In this paper, we propose \textbf{UniKE}, a ...

---

## 39. Medical Dialogue Generation via Dual Flow Modeling

**Authors:** Kaishuai Xu, Wenjun Hou, Yi Cheng, Jian Wang, Wenjie Li

**Year:** 2023 | **Venue:** ACL 2023 | **Citations:** N/A | **Score:** 0.431

[PDF](https://aclanthology.org/2023.findings-acl.423.pdf) | > Medical dialogue systems (MDS) aim to provide patients with medical services, such as diagnosis and prescription. Since most patients cannot precisely describe their symptoms, dialogue understanding is challenging for MDS. Previous studies mainly addressed this by extracting the mentioned medical entities as critical dialogue history information. In this work, we argue that it is also essential to...

---

## 40. UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation

**Authors:** Xiangyu Zhao, Yuehan Zhang, Wenlong Zhang, Xiao-Ming Wu

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.449

[PDF](https://aclanthology.org/2024.emnlp-main.89.pdf) | > The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in...

---

## 41. UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model

**Authors:** Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan

**Year:** 2023 | **Venue:** EMNLP 2023 | **Citations:** N/A | **Score:** 0.407

> Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM). By leveraging the shallow text recognition ability of the MLLM, we only finetuned 1.2% parameters and...

---

## 42. Meta-Transformer: A Unified Framework for Multimodal Learning

**Authors:** Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.437

> Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we ...

---

## 43. Towards More Unified In-context Visual Understanding

**Authors:** Dianmo Sheng, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.425

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Sheng_Towards_More_Unified_In-context_Visual_Understanding_CVPR_2024_paper.pdf) | > The rapid advancement of large language models (LLMs) has accelerated the emergence of in-context learning (ICL) as a cutting-edge approach in the natural language processing domain. Recently ICL has been employed in visual understanding tasks such as semantic segmentation and image captioning yielding promising results. However existing visual ICL framework can not enable producing content across...

---

## 44. Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models

**Authors:** Songtao Jiang, Tuo Zheng, Yan Zhang, Yeying Jin, Li Yuan

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.464

[PDF](https://aclanthology.org/2024.findings-emnlp.221.pdf) | > Recent advancements in general-purpose or domain-specific multimodal large language models (LLMs) have witnessed remarkable progress for medical decision-making. However, they are designated for specific classification or generative tasks, and require model training or finetuning on large-scale datasets with sizeable parameters and tremendous computing, hindering their clinical utility across dive...

---

## 45. Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos

**Authors:** Dong Won Lee, Chaitanya Ahuja, Paul Pu Liang, Sanika Natu, Louis-Philippe Morency

**Year:** 2023 | **Venue:** ICCV 2023 | **Citations:** N/A | **Score:** 0.416

[PDF](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Lecture_Presentations_Multimodal_Dataset_Towards_Understanding_Multimodality_in_Educational_Videos_ICCV_2023_paper.pdf) | > Many educational videos use slide presentations, a sequence of visual pages that contain text and figures accompanied by spoken language, which are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing vision-...

---

## 46. LLMGA: Multimodal Large Language Model based Generation Assistant

**Authors:** bin xia*, Shiyin Wang, Yingfan Tao, Yitong Wang, Jiaya Jia

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.424

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05554.pdf) | > "In this paper, we introduce a Multimodal Large Language Model-based Generation Assistant (LLMGA), leveraging the vast reservoir of knowledge and proficiency in reasoning, comprehension, and response inherent in Large Language Models (LLMs) to assist users in image generation and editing. Diverging from existing approaches where Multimodal Large Language Models (MLLMs) generate fixed-size embeddin...

---

## 47. Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling

**Authors:** Xinlu Zhang, Shiyang Li, Zhiyu Chen, Xifeng Yan, Linda Ruth Petzold

**Year:** 2023 | **Venue:** ICML 2023 | **Citations:** N/A | **Score:** 0.425

[PDF](https://openreview.net/pdf?id=zi1iKanf9k) | > Health conditions among patients in intensive care units (ICUs) are monitored via electronic health records (EHRs), composed of numerical time series and lengthy clinical note sequences, both taken at $\textit{irregular}$ time intervals. Dealing with such irregularity in every modality, and integrating irregularity into multimodal representations to improve medical predictions, is a challenging pr...

---

## 48. GigaPevt: Multimodal Medical Assistant

**Authors:** Pavel Blinov, Konstantin Egorov, Ivan Sviridov, Nikolay Ivanov, Stepan Botman

**Year:** 2024 | **Venue:** IJCAI 2024 | **Citations:** N/A | **Score:** 0.476

[PDF](https://www.ijcai.org/proceedings/2024/0992.pdf) | > Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models. Such an approach shows immedi...

---

## 49. UniCode : Learning a Unified Codebook for Multimodal Large Language Models

**Authors:** Sipeng Zheng*, Bohan Zhou, Yicheng Feng, Ye Wang, Zongqing Lu*

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.425

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01331.pdf) | > "In this paper, we propose UniCode, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLMs’ ability to generate images and texts in a mult...

---

## 50. TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation

**Authors:** Gökçe Uludoğan, Zeynep Balal, Furkan Akkurt, Meliksah Turker, Onur Gungor

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.411

[PDF](https://aclanthology.org/2024.findings-acl.600.pdf) | > The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce TURNA, a language model developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks.TURNA is pretrained with an encoder-decoder archi...

---

## 51. Spider: A Unified Framework for Context-dependent Concept Segmentation

**Authors:** Xiaoqi Zhao, Youwei Pang, Wei Ji, Baicheng Sheng, Jiaming Zuo

**Year:** 2024 | **Venue:** ICML 2024 | **Citations:** N/A | **Score:** 0.418

[PDF](https://openreview.net/pdf?id=mWV8NeU79e) | > Different from the context-independent (CI) concepts such as human, car, and airplane, context-dependent (CD) concepts require higher visual understanding ability, such as camouflaged object and medical lesion. Despite the rapid advance of many CD understanding tasks in respective branches, the isolated evolution leads to their limited cross-domain generalisation and repetitive technique innovatio...

---

## 52. EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs

**Authors:** Xiangyu Zhao, Bo Liu, Qijiong Liu, Guangyuan Shi, Xiao-Ming Wu

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.426

[PDF](https://aclanthology.org/2024.acl-long.74.pdf) | > We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge modalities, EasyGen leverages BiDiffuser, a bidirectional conditional d...

---

## 53. MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine

**Authors:** Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.442

> This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as deta...

---

## 54. UMIE: Unified Multimodal Information Extraction with Instruction Tuning

**Authors:** Lin  Sun, Kai Zhang, Qingyuan Li, Renze Lou

**Year:** 2024 | **Venue:** AAAI 2024 | **Citations:** N/A | **Score:** 0.427

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/29873/31523) | > Multimodal information extraction (MIE) gains significant attention as the popularity of multimedia content increases. However, current MIE methods often resort to using task-specific model structures, which results in limited generalizability across tasks and underutilizes shared knowledge across MIE tasks. To address these issues, we propose UMIE, a unified multimodal information extractor to un...

---

## 55. Achieving Cross Modal Generalization with Multimodal Unified Representation

**Authors:** Yan Xia, Hai Huang, Jieming Zhu, Zhou Zhao

**Year:** 2023 | **Venue:** NIPS 2023 | **Citations:** N/A | **Score:** 0.402

[PDF](https://openreview.net/pdf?id=t7ZowrDWVw) | > This paper introduces a novel task called Cross Modal Generalization (CMG), which addresses the challenge of learning a unified discrete representation from paired multimodal data during pre-training. Then in downstream tasks, the model can achieve zero-shot generalization ability in other modalities when only one modal is labeled. Existing approaches in multimodal representation learning focus mo...

---

## 56. Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization

**Authors:** Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen

**Year:** 2024 | **Venue:** ICML 2024 | **Citations:** N/A | **Score:** 0.414

[PDF](https://openreview.net/pdf?id=S9lk6dk4LL) | > In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training ...

---

## 57. ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences

**Authors:** Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, Yongdong Zhang

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.446

[PDF](https://aclanthology.org/2024.acl-long.386.pdf) | > Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising result...

---

## 58. OneLLM: One Framework to Align All Modalities with Language

**Authors:** Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.428

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_OneLLM_One_Framework_to_Align_All_Modalities_with_Language_CVPR_2024_paper.pdf) | > Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However existing works rely heavily on modality-specific encoders which usually differ in architecture and are limited to common modalities. In this paper we present OneLLM an MLLM that aligns eight modalities to language using a unified framework. We achieve this thr...

---

## 59. SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs

**Authors:** Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey

**Year:** 2023 | **Venue:** NIPS 2023 | **Citations:** N/A | **Score:** 0.393

[PDF](https://openreview.net/pdf?id=CXPUg86A1D) | > In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the rich semantic meaning and the fine-grained deta...

---

## 60. RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models

**Authors:** Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.429

[PDF](https://aclanthology.org/2024.emnlp-main.62.pdf) | > The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challen...

---

## 61. FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion

**Authors:** Zehan Wang, Ziang Zhang, Xize Cheng, Rongjie Huang, Luping Liu

**Year:** 2024 | **Venue:** ICML 2024 | **Citations:** N/A | **Score:** 0.412

[PDF](https://openreview.net/pdf?id=3XG69ZmfsB) | > Unified multi-model representation spaces are the foundation of multimodal understanding and generation. However, the billions of model parameters and catastrophic forgetting problems make it challenging to further enhance pre-trained unified spaces. In this work, we propose FreeBind, an idea that treats multimodal representation spaces as basic units, and freely augments pre-trained unified space...

---

## 62. Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction

**Authors:** Cam Van Thi Nguyen, Tuan Anh Mai, Son Le The, Dang Hai Kieu, Duc-Trong Le

**Year:** 2023 | **Venue:** EMNLP 2023 | **Citations:** N/A | **Score:** 0.390

> Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representatio...

---

## 63. OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM

**Authors:** Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.464

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_OmniMedVQA_A_New_Large-Scale_Comprehensive_Evaluation_Benchmark_for_Medical_LVLM_CVPR_2024_paper.pdf) | > Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks. However their potential in the medical domain remains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions which is essential in real-world medical applications. To solve this problem in this paper w...

---

## 64. Breaking the Boundaries: A Unified Framework for Chinese Named Entity Recognition Across Text and Speech

**Authors:** Jinzhong Ning, Yuanyuan Sun, Bo Xu, Zhihao Yang, Ling Luo

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.412

[PDF](https://aclanthology.org/2024.findings-emnlp.67.pdf) | > In recent years, with the vast and rapidly increasing amounts of spoken and textual data, Named Entity Recognition (NER) tasks have evolved into three distinct categories, i.e., text-based NER (TNER), Speech NER (SNER) and Multimodal NER (MNER). However, existing approaches typically require designing separate models for each task, overlooking the potential connections between tasks and limiting t...

---

## 65. Unified Medical Image Pre-training in Language-Guided Common Semantic Space

**Authors:** Xiaoxuan He, Yifan Yang, Xinyang Jiang, Xufang Luo*, Haoji Hu

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.442

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10510.pdf) | > "Vision-Language Pre-training (VLP) has shown the merits of analysing medical images. It efficiently learns visual representations by leveraging supervisions in their corresponding reports, and in turn facilitates analysis and interpretation of intricate imaging data. However, such observation is predominantly justified on single-modality data (mostly 2D images like X-rays), adapting VLP to learni...

---

## 66. Understanding Multimodal Instruction Format for In-context Learning

**Authors:** Yingzi Ma, Chunyuan Li, Chaowei Xiao

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.422

> The field of vision and language machine learning has witnessed a surge in interest regarding in-context learning—a technique that enables rapid adaptation to new tasks with just a handful of annotated examples. To bolster the in-context learning capabilities of multimodal vision and language models, researchers have explored various instruction tuning formats. In this paper, we aim to study what ...

---

## 67. Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding

**Authors:** Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.404

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.pdf) | > Large language models have demonstrated impressive universal capabilities across a wide range of open-ended tasks and have extended their utility to encompass multimodal conversations. However existing methods encounter challenges in effectively handling both image and video understanding particularly with limited visual tokens. In this work we introduce Chat-UniVi a Unified Vision-language model ...

---

## 68. Revisiting Multimodal Transformers for Tabular Data with Text Fields

**Authors:** Thomas Bonnier

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.421

[PDF](https://aclanthology.org/2024.findings-acl.87.pdf) | > Tabular data with text fields can be leveraged in applications such as financial risk assessment or medical diagnosis prediction. When employing multimodal approaches to make predictions based on these modalities, it is crucial to make the most appropriate modeling choices in terms of numerical feature encoding or fusion strategy. In this paper, we focus on multimodal classification tasks based on...

---

## 69. MVoice: Multilingual Unified Voice Generation With Discrete Representation at Scale

**Authors:** Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Jinchuan Tian

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.402

> Various applications of voice synthesis have been developed independently despite the fact that they generate "voice" as output in common. In addition, the majority of voice synthesis models currently rely on annotated data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations presented in human voice, including speaker i...

---

## 70. Versatile Diffusion: Text, Images and Variations All in One Diffusion Model

**Authors:** Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, Humphrey Shi

**Year:** 2023 | **Venue:** ICCV 2023 | **Citations:** N/A | **Score:** 0.395

[PDF](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Versatile_Diffusion_Text_Images_and_Variations_All_in_One_Diffusion_ICCV_2023_paper.pdf) | > Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing sing...

---

## 71. Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action

**Authors:** Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.418

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.pdf) | > We present Unified-IO 2 a multimodal and multi-skill unified model capable of following novel instructions. Unified-IO 2 can use text images audio and/or videos as input and can generate text image or audio outputs which is accomplished in a unified way by tokenizing these different inputs and outputs into a shared semantic space that can then be processed by a single encoder-decoder transformer m...

---

## 72. Unified Discrete Diffusion for Simultaneous Vision-Language Generation

**Authors:** Minghui Hu, Chuanxia Zheng, Zuopeng Yang, Tat-Jen Cham, Heliang Zheng

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.408

[PDF](https://openreview.net/pdf?id=8JqINxA-2a) | > The recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single mode...

---

## 73. VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding

**Authors:** Yue Fan, Xiaojian Ma*, Rujie Wu, yuntao du, Jiaqi Li

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.407

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03241.pdf) | > "We explore how reconciling several foundation models (large language models and vision-language models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos. In particular, the proposed multimodal agent : 1) constructs a structured memory to store both the generic temporal event desc...

---

## 74. Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.436

> Advances in generative medical models are often constrained by modality-specific scenarios that hinder the integration of complementary evidence, such as imaging, pathology, and clinical notes. This fragmentation limits their development to true foundation models that empower medical AI agents to learn from and predict across the full spectrum of biomedical knowledge. To address the challenges, we...

---

## 75. Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.387

> Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. 
Omni-Weather integrates a radar encoder for weather gener...

---

## 76. InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation

**Authors:** Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.351

[PDF](https://openreview.net/pdf?id=MLBdiWu4Fw) | > This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. InternVid contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable a...

---

## 77. Unified Cross-Scale 3D Generation and Understanding via Autoregressive Modeling

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.372

> 3D structure modeling is essential across scales, enabling applications from fluid simulation and 3D reconstruction to protein folding and molecular docking. Yet, despite shared 3D spatial patterns, current approaches remain fragmented, with models narrowly specialized for specific domains and unable to generalize across tasks or scales.
We propose Uni-3DAR, a unified autoregressive framework for ...

---

## 78. RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis

**Authors:** Yao Mu, Junting Chen, Qing-Long Zhang, Shoufa Chen, Qiaojun Yu

**Year:** 2024 | **Venue:** ICML 2024 | **Citations:** N/A | **Score:** 0.361

[PDF](https://openreview.net/pdf?id=xnQ1qoly7Q) | > Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various...

---

## 79. UniPrompt-CL: Sustainable Continual Learning in Medical AI with Unified Prompt Pools

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.405

> Although modern AI models achieve state-of-the-art performance with large-scale datasets, strict ethical and institutional constraints in medicine make centralised learning nearly impossible. Institutions must therefore rely on local data, but traditional training methods quickly overfit new samples and suffer from catastrophic forgetting, making continual learning (CL) essential. While CL has adv...

---

## 80. Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs

**Authors:** Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.336

[PDF](https://openreview.net/pdf?id=hFVpqkRRH1) | > Multimodal large language models (MLLMs) have shown impressive success across modalities such as image, video, and audio in a variety of understanding and generation tasks.
  However, current MLLMs are surprisingly poor at understanding webpage screenshots and generating their corresponding HTML code.
  To address this problem, 
  we propose Web2Code, a benchmark consisting of a new large-scale we...

---

## 81. MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding

**Authors:** Baixuan Xu, Weiqi Wang, Haochen Shi, Wenxuan Ding, Huihao Jing

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.346

[PDF](https://aclanthology.org/2024.emnlp-main.446.pdf) | > Improving user experience and providing personalized search results in E-commerce platforms heavily rely on understanding purchase intention. However, existing methods for acquiring large-scale intentions bank on distilling large language models with human annotation for verification. Such an approach tends to generate product-centric intentions, overlook valuable visual information from product i...

---

## 82. README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP

**Authors:** Zonghai Yao, Nandyala Siddharth Kantu, Guanghao Wei, Hieu Tran, Zhangqi Duan

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.350

[PDF](https://aclanthology.org/2024.findings-emnlp.737.pdf) | > The advancement in healthcare has shifted focus toward patient-centric approaches, particularly in self-care and patient education, facilitated by access to Electronic Health Records (EHR). However, medical jargon in EHRs poses significant challenges in patient comprehension. To address this, we introduce a new task of automatically generating lay definitions, aiming to simplify complex medical te...

---

## 83. Making Multimodal Generation Easier: When Diffusion Models Meet LLMS

**Authors:** Xiangyu Zhao, Bo LIU, Qijiong Liu, Guangyuan SHI, Xiao-Ming Wu

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.342

> We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional condi...

---

## 84. HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation

**Authors:** Tianwei Lin, Wenqiao Zhang, SIJING LI, Yuqian Yuan, Binhe Yu

**Year:** 2025 | **Venue:** ICML 2025 | **Citations:** N/A | **Score:** 0.368

[PDF](https://openreview.net/pdf?id=WbP2OwMULq) | > We present **HealthGPT**, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained Large Language Models (LLMs). This is achieved through a novel heterogeneous l...

---

## 85. VIMI: Grounding Video Generation through Multi-modal Instruction

**Authors:** Yuwei Fang, Willi Menapace, Aliaksandr Siarohin, Tsai-Shien Chen, Kuan-Chieh Wang

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.346

[PDF](https://aclanthology.org/2024.emnlp-main.254.pdf) | > Existing text-to-video diffusion models rely solely on text-only encoders for their pretraining. This limitation stems from the absence of large-scale multimodal prompt video datasets, resulting in a lack of visual grounding and restricting their versatility and application in multimodal integration. To address this, we construct a large-scale multimodal prompt dataset by employing retrieval metho...

---

## 86. Multimodal Representation Distribution Learning for Medical Image Segmentation

**Authors:** Chao Huang, Weichao Cai, Qiuping Jiang, Zhihua Wang

**Year:** 2024 | **Venue:** IJCAI 2024 | **Citations:** N/A | **Score:** 0.365

[PDF](https://www.ijcai.org/proceedings/2024/0459.pdf) | > Medical image segmentation is one of the most critical tasks in medical image analysis. However, the performance of existing methods is limited by the lack of high-quality labeled data due to the expensive data annotation. To alleviate this limitation, we propose a novel multi-modal learning method for medical image segmentation. In our method, medical text annotation is incorporated to compensate...

---

## 87. EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot

**Authors:** Hao Fei, Han Zhang, Bin Wang, Lizi Liao, Qian Liu

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.334

[PDF](https://aclanthology.org/2024.acl-demos.7.pdf) | > This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal e...

---

## 88. CookingCLIP：Learning a Contextualized Multimodal Embedding from Instructional Cooking Videos for Zero-shot Recipe Generation

**Authors:** lin wang, Jing Chen, Zhang Hongyi

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.320

> Cooking is one of the oldest and the most common human activities in daily life. Instructional cooking videos have also become one of the most common data sources for multi-modal visual understanding research, compared to general domains, cooking videos: (1) not only have a significantly stronger cross-modal dependency between the speech texts and their corresponding visual frames at each individu...

---

## 89. Exploring the Capability of Multimodal LLMs with Yonkoma Manga: The YManga Dataset and Its Challenging Tasks

**Authors:** Qi Yang, Jingjie Zeng, Liang Yang, Zhihao Yang, Hongfei Lin

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.330

[PDF](https://aclanthology.org/2024.findings-emnlp.506.pdf) | > Yonkoma Manga, characterized by its four-panel structure, presents unique challenges due to its rich contextual information and strong sequential features. To address the limitations of current multimodal large language models (MLLMs) in understanding this type of data, we create a novel dataset named YManga from the Internet. After filtering out low-quality content, we collect a dataset of 1,015 ...

---

## 90. Multimodal Prompt Learning for Product Title Generation with Extremely Limited Labels

**Authors:** Bang Yang, Fenglin Liu, Zheng Li, Qingyu Yin, Chenyu You

**Year:** 2023 | **Venue:** ACL 2023 | **Citations:** N/A | **Score:** 0.311

[PDF](https://aclanthology.org/2023.findings-acl.166.pdf) | > Generating an informative and attractive title for the product is a crucial task for e-commerce. Most existing works follow the standard multimodal natural language generation approaches, e.g., image captioning, and employ the large scale of human-labelled datasets to train desirable models. However, for novel products, especially in a different domain, there are few existing labelled data. In thi...

---

## 91. TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning

**Authors:** Kate Sanders, Nathaniel Weir, Benjamin Van Durme

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.333

[PDF](https://aclanthology.org/2024.emnlp-main.1059.pdf) | > It is challenging for models to understand complex, multimodal content such as television clips, and this is in part because video-language models often rely on single-modality reasoning and lack interpretability. To combat these issues we propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modali...

---

## 92. TRINS: Towards Multimodal Language Models that Can Read

**Authors:** Ruiyi Zhang, Yanzhe Zhang, Jian Chen, Yufan Zhou, Jiuxiang Gu

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.329

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_TRINS_Towards_Multimodal_Language_Models_that_Can_Read_CVPR_2024_paper.pdf) | > Large multimodal language models have shown remarkable proficiency in understanding and editing images. However a majority of these visually-tuned models struggle to comprehend the textual content embedded in images primarily due to the limitation of training data. In this work we introduce TRINS: a Text-Rich image1 INStruction dataset with the objective of enhancing the reading ability of the mul...

---

## 93. ChatPose: Chatting about 3D Human Pose

**Authors:** Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.322

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_ChatPose_Chatting_about_3D_Human_Pose_CVPR_2024_paper.pdf) | > We introduce ChatPose a framework employing Large Language Models (LLMs) to understand and reason about 3D human poses from images or textual descriptions. Our work is motivated by the human ability to intuitively understand postures from a single image or a brief description a process that intertwines image interpretation world knowledge and an understanding of body language. Traditional human po...

---

## 94. Large Multimodal Model for Real-World Radiology Report Generation

**Authors:** Brian Nlong Zhao, XINYANG JIANG, Xufang Luo, Yifan Yang, Bo Li

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.332

> While automatic report generation has demonstrated promising results using deep learning-based methods, deploying these algorithms in real-world scenarios remains challenging. Compared to conventional report generation, real-world report generation requires model to follow the instruction from the radiologists and consider contextual information. Thus, this paper focuses on developing a practical ...

---

## 95. Fair in Mind, Fair in Action? A Synchronous Benchmark for Understanding and Generation in UMLLMs

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.359

> As artificial intelligence (AI) permeates society, ensuring fairness has become a foundational challenge. However, the field faces a “Babel Tower” dilemma: fairness metrics abound, yet their underlying philosophical assumptions often conflict, hindering unified paradigms—particularly in unified multimodal large language models (UMLLMs), where biases propagate systemically across tasks. To address ...

---

## 96. ReCLLaMA: A Reasoning-Centered LLM Agent for Medical Diagnosis

**Authors:** Yang Zhao, Bowen Xu, Saiyun Dong, Xinghua Shi

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.373

> Large Language Models (LLMs) have demonstrated impressive capabilities in natural language understanding, yet their application to clinical diagnosis remains constrained by hallucinations, limited interpretability, and the absence of formal reasoning mechanisms. To address these limitations, we propose ReCLLaMA, a Reasoning-Centered LLM Agent for Medical Diagnosis, which integrates statistical lan...

---

## 97. HealthAlignSumm : Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues

**Authors:** Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Gaurav Pandey, Dinesh Raghu

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.347

[PDF](https://aclanthology.org/2024.findings-emnlp.675.pdf) | > As generative AI progresses, collaboration be-tween doctors and AI scientists is leading to thedevelopment of personalized models to stream-line healthcare tasks and improve productivity.Summarizing doctor-patient dialogues has be-come important, helping doctors understandconversations faster and improving patient care.While previous research has mostly focused ontext data, incorporating visual cu...

---

## 98. MedSimSearch: Sim2Real Agentic Learning for Medical Visual Reasoning

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.390

> Developing autonomous agents for complex Medical Visual Reasoning is a critical goal, yet training them in real-world clinical settings is largely infeasible due to severe privacy, data, and safety constraints. While retrieval-augmented methods exist, they often depend on impractical multimodal indexing or fail to address the core challenge of learning interactive policies without real-world expos...

---

## 99. GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI

**Authors:** pengcheng chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.361

[PDF](https://openreview.net/pdf?id=K6b8LCXBeQ) | > Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Curren...

---

## 100. MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models

**Authors:** Mohammad Shahab Sepehri, Zalan Fabian, Maryam Soltanolkotabi, Mahdi Soltanolkotabi

**Year:** 2025 | **Venue:** ICLR 2025 | **Citations:** N/A | **Score:** 0.392

[PDF](https://openreview.net/pdf?id=H9UnNgdq0g) | > Multimodal Large Language Models (MLLMs) have tremendous potential to improve the accuracy, availability, and cost-effectiveness of healthcare by providing automated solutions or serving as aids to medical professionals. Despite promising first steps in developing medical MLLMs in the past few years, their capabilities and limitations are not well understood. Recently, many benchmark datasets have...

---

## 101. MedSCAMA: Medical SCale-Aware Multi-Agent Framework for Medical Image Retrieval and Retrieval-Augmented Generation

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.379

> Medical image retrieval and retrieval-augmented generation require representations that capture both visual similarity and clinically meaningful semantics across multiple levels of granularity. However, existing image encoders may miss pixel-to-organ-to-context cues, and report encoders may flatten hierarchical findings with global summaries, weakening vision-language alignment and alignment betwe...

---

## 102. Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.348

> Recent advancements in unified vision-language models (VLMs), which integrate both visual understanding and generation capabilities, have attracted significant attention. The underlying hypothesis is that a unified architecture with mixed training on both understanding and generation tasks can enable mutual enhancement between understanding and generation. However, this hypothesis remains underexp...

---

## 103. MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.360

> Modern clinical diagnosis relies on the comprehensive analysis of multi-modal patient data, drawing on medical expertise to ensure systematic and rigorous reasoning. Recent advances in Vision–Language Models (VLMs) and agent-based methods are reshaping medical diagnosis by effectively integrating multi-modal information. However, they often output direct answers and empirical-driven conclusions wi...

---

## 104. Unified Medical Lesion Segmentation via Self-referring Indicator

**Authors:** Shijie Chang, Xiaoqi Zhao, Lihe Zhang, Tiancheng Wang

**Year:** 2025 | **Venue:** CVPR 2025 | **Citations:** N/A | **Score:** 0.340

[PDF](https://openaccess.thecvf.com/content/CVPR2025/papers/Chang_Unified_Medical_Lesion_Segmentation_via_Self-referring_Indicator_CVPR_2025_paper.pdf) | > The recently emerged in-context-learning-based (ICL-based) models have the potential towards the unification of medical lesion segmentation. However, due to their cross-fusion designs, existing ICL-based unified segmentation models fail to accurately localize lesions with low-matched reference sets. Considering that the query itself can be regarded as a high-matched reference, which better indicat...

---

## 105. Towards Multi-dimensional Explanation Alignment for Medical Classification

**Authors:** Lijie Hu, Songning Lai, Wenshuo Chen, Hongru Xiao, Hongbin Lin

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.319

[PDF](https://openreview.net/pdf?id=3A5VgiH5Pw) | > The lack of interpretability in the field of medical image analysis has significant ethical and legal implications. Existing interpretable methods in this domain encounter several challenges, including dependency on specific models, difficulties in understanding and visualization, and issues related to efficiency. To address these limitations, we propose a novel framework called Med-MICN (Medical ...

---

## 106. Think Twice to See More: Iterative Visual Reasoning in Medical VLMs

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.368

> Medical vision-language models (VLMs) excel at image-text understanding but typically rely on a single-pass reasoning that neglects localized visual cues. In clinical practice, however, human experts iteratively scan, focus, and refine the regions of interest before reaching a final diagnosis. To narrow this machine-human perception gap, we introduce ViTAR, a novel VLM framework that emulates the ...

---

## 107. M3CoTBench: Benchmark Chain-of-Thought of MLLMs in Medical Image Understanding

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.375

> Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models by encouraging step-by-step intermediate reasoning, and recent advances have extended this paradigm to Multimodal Large Language Models (MLLMs). In the medical domain, where diagnostic decisions depend on nuanced visual cues and sequential reasoning, CoT aligns naturally with clinical thinking processes. Howev...

---

## 108. AToken: A Unified Tokenizer for Vision

**Authors:** Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.356

> We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs in a shared 4D latent space, optimizing without separate model designs. Specifical...

---

## 109. Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks

**Authors:** Lehan Wang, Haonan Wang, Honglong Yang, Jiaji Mao, Zehong Yang

**Year:** 2025 | **Venue:** ICLR 2025 | **Citations:** N/A | **Score:** 0.397

[PDF](https://openreview.net/pdf?id=YuHQTo6G9S) | > Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. 
Most current medical generalist models are region-agnostic, treating the entire image as a holistic representation. However, they struggle to identify which specific regions they are focusi...

---

## 110. Unified Single Transformer for Multimodal Video Understanding and Generation

**Authors:** Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Zunnan Xu

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.377

> With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strat...

---

## 111. TRAUMA THOMPSON: A Novel Dataset and Benchmarks For AI Copilots For Humanitarian Operational Medicine

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.371

> This paper introduces the Trauma THOMPSON dataset, a novel dataset and benchmarks to foster research towards designing artificial intelligence-based decision-making algorithms specifically suited for life-saving interventions performed by less experienced caregivers. This paradigm is particularly relevant to support humanitarian operational medicine, where the essential resources are either unavai...

---

## 112. Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.374

> Solving tough clinical questions that require both image and text understanding is still a major challenge in healthcare AI. In this work, we propose Q-FSRU, a new model that combines Frequency Spectrum Representation and Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation (Quantum RAG) for medical Visual Question Answering (VQA). The model takes in features from medical imag...

---

## 113. A Research on Result Interpretability of Medical AI Based on Large Language Model

**Authors:** Wang Chen, Tiezheng Guo, Yanyi Liu, Huihan Wang, Yingyou Wen

**Year:** 2025 | **Venue:** ICLR 2025 | **Citations:** N/A | **Score:** 0.435

> Explainability is one of the important challenges facing the application of medical AI. The existing AI explainability research is more of a kind of process explainability study. Drawing on the behavioral habits of human beings to communicate on a certain topic, this paper proposes a definition of result interpretability for medical AI, divides explainable medical AI research into three phases: da...

---

## 114. UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.340

> Despite the impressive progress on understanding and generating images shown by the recent unified architectures, the integration of 3D tasks remains challenging and largely unexplored. In this paper, we introduce UniUGG, the first unified understanding and generation framework for 3D modalities. Our unified framework employs an LLM to comprehend and decode sentences and 3D representations. At its...

---

## 115. Enhancing Medical Image Generation with Anatomical Precision: A Multi-Headed VAE-Based Diffusion Model

**Authors:** Hongfei Yang, YoungSeok Jeon, Mengling Feng

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.312

> Score-based image generation models, also known as diffusion models, can generate highly realistic and diverse natural images. However, a common challenge emerges when applying diffusion models to medical image generation and segmentation. While these models excel at producing realistic local textures, they struggle to accurately capture global anatomical priors, such as organ shape and location. ...

---

## 116. UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens

**Authors:** Ruichuan An, Sihan Yang, Renrui Zhang, zijun shen, Ming Lu

**Year:** 2025 | **Venue:** NIPS 2025 | **Citations:** N/A | **Score:** 0.320

> Personalized models have demonstrated remarkable success in understanding and generating concepts provided by users. However, existing methods use separate concept tokens for understanding and generation, treating these tasks in isolation. This may result in limitations for generating images with complex prompts. For example, given the concept $\langle bo\rangle$, generating "$\langle bo\rangle$ w...

---

## 117. ORION: Decoupling and Alignment for Unified Autoregressive Understanding and Generation

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.353

> Unified multimodal Large Language Models (MLLMs) hold great promise for seamlessly integrating understanding and generation. However, monolithic autoregressive architectures, despite their elegance and conversational fluency, suffer from a fundamental semantic–structural conflict: optimizing for low-level reconstructability in generation leads to catastrophic forgetting of high-level semantic unde...

---

## 118. Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.339

> Recent works have made notable advancements in enhancing unified models for text-to-image generation through the Chain-of-Thought (CoT). However, these reasoning methods separate the processes of understanding and generation, which limits their ability to guide the reasoning of unified models in addressing the deficiencies of their generative capabilities. To this end, we propose a novel reasoning...

---

## 119. Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.343

> This paper presents Omni-View, which extends the unified multimodal understanding and generation to 3D scenes based on multiview images, exploring the principle that ``generation facilitates understanding". Consisting of understanding model, texture module, and geometry module, Omni-View jointly models scene understanding, novel view synthesis, and geometry estimation, enabling synergistic interac...

---

## 120. Co-Reinforcement Learning for Unified Multimodal Understanding and Generation

**Authors:** Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma

**Year:** 2025 | **Venue:** NIPS 2025 | **Citations:** N/A | **Score:** 0.330

> This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a share...

---

## 121. Libra: Leveraging Temporal Images for Biomedical Radiology Analysis

**Authors:** Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho

**Year:** 2025 | **Venue:** ICLR 2025 | **Citations:** N/A | **Score:** 0.334

> Radiology report generation (RRG) is a challenging task, as it requires a thorough understanding of medical images, integration of multiple temporal inputs, and accurate report generation. Effective interpretation of medical images, such as chest X-rays (CXRs), demands sophisticated visual-language reasoning to map visual findings to structured reports. Recent studies have shown that multimodal la...

---

## 122. XAI-Annotate: Automated Medical Segmentation via Explainable AI Saliency Distillation

**Authors:** Rong Fu, Yaoyang WU, Anqi Zhao, weizhi Tang, Simon James Fong

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.387

> The deployment of medical AI is hindered by the high cost of expert annotations, particularly for segmentation tasks. We introduce XAI-Annotate, a novel framework that transforms classification explainability into automated annotation. By leveraging saliency maps from correctly classified samples, our method distills latent segmentation cues into high-quality masks—without requiring segmentation l...

---

## 123. Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE

**Authors:** Xun Zhu, Ying Hu, Fanbin Mo, Miao Li, Ji Wu

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.315

[PDF](https://openreview.net/pdf?id=oyl2Fnzune) | > Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal multi-task optimization in MLLMs, recent advances primarily focus on improving the LLM componen...

---

## 124. JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation

**Authors:** Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu

**Year:** 2025 | **Venue:** CVPR 2025 | **Citations:** N/A | **Score:** 0.322

[PDF](https://openaccess.thecvf.com/content/CVPR2025/papers/Ma_JanusFlow_Harmonizing_Autoregression_and_Rectified_Flow_for_Unified_Multimodal_Understanding_CVPR_2025_paper.pdf) | > We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model.JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling.Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, el...

---

## 125. VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation

**Authors:** Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li

**Year:** 2025 | **Venue:** ICLR 2025 | **Citations:** N/A | **Score:** 0.316

[PDF](https://openreview.net/pdf?id=02haSpO453) | > VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for ...

---

## 126. UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation

**Authors:** Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu

**Year:** 2025 | **Venue:** NIPS 2025 | **Citations:** N/A | **Score:** 0.323

> We introduce UniGen, a unified multimodal large language model (MLLM) capable of image understanding and generation. We study the full training pipeline of UniGen from a data-centric perspective, including multi-stage pre-training, supervised fine-tuning, and direct preference optimization. More importantly, we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time scaling, whi...

---

## 127. MedRAX: Medical Reasoning Agent for Chest X-ray

**Authors:** Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, BO WANG

**Year:** 2025 | **Venue:** ICML 2025 | **Citations:** N/A | **Score:** 0.346

[PDF](https://openreview.net/pdf?id=JiFfij5iv0) | > Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art  CX...

---

## 128. LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation

**Authors:** Suhyeon Lee, Won Jun Kim, Jinho Chang, Jong Chul Ye

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.320

[PDF](https://openreview.net/pdf?id=BqHaLnans2) | > Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist of a combination of reasoning based on visual features and prior knowledge. Many recent works have fo...

---

## 129. YouTubePD: A Multimodal Benchmark for Parkinson’s Disease Analysis

**Authors:** Andy Zhou, Samuel Li, Pranav Sriram, Xiang Li, Jiahua Dong

**Year:** 2023 | **Venue:** NIPS 2023 | **Citations:** N/A | **Score:** 0.308

[PDF](https://openreview.net/pdf?id=AIeeXKsspI) | > The healthcare and AI communities have witnessed a growing interest in the development of AI-assisted systems for automated diagnosis of Parkinson's Disease (PD), one of the most prevalent neurodegenerative disorders. However, the progress in this area has been significantly impeded by the absence of a unified, publicly available benchmark, which prevents comprehensive evaluation of existing PD an...

---

## 130. Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation

**Authors:** Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu

**Year:** 2025 | **Venue:** CVPR 2025 | **Citations:** N/A | **Score:** 0.344

[PDF](https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Janus_Decoupling_Visual_Encoding_for_Unified_Multimodal_Understanding_and_Generation_CVPR_2025_paper.pdf) | > We introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding....

---

## 131. Medical Vision Generalist: Unifying Medical Imaging Tasks in Context

**Authors:** Sucheng Ren, Xiaoke Huang, Xianhang Li, Junfei Xiao, Jieru Mei

**Year:** 2025 | **Venue:** ICLR 2025 | **Citations:** N/A | **Score:** 0.343

> This study presents Medical Vision Generalist (MVG), the first foundation model capable of handling various medical imaging tasks---such as cross-modal synthesis, image segmentation, denoising, and inpainting---within a unified image-to-image generation framework. Specifically, MVG employs an in-context generation strategy that standardizes the handling of inputs and outputs as images. By treating...

---

## 132. MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.338

> Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical ...

---

## 133. Unified Reward Model for Multimodal Understanding and Generation

**Authors:** Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, Jiaqi Wang

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.345

> Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing reward models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that a reward model jointly learning to assess multiple vision tasks may fo...

---

## 134. SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.348

> In this paper, we introduce SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook (SGHC) that provides consistent discrete representations for multimodal understanding and generation. Recently, unified image tokenizers have sparked exploration within the research community, which is designed to capture high-level semantic features for understanding and retaining low-level p...

---

## 135. GPT4Point: A Unified Framework for Point-Language Understanding and Generation

**Authors:** Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.300

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_GPT4Point_A_Unified_Framework_for_Point-Language_Understanding_and_Generation_CVPR_2024_paper.pdf) | > Multimodal Large Language Models (MLLMs) have excelled in 2D image-text comprehension and image generation but their understanding of the 3D world is notably deficient limiting progress in 3D language understanding and generation. To solve this problem we introduce GPT4Point an innovative groundbreaking point-language multimodal model designed specifically for unified 3D object understanding and g...

---

## 136. MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models

**Authors:** 

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.346

> Unified Multimodal Large Language Models (U-MLLMs) have garnered considerable interest for their ability to seamlessly integrate generation and comprehension tasks. However, existing research lacks a unified evaluation standard, often relying on isolated benchmarks to assess these capabilities. Moreover, current work highlights the potential of ``mixed-modality generation capabilities'' through ca...

---

## 137. UniTok: a Unified Tokenizer for Visual Generation and Understanding

**Authors:** Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu

**Year:** 2025 | **Venue:** NIPS 2025 | **Citations:** N/A | **Score:** 0.313

> Visual generative and understanding models typically rely on distinct tokenizers to process images, presenting a key challenge for unifying them within a single framework. Recent studies attempt to address this by connecting the training of VQVAE (for autoregressive generation) and CLIP (for understanding) to build a unified tokenizer. However, directly combining these training objectives has been...

---

## 138. User-Centric Democratization towards Social Value Aligned Medical AI Services

**Authors:** Zhaonian Zhang, Richard Jiang

**Year:** 2023 | **Venue:** IJCAI 2023 | **Citations:** N/A | **Score:** 0.389

[PDF](https://www.ijcai.org/proceedings/2023/0702.pdf) | > Democratic AI, aiming at developing AI systems aligned with human values, holds promise for making AI services accessible to people. However, concerns have been raised regarding the participation of non-technical individuals, potentially undermining the carefully designed values of AI systems by experts. In this paper, we investigate Democratic AI, define it mathematically, and propose a user-cent...

---

## 139. Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation

**Authors:** Huu Tien Nguyen, Dac Thai Nguyen, Duc Nguyen The Minh, Trung Thanh Nguyen, Thao Nguyen Truong

**Year:** 2025 | **Venue:** NIPS 2025 | **Citations:** N/A | **Score:** 0.362

> Vision-Language Foundation Models (VLMs), trained on large-scale multimodal datasets, have driven significant advances in Artificial Intelligence (AI) by enabling rich cross-modal reasoning. Despite their success in general domains, applying these models to medical imaging remains challenging due to the limited availability of diverse imaging modalities and multilingual clinical data. Most existin...

---

## 140. LUCID-3D: A Lightweight and Compatible Framework for Unified 3D Understanding and Generation

**Authors:** Yongwei Chen, Tianyi Wei, Yushi LAN, Zhaoyang Lyu, Shangchen Zhou

**Year:** 2026 | **Venue:** ICLR 2026 | **Citations:** N/A | **Score:** 0.336

> The rapid progress of large multimodal models has inspired efforts toward unified frameworks that couple understanding and generation. While such paradigms have shown remarkable success in 2D, extending them to 3D remains largely underexplored. Existing attempts to unify 3D tasks under a single autoregressive (AR) paradigm lead to significant performance degradation due to forced signal quantizati...

---

## 141. Leveraging Hard Negative Priors for Automatic Medical Report Generation

**Authors:** Bhanu Prakash Voutharoja, Lei Wang, Luping Zhou

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.285

> Recently, automatic medical report generation has become an active research topic in medical imaging field. It is imperative for the model to identify normal and abnormal regions in a medical image to generate a coherent and diverse report. However, medical datasets are highly biased towards normal regions. This makes most existing models tend to generate a generic report without sufficiently cons...

---

## 142. Unity in Diversity: Collaborative Pre-training Across Multimodal Medical Sources

**Authors:** Xiaochen Wang, Junyu Luo, Jiaqi Wang, Yuan Zhong, Xiaokun Zhang

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.325

[PDF](https://aclanthology.org/2024.acl-long.199.pdf) | > Although pre-training has become a prevalent approach for addressing various biomedical tasks, the current efficacy of pre-trained models is hindered by their reliance on a limited scope of medical sources. This limitation results in data scarcity during pre-training and restricts the range of applicable downstream tasks. In response to these challenges, we develop MedCSP, a new pre-training strat...

---

## 143. Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration

**Authors:** Tony C. W. Mok, Zi Li, Yunhao Bai, Jianpeng Zhang, Wei Liu

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.301

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Mok_Modality-Agnostic_Structural_Image_Representation_Learning_for_Deformable_Multi-Modality_Medical_Image_CVPR_2024_paper.pdf) | > Establishing dense anatomical correspondence across distinct imaging modalities is a foundational yet challenging procedure for numerous medical image analysis studies and image-guided radiotherapy. Existing multi-modality image registration algorithms rely on statistical-based similarity measures or local structural image representations. However the former is sensitive to locally varying noise w...

---

## 144. Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation

**Authors:** Wenting Chen, Linlin Shen, Jingyang Lin, Jiebo Luo, Xiang Li

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.301

[PDF](https://aclanthology.org/2024.acl-long.514.pdf) | > Fine-grained vision-language models (VLM) have been widely used for inter-modality local alignment between the predefined fixed patches and textual words. However, in medical analysis, lesions exhibit varying sizes and positions, and using fixed patches may cause incomplete representations of lesions. Moreover, these methods provide explainability by using heatmaps to show the general image areas ...

---

## 145. Human-Centred Multimodal Deep Learning Models for Chest X-Ray Diagnosis

**Authors:** Chihcheng Hsieh

**Year:** 2023 | **Venue:** IJCAI 2023 | **Citations:** N/A | **Score:** 0.295

[PDF](https://www.ijcai.org/proceedings/2023/0817.pdf) | > My thesis consists of investigating how chest X-ray images, radiologists' eye movements and patients' clinical data can be used to teach a machine how radiologists read and classify images with the goal of creating human-centric AI architectures that can (1) capture radiologists' search behavioural patterns using their eye-movements in order to improve classification in DL systems, and (2) automat...

---

## 146. Multimodal Deep Generative Models for Remote Medical Applications

**Authors:** Catherine Ordun

**Year:** 2023 | **Venue:** AAAI 2023 | **Citations:** N/A | **Score:** 0.297

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/26924/26696) | > Visible-to-Thermal (VT) face translation is an under-studied problem of image-to-image translation that offers an AI-enabled alternative to traditional thermal sensors. Over three phases, my Doctoral Proposal explores developing multimodal deep generative solutions that can be applied towards telemedicine applications. These include the contribution of a novel Thermal Face Contrastive GAN (TFC-GAN...

---

## 147. MRM: Masked Relation Modeling for Medical Image Pre-Training with Genetics

**Authors:** Qiushi Yang, Wuyang Li, Baopu Li, Yixuan Yuan

**Year:** 2023 | **Venue:** ICCV 2023 | **Citations:** N/A | **Score:** 0.283

[PDF](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_MRM_Masked_Relation_Modeling_for_Medical_Image_Pre-Training_with_Genetics_ICCV_2023_paper.pdf) | > Modern deep learning techniques on automatic multimodal medical diagnosis rely on massive expert annotations, which is time-consuming and prohibitive. Recent masked image modeling (MIM)-based pre-training methods have witnessed impressive advances for learning meaningful representations from unlabeled data and transferring to downstream tasks. However, these methods focus on natural images and ign...

---

## 148. Enhancing Weakly Supervised 3D Medical Image Segmentation through Probabilistic-aware Learning

**Authors:** Zhaoxin Fan, Runmin Jiang, Junhao Wu, Xin Huang, Tianyang Wang

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.303

> 3D medical image segmentation is a challenging task with crucial implications for disease diagnosis and treatment planning. Recent advances in deep learning have significantly enhanced fully supervised medical image segmentation. However, this approach heavily relies on labor-intensive and time-consuming fully annotated ground-truth labels, particularly for 3D volumes. To overcome this limitation,...

---

## 149. Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?

**Authors:** Pedro R. A. S. Bassi, Wenxuan Li, Yucheng Tang, Fabian Isensee, Zifu Wang

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.343

[PDF](https://openreview.net/pdf?id=YzM10FEJ2D) | > How can we test AI performance? This question seems trivial, but it isn't. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone...

---

## 150. MedSegDiff-V2: Diffusion-Based Medical Image Segmentation with Transformer

**Authors:** Junde Wu, Wei Ji, Huazhu Fu, Min Xu, Yueming Jin

**Year:** 2024 | **Venue:** AAAI 2024 | **Citations:** N/A | **Score:** 0.311

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/28418/28816) | > The Diffusion Probabilistic Model (DPM) has recently gained popularity in the field of computer vision, thanks to its image generation applications, such as Imagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated impressive capabilities and sparked much discussion within the community. Recent investigations have further unveiled the utility of DPM in the domain of medical im...

---

## 151. Improving Context Understanding in Multimodal Large Language Models via Multimodal Composition Learning

**Authors:** Wei Li, Hehe Fan, Yongkang Wong, Yi Yang, Mohan Kankanhalli

**Year:** 2024 | **Venue:** ICML 2024 | **Citations:** N/A | **Score:** 0.318

[PDF](https://openreview.net/pdf?id=Nm6jYZsBum) | > Previous efforts using frozen Large Language Models (LLMs) for visual understanding, via image captioning or image-text retrieval tasks, face challenges when dealing with complex multimodal scenarios. In order to enhance the capabilities of Multimodal Large Language Models (MLLM) in comprehending the context of vision and language, we introduce Multimodal Composition Learning (MCL) for the purpose...

---

## 152. Knowledge-based in silico models and dataset for the comparative evaluation of mammography AI for a range of breast characteristics, lesion conspicuities and doses

**Authors:** Elena Sizikova, Niloufar Saharkhiz, Diksha Sharma, Miguel Lago, Berkman Sahiner

**Year:** 2023 | **Venue:** NIPS 2023 | **Citations:** N/A | **Score:** 0.327

[PDF](https://openreview.net/pdf?id=4dsMX3RnF0) | > To generate evidence regarding the safety and efficacy of artificial intelligence (AI) enabled medical devices, AI models need to be evaluated on a diverse population of patient cases, some of which may not be readily available. We propose an evaluation approach for testing medical imaging AI models that relies on in silico imaging pipelines in which stochastic digital models of human anatomy (in ...

---

## 153. From Sights to Insights: Towards Summarization of Multimodal Clinical Documents

**Authors:** Akash Ghosh, Mohit Tomar, Abhisek Tiwari, Sriparna Saha, Jatin Salve

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.309

[PDF](https://aclanthology.org/2024.acl-long.708.pdf) | > The advancement of Artificial Intelligence is pivotal in reshaping healthcare, enhancing diagnostic precision, and facilitating personalized treatment strategies. One major challenge for healthcare professionals is quickly navigating through long clinical documents to provide timely and effective solutions. Doctors often struggle to draw quick conclusions from these extensive documents. To address...

---

## 154. Training Like a Medical Resident: Context-Prior Learning Toward Universal Medical Image Segmentation

**Authors:** Yunhe Gao

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.315

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_Training_Like_a_Medical_Resident_Context-Prior_Learning_Toward_Universal_Medical_CVPR_2024_paper.pdf) | > A major focus of clinical imaging workflow is disease diagnosis and management leading to medical imaging datasets strongly tied to specific clinical objectives. This scenario has led to the prevailing practice of developing task-specific segmentation models without gaining insights from widespread imaging cohorts. Inspired by the training program of medical radiology residents we propose a shift ...

---

## 155. MLeVLM: Improve Multi-level Progressive Capabilities based on Multimodal Large Language Model for Medical Visual Question Answering

**Authors:** Dexuan Xu, Yanyuan Chen, Jieyi Wang, Yue Huang, Hanpin Wang

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.306

[PDF](https://aclanthology.org/2024.findings-acl.296.pdf) | > Medical visual question answering (MVQA) requires in-depth understanding of medical images and questions to provide reliable answers. We summarize multi-level progressive capabilities that models need to focus on in MVQA: recognition, details, diagnosis, knowledge, and reasoning. Existing MVQA models tend to ignore the above capabilities due to unspecific data and plain architecture. To address th...

---

## 156. MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning

**Authors:** Zhe Li, Laurence T. Yang, Bocheng Ren, Xin Nie, Zhangyang Gao

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.314

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MLIP_Enhancing_Medical_Visual_Representation_with_Divergence_Encoder_and_Knowledge-guided_CVPR_2024_paper.pdf) | > The scarcity of annotated data has sparked significant interest in unsupervised pre-training methods that leverage medical reports as auxiliary signals for medical visual representation learning. However existing research overlooks the multi-granularity nature of medical visual representation and lacks suitable contrastive learning techniques to improve the models' generalizability across differen...

---

## 157. MEDICAL IMAGE UNDERSTANDING WITH PRETRAINED VISION LANGUAGE MODELS: A COMPREHENSIVE STUDY

**Authors:** Ziyuan Qin, Huahui Yi, Qicheng Lao, Kang Li

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.300

[PDF](https://openreview.net/pdf?id=txlWziuCE5W) | > The large-scale pre-trained vision language models (VLM) have shown remarkable domain transfer capability on natural images. However, it remains unknown whether this capability can also apply to the medical image domain. This paper thoroughly studies the knowledge transferability of pre-trained VLMs to the medical domain, where we show that well-designed medical prompts are the key to elicit knowl...

---

## 158. BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation

**Authors:** Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Kang Zhang, Yu-Jung Heo

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.298

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04591.pdf) | > "Multimodal Dialogue Response Generation (MDRG) is a recently proposed task where the model needs to generate responses in texts, images, or a blend of both based on the dialogue context. Due to the lack of a large-scale dataset specifically for this task and the benefits of leveraging powerful pre-trained models, previous work relies on the text modality as an intermediary step for both the image...

---

## 159. Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection

**Authors:** Yuli Wang, Peng jian, Yuwei Dai, Craig Jones, Haris I. Sair

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.312

[PDF](https://openreview.net/pdf?id=JrJW21IP9p) | > Recent approaches to vision-language tasks are built on the remarkable capabilities of large vision-language models (VLMs). These models excel in zero-shot and few-shot learning, enabling them to learn new tasks without parameter updates. However, their primary challenge lies in their design, which primarily accommodates 2D input, thus limiting their effectiveness for medical images, particularly ...

---

## 160. CC-SAM: Enhancing SAM with Cross-feature Attention and Context for Ultrasound Image Segmentation

**Authors:** Shreyank N Gowda*, David A Clifton

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.310

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06124.pdf) | > "The Segment Anything Model (SAM) has achieved remarkable successes in the realm of natural image segmentation, but its deployment in the medical imaging sphere has encountered challenges. Specifically, the model struggles with medical images that feature low contrast, faint boundaries, intricate morphologies, and small-sized objects. To address these challenges and enhance SAM’s performance in th...

---

## 161. VPL: Visual Proxy Learning Framework for Zero-Shot Medical Image Diagnosis

**Authors:** Jiaxiang Liu, Tianxiang Hu, Huimin Xiong, Jiawei Du, Yang Feng

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.319

[PDF](https://aclanthology.org/2024.findings-emnlp.583.pdf) | > Vision-language models like CLIP, utilizing class proxies derived from class name text features, have shown a notable capability in zero-shot medical image diagnosis which is vital in scenarios with limited disease databases or labeled samples. However, insufficient medical text precision and the modal disparity between text and vision spaces pose challenges for such paradigm. We show analytically...

---

## 162. Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training

**Authors:** Jinxia Yang, Bing Su, Xin Zhao, Ji-Rong Wen

**Year:** 2024 | **Venue:** ICML 2024 | **Citations:** N/A | **Score:** 0.298

[PDF](https://openreview.net/pdf?id=87ZrVHDqmR) | > Medical vision-language pre-training methods mainly leverage the correspondence between paired medical images and radiological reports. Although multi-view spatial images and temporal sequences of image-report pairs are available in off-the-shelf multi-modal medical datasets, most existing methods have not thoroughly tapped into such extensive supervision signals. In this paper, we introduce the M...

---

## 163. Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback

**Authors:** Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, Jonghyun Choi

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.337

[PDF](https://aclanthology.org/2024.acl-long.52.pdf) | > Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). Previous approaches for VLMMs involve Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and additional learnable parameters. Here, aligning video with text, and vice versa, remains a challenge, primarily due to the insufficient qua...

---

## 164. II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models

**Authors:** Ziqiang Liu, Feiteng Fang, Xi Feng, Xeron Du, Chenhao Zhang

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.284

[PDF](https://openreview.net/pdf?id=iEN2linUr8) | > The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks. In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs. However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs. To fill this gap,...

---

## 165. MCF: Mutual Correction Framework for Semi-Supervised Medical Image Segmentation

**Authors:** Yongchao Wang, Bin Xiao, Xiuli Bi, Weisheng Li, Xinbo Gao

**Year:** 2023 | **Venue:** CVPR 2023 | **Citations:** N/A | **Score:** 0.274

[PDF](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MCF_Mutual_Correction_Framework_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf) | > Semi-supervised learning is a promising method for medical image segmentation under limited annotation. However, the model cognitive bias impairs the segmentation performance, especially for edge regions. Furthermore, current mainstream semi-supervised medical image segmentation (SSMIS) methods lack designs to handle model bias. The neural network has a strong learning ability, but the cognitive b...

---

## 166. Retrieval-Augmented Multimodal Language Modeling

**Authors:** Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec

**Year:** 2023 | **Venue:** ICML 2023 | **Citations:** N/A | **Score:** 0.280

[PDF](https://openreview.net/pdf?id=VZ8bs0fwoO) | > Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all their knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a r...

---

## 167. MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation

**Authors:** Kunpeng Song*, Yizhe Zhu*, Bingchen Liu*, Qing Yan*, Ahmed Elgammal*

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.284

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05653.pdf) | > "In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), ...

---

## 168. Self-Training Large Language and Vision Assistant for Medical Question Answering

**Authors:** Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, Zhiqiang Tao

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.319

[PDF](https://aclanthology.org/2024.emnlp-main.1119.pdf) | > Large Vision-Language Models (LVLMs) have shown significant potential in assisting medical diagnosis by leveraging extensive biomedical datasets. However, the advancement of medical image understanding and reasoning critically depends on building high-quality visual instruction data, which is costly and labor-intensive to obtain, particularly in the medical domain. To mitigate this data-starving i...

---

## 169. Idea2Img: Iterative Self-Refinement with GPT-4V for Automatic Image Design and Generation

**Authors:** Zhengyuan Yang*, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.288

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05515.pdf) | > "We introduce “Idea to Image,”1 an agent system that enables multimodal iterative self-refinement with for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We inv...

---

## 170. Advancing Medical Image Segmentation via Self-supervised Instance-adaptive Prototype Learning

**Authors:** Guoyan Liang, Qin Zhou, Jingyuan Chen, Zhe Wang, Chang Yao

**Year:** 2024 | **Venue:** IJCAI 2024 | **Citations:** N/A | **Score:** 0.288

[PDF](https://www.ijcai.org/proceedings/2024/0117.pdf) | > Medical Image Segmentation (MIS) plays a crucial role in medical therapy planning and robot navigation. Prototype learning methods in MIS focus on generating segmentation masks through pixel-to-prototype comparison. However, current approaches often overlook sample diversity by using a fixed prototype per semantic class and neglect intra-class variation within each input. In this paper, we propose...

---

## 171. Generating Images with Multimodal Language Models

**Authors:** Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov

**Year:** 2023 | **Venue:** NIPS 2023 | **Citations:** N/A | **Score:** 0.271

[PDF](https://openreview.net/pdf?id=Uczck6TlSZ) | > We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to...

---

## 172. Kosmos-G: Generating Images in Context with Multimodal Large Language Models

**Authors:** Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.287

[PDF](https://openreview.net/pdf?id=he6mX9LTyE) | > Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of "image as a foreign language in image generation." This paper presents Kosmos-G, a model ...

---

## 173. PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation

**Authors:** Haibo Jin, Haoxuan Che, Yi Lin, Hao Chen

**Year:** 2024 | **Venue:** AAAI 2024 | **Citations:** N/A | **Score:** 0.287

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/28038/28087) | > Automatic medical report generation (MRG) is of great research value as it has the potential to relieve radiologists from the heavy burden of report writing. Despite recent advancements, accurate MRG remains challenging due to the need for precise clinical understanding and disease identification. Moreover, the imbalanced distribution of diseases makes the challenge even more pronounced, as rare d...

---

## 174. FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling

**Authors:** Yu Tian, Min Shi, Yan Luo, Ava Kouhana, Tobias Elze

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.295

[PDF](https://openreview.net/pdf?id=qNrJJZAKI3) | > Fairness in artificial intelligence models has gained significantly more attention in recent years, especially in the area of medicine, as fairness in medical models is critical to people's well-being and lives. High-quality medical fairness datasets are needed to promote fairness learning research. Existing medical fairness datasets are all for classification tasks, and no fairness datasets are a...

---

## 175. Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model

**Authors:** Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.286

[PDF](https://aclanthology.org/2024.emnlp-main.1072.pdf) | > Although most current large multimodal models (LMMs) can already understand photos of natural scenes and portraits, their understanding of abstract images, e.g., charts, maps, or layouts, and visual reasoning capabilities remains quite rudimentary. They often struggle with simple daily tasks, such as reading time from a clock, understanding a flowchart, or planning a route using a road map. In lig...

---

## 176. MMTN: Multi-Modal Memory Transformer Network for Image-Report Consistent Medical Report Generation

**Authors:** Yiming Cao, Lizhen Cui, Lei Zhang, Fuqiang Yu, Zhen Li

**Year:** 2023 | **Venue:** AAAI 2023 | **Citations:** N/A | **Score:** 0.303

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/25100/24872) | > Automatic medical report generation is an essential task in applying artificial intelligence to the medical domain, which can lighten the workloads of doctors and promote clinical automation. The state-of-the-art approaches employ Transformer-based encoder-decoder architectures to generate reports for medical images. However, they do not fully explore the relationships between multi-modal medical ...

---

## 177. Dense Multimodal Alignment for Open-Vocabulary 3D Scene Understanding

**Authors:** Ruihuang Li*, Zhengqiang ZHANG, Chenhang He, Zhiyuan Ma, Vishal Patel

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.282

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06612.pdf) | > "Recent vision-language pre-training models have exhibited remarkable generalization ability in zero-shot recognition tasks. Previous open-vocabulary 3D scene understanding methods mostly focus on training 3D models using either image or text supervision while neglecting the collective strength of all modalities. In this work, we propose a Dense Multimodal Alignment (DMA) framework to densely co-e...

---

## 178. CoDi-2: In-Context Interleaved and Interactive Any-to-Any Generation

**Authors:** Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.294

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_CoDi-2_In-Context_Interleaved_and_Interactive_Any-to-Any_Generation_CVPR_2024_paper.pdf) | > We present CoDi-2 a Multimodal Large Language Model (MLLM) for learning in-context interleaved multimodal representations. By aligning modalities with language for both encoding and generation CoDi-2 empowers Large Language Models (LLMs) to understand modality-interleaved instructions and in-context examples and autoregressively generate grounded and coherent multimodal outputs in an any-to-any in...

---

## 179. VLIS: Unimodal Language Models Guide Multimodal Language Generation

**Authors:** Jiwan Chung, Youngjae Yu

**Year:** 2023 | **Venue:** EMNLP 2023 | **Citations:** N/A | **Score:** 0.266

> Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of visio...

---

## 180. Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation

**Authors:** Wenyu Guo, Qingkai Fang, Dong Yu, Yang Feng

**Year:** 2023 | **Venue:** EMNLP 2023 | **Citations:** N/A | **Score:** 0.259

> Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions com...

---

## 181. Knowledge-Aware Reasoning over Multimodal Semi-structured Tables

**Authors:** Suyash Vardhan Mathur, Jainit Sushil Bafna, Kunal Kartik, Harshita Khandelwal, Manish Shrivastava

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.347

[PDF](https://aclanthology.org/2024.findings-emnlp.822.pdf) | > Existing datasets for tabular question answering typically focus exclusively on text within cells. However, real-world data is inherently multimodal, often blending images such as symbols, faces, icons, patterns, and charts with textual content in tables. With the evolution of AI models capable of multimodal reasoning, it is pertinent to assess their efficacy in handling such structured data. This...

---

## 182. Intersection of Artificial Intelligence and Medical Education (Student Abstract)

**Authors:** Keefer P. Wu, Patricia C. Tsang

**Year:** 2024 | **Venue:** AAAI 2024 | **Citations:** N/A | **Score:** 0.328

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/30525/32676) | > Can advanced AI-driven technologies transform the traditionally arduous educational process in medicine? This study takes a deep dive into how the publicly available OpenAI ChatGPT-3.5 performs in answering board-style questions designed for physicians training to become pathologists. Correctly answering 75% of 543 questions using an engaging and fast-paced format was an impressive performance. It...

---

## 183. PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation

**Authors:** Ning Gao, Sanping Zhou*, Le Wang, Nanning Zheng

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.288

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08535.pdf) | > "Semi-supervised learning has emerged as a widely adopted technique in the field of medical image segmentation. The existing works either focuses on the construction of consistency constraints or the generation of pseudo labels to provide high-quality supervisory signals, whose main challenge mainly comes from how to keep the continuous improvement of model capabilities. In this paper, we propose ...

---

## 184. MoVA: Adapting Mixture of Vision Experts to Multimodal Context

**Authors:** Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.285

[PDF](https://openreview.net/pdf?id=uHs6RJFDsg) | > As the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM's understanding on diverse image content. Although some large-scale pretrained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understandi...

---

## 185. Fighting against Organized Fraudsters Using Risk Diffusion-based Parallel Graph Neural Network

**Authors:** Jiacheng Ma, Fan Li, Rui Zhang, Zhikang Xu, Dawei Cheng

**Year:** 2023 | **Venue:** IJCAI 2023 | **Citations:** N/A | **Score:** 0.263

[PDF](https://www.ijcai.org/proceedings/2023/0681.pdf) | > Medical insurance plays a vital role in modern society, yet organized healthcare fraud causes billions of dollars in annual losses, severely harming the sustainability of the social welfare system. Existing works mostly focus on detecting individual fraud entities or claims, ignoring hidden conspiracy patterns. Hence, they face severe challenges in tackling organized fraud. In this paper, we propo...

---

## 186. SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models

**Authors:** Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.279

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_SmartEdit_Exploring_Complex_Instruction-based_Image_Editing_with_Multimodal_Large_Language_CVPR_2024_paper.pdf) | > Current instruction-based image editing methods such as InstructPix2Pix often fail to produce satisfactory results in complex scenarios due to their dependence on the simple CLIP text encoder in diffusion models. To rectify this this paper introduces SmartEdit a novel approach of instruction-based image editing that leverages Multimodal Large Language Models (MLLMs) to enhance its understanding an...

---

## 187. JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images

**Authors:** Zhecan Wang, Junzhang Liu, Chia-Wei Tang, Hani Alomari, Anushka Sivakumar

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.287

[PDF](https://openreview.net/pdf?id=mZLlWaoeKq) | > Existing vision-language understanding benchmarks largely consist of images of objects in their usual contexts.
As a consequence, recent multimodal large language models can perform well with only a shallow visual understanding by relying on background language biases. Thus, strong performance on these benchmarks does not necessarily correlate with strong visual understanding. In this paper, we re...

---

## 188. CLIPPO: Image-and-Language Understanding From Pixels Only

**Authors:** Michael Tschannen, Basil Mustafa, Neil Houlsby

**Year:** 2023 | **Venue:** CVPR 2023 | **Citations:** N/A | **Score:** 0.264

[PDF](https://openaccess.thecvf.com/content/CVPR2023/papers/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.pdf) | > Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a ...

---

## 189. TIP: Tabular-Image Pre-training for Multimodal Classification with Incomplete Data

**Authors:** Siyi Du*, Shaoming Zheng, Yinsong Wang, Wenjia Bai, Declan P. O'Regan

**Year:** 2024 | **Venue:** ECCV 2024 | **Citations:** N/A | **Score:** 0.294

[PDF](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02403.pdf) | > "Images and structured tables are essential parts of real-world databases. Though tabular-image representation learning is promising for creating new insights, it remains a challenging task, as tabular data is typically heterogeneous and incomplete, presenting significant modality disparities with images. Earlier works have mainly focused on simple modality fusion strategies in complete data scena...

---

## 190. INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis

**Authors:** Shih-Cheng Huang, Zepeng Huo, Ethan Steinberg, Chia-Chun Chiang, Matthew P. Lungren

**Year:** 2023 | **Venue:** NIPS 2023 | **Citations:** N/A | **Score:** 0.273

[PDF](https://openreview.net/pdf?id=3sRR2u72oQ) | > Synthesizing information from various data sources plays a crucial role in the practice of modern medicine. Current applications of artificial intelligence in medicine often focus on single-modality data due to a lack of publicly available, multimodal medical datasets. To address this limitation, we introduce INSPECT, which contains de-identified longitudinal records from a large cohort of pulmona...

---

## 191. ***YesBut***: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models

**Authors:** Abhilash Nandy, Yash Agarwal, Ashish Patwa, Millon Madhur Das, Aman Bansal

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.278

[PDF](https://aclanthology.org/2024.emnlp-main.937.pdf) | > Understanding satire and humor is a challenging task for even current Vision-Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the reason behind the image being satirical), and Completion (given one half of the image, selecting the other half from 2 given options, such that the complete...

---

## 192. ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts

**Authors:** Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai

**Year:** 2024 | **Venue:** CVPR 2024 | **Citations:** N/A | **Score:** 0.285

[PDF](https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.pdf) | > While existing large vision-language multimodal models focus on whole image understanding there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge we introduce a novel multimodal model capable of decoding arbitrary (free-fo...

---

## 193. Deep Learning for Medical Prediction in Electronic Health Records

**Authors:** Xinlu Zhang

**Year:** 2023 | **Venue:** AAAI 2023 | **Citations:** N/A | **Score:** 0.272

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/26933/26705) | > The widespread adoption of electronic health records (EHRs) has opened up new opportunities for using deep neural networks to enhance healthcare. However, modeling EHR data can be challenging due to its complex properties, such as missing values, data scarcity in multi-hospital systems, and multimodal irregularity. How to tackle various issues in EHRs for improving medical prediction is challengin...

---

## 194. Decoding the Underlying Meaning of Multimodal Hateful Memes

**Authors:** Ming Shan Hee, Wen-Haw Chong, Roy Ka-Wei Lee

**Year:** 2023 | **Venue:** IJCAI 2023 | **Citations:** N/A | **Score:** 0.261

[PDF](https://www.ijcai.org/proceedings/2023/0665.pdf) | > Recent studies have proposed models that yielded promising performance for the hateful meme classification task. Nevertheless, these proposed models do not generate interpretable explanations that uncover the underlying meaning and support the classification output. A major reason for the lack of explainable hateful meme methods is the absence of a hateful meme dataset that contains ground truth e...

---

## 195. Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding

**Authors:** Lorenzo Jaime Yu Flores, Heyuan Huang, Kejian Shi, Sophie Chheang, Arman Cohan

**Year:** 2023 | **Venue:** EMNLP 2023 | **Citations:** N/A | **Score:** 0.281

> Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to ...

---

## 196. A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding

**Authors:** Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer

**Year:** 2023 | **Venue:** EMNLP 2023 | **Citations:** N/A | **Score:** 0.258

> Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept in existing datasets: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia...

---

## 197. M3AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset

**Authors:** Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.289

[PDF](https://aclanthology.org/2024.acl-long.489.pdf) | > Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them suppo...

---

## 198. MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models

**Authors:** Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.319

[PDF](https://openreview.net/pdf?id=cFyagd2Yh4) | > As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to ev...

---

## 199. DocMSU: A Comprehensive Benchmark for Document-Level Multimodal Sarcasm Understanding

**Authors:** Hang Du, Guoshun Nan, Sicheng Zhang, Binzhu Xie, Junrui Xu

**Year:** 2024 | **Venue:** AAAI 2024 | **Citations:** N/A | **Score:** 0.277

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/29748/31288) | > Multimodal Sarcasm Understanding (MSU) has a wide range of applications in the news field such as public opinion analysis and forgery detection. 
However, existing MSU benchmarks and approaches usually focus on sentence-level MSU. 
In document-level news, sarcasm clues are sparse or small and are often concealed in long text. 
Moreover, compared to sentence-level comments like tweets, which mainly...

---

## 200. Safety of Multimodal Large Language Models on Images and Text

**Authors:** Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao

**Year:** 2024 | **Venue:** IJCAI 2024 | **Citations:** N/A | **Score:** 0.277

[PDF](https://www.ijcai.org/proceedings/2024/0901.pdf) | > Attracted by the impressive power of Multimodal Large Language Models (MLLMs), the public is increasingly utilizing them to improve the efficiency of daily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios. In this paper, we systematically survey current efforts on the evaluation, attack, and defens...

---

## 201. Guiding Instruction-based Image Editing via Multimodal Large Language Models

**Authors:** Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang

**Year:** 2024 | **Venue:** ICLR 2024 | **Citations:** N/A | **Score:** 0.277

[PDF](https://openreview.net/pdf?id=S1RKWSyZ2Y) | > Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation ...

---

## 202. SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages

**Authors:** Holy Lovenia, Rahmad Mahendra, Salsabil Maulana Akbar, Lester James Validad Miranda, Jennifer Santoso

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.317

[PDF](https://aclanthology.org/2024.emnlp-main.296.pdf) | > Southeast Asia (SEA) is a region rich in linguistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. However, prevailing AI models suffer from a significant lack of representation of texts, images, and audio datasets from SEA, compromising the quality of AI models for SEA languages. Evaluating models for SEA languages is challenging due t...

---

## 203. GEM: Generating Engaging Multimodal Content

**Authors:** Chongyang Gao, Yiren Jian, Natalia Denisenko, Soroush Vosoughi, V. S. Subrahmanian

**Year:** 2024 | **Venue:** IJCAI 2024 | **Citations:** N/A | **Score:** 0.281

[PDF](https://www.ijcai.org/proceedings/2024/0847.pdf) | > Generating engaging multimodal content is a key objective in numerous applications, such as the creation of online advertisements that captivate user attention through a synergy of images and text. In this paper, we introduce GEM, a novel framework engineered for the generation of engaging multimodal image-text posts. The GEM framework operates in two primary phases. Initially, GEM integrates a pr...

---

## 204. Multimodal Propaganda Processing

**Authors:** Vincent Ng, Shengjie Li

**Year:** 2023 | **Venue:** AAAI 2023 | **Citations:** N/A | **Score:** 0.279

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/26792/26564) | > Propaganda campaigns have long been used to influence public opinion via disseminating biased and/or misleading information. Despite the increasing prevalence of propaganda content on the Internet, few attempts have been made by AI researchers to analyze such content. We introduce the task of multimodal propaganda processing, where the goal is to automatically analyze propaganda content. We believ...

---

## 205. Instruction-Guided Visual Masking

**Authors:** Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li

**Year:** 2024 | **Venue:** NIPS 2024 | **Citations:** N/A | **Score:** 0.292

[PDF](https://openreview.net/pdf?id=cA9gLXFaRo) | > Instruction following is crucial in contemporary LLM. However, when extended to multimodal setting, it often suffers from misalignment between specific textual instruction and targeted local region of an image. To achieve more accurate and nuanced multimodal instruction following, we introduce Instruction-guided Visual Masking (IVM), a new versatile visual grounding model that is compatible with d...

---

## 206. On AI-Assisted Pneumoconiosis Detection from Chest X-rays

**Authors:** Yasmeena Akhter, Rishabh Ranjan, Richa Singh, Mayank Vatsa, Santanu Chaudhury

**Year:** 2023 | **Venue:** IJCAI 2023 | **Citations:** N/A | **Score:** 0.270

[PDF](https://www.ijcai.org/proceedings/2023/0705.pdf) | > According to theWorld Health Organization, Pneumoconiosis
affects millions of workers globally,
with an estimated 260,000 deaths annually. The
burden of Pneumoconiosis is particularly high in
low-income countries, where occupational safety
standards are often inadequate, and the prevalence
of the disease is increasing rapidly. The reduced
availability of expert medical care in rural areas,
...

---

## 207. i-Code Studio: A Configurable and Composable Framework for Integrative AI

**Authors:** Yuwei Fang, Mahmoud Khademi, Chenguang Zhu, Ziyi Yang, Reid Pryzant

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.303

[PDF](https://aclanthology.org/2024.emnlp-demo.2.pdf) | > Artificial General Intelligence (AGI) requires comprehensive understanding and generation capabilities for a variety of tasks spanning different modalities and functionalities. Integrative AI is one important direction to approach AGI, through combining multiple models to tackle complex multimodal tasks. However, there is a lack of a flexible and composable platform to facilitate efficient and eff...

---

## 208. RaTEScore: A Metric for Radiology Report Generation

**Authors:** Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.304

[PDF](https://aclanthology.org/2024.emnlp-main.836.pdf) | > This paper introduces a novel, entity-aware metric, termed as Radiological Report (Text) Evaluation (RaTEScore), to assess the quality of medical reports generated by AI models. RaTEScore emphasizes crucial medical entities such as diagnostic outcomes and anatomical details, and is robust against complex medical synonyms and sensitive to negation expressions. Technically, we developed a comprehens...

---

## 209. FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models

**Authors:** Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, Fenglong Ma

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.303

[PDF](https://aclanthology.org/2024.emnlp-main.464.pdf) | > Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To add...

---

## 210. PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology

**Authors:** Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Lin Sun

**Year:** 2024 | **Venue:** AAAI 2024 | **Citations:** N/A | **Score:** 0.298

[PDF](https://ojs.aaai.org/index.php/AAAI/article/view/28308/28605) | > As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To ...

---

## 211. Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats

**Authors:** Lucia Gordon, Nikhil Behari, Samuel Collier, Elizabeth Bondi-Kelly, Jackson A. Killian

**Year:** 2023 | **Venue:** IJCAI 2023 | **Citations:** N/A | **Score:** 0.250

[PDF](https://www.ijcai.org/proceedings/2023/0663.pdf) | > Much of Earth's charismatic megafauna is endangered by human activities, particularly the rhino, which is at risk of extinction due to the poaching crisis in Africa. Monitoring rhinos' movement is crucial to their protection but has unfortunately proven difficult because rhinos are elusive. Therefore, instead of tracking rhinos, we propose the novel approach of mapping communal defecation sites, c...

---

## 212. AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding

**Authors:** Alessandro Suglia, Claudio Greco, Katie Baker, Jose L. Part, Ioannis Papaioannou

**Year:** 2024 | **Venue:** EMNLP 2024 | **Citations:** N/A | **Score:** 0.291

[PDF](https://aclanthology.org/2024.findings-emnlp.649.pdf) | > AI personal assistants deployed via robots or wearables require embodied understanding to collaborate with humans effectively. However, current Vision-Language Models (VLMs) primarily focus on third-person view videos, neglecting the richness of egocentric perceptual experience. To address this gap, we propose three key contributions. First, we introduce the Egocentric Video Understanding Dataset ...

---

## 213. UniS-MMC: Learning Unimodality-supervised Multimodal Contrastive Representations

**Authors:** Heqing Zou, Meng Shen, Chen Chen, Yuchen Hu, Deepu Rajan

**Year:** 2023 | **Venue:** ICLR 2023 | **Citations:** N/A | **Score:** 0.275

> Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for final decisions.  
However, just like a human's final decision can be confused by specific erroneous information from the environment, current multimodal learning methods also suffer from uncertain unimodal prediction when learning multimodal representations. In this work, we propose ...

---

## 214. Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development

**Authors:** Pranab Sahoo, Ayush Singh, Sriparna Saha, Aman Chadha, Samrat Mondal

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.275

[PDF](https://aclanthology.org/2024.findings-acl.667.pdf) | > The mining of adverse drug events (ADEs) is pivotal in pharmacovigilance, enhancing patient safety by identifying potential risks associated with medications, facilitating early detection of adverse events, and guiding regulatory decision-making. Traditional ADE detection methods are reliable but slow, not easily adaptable to large-scale operations, and offer limited information. With the exponent...

---

## 215. Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality

**Authors:** Jiahuan Pei, Irene Viola, Haochen Huang, Junxiao Wang, Moonisa Ahsan

**Year:** 2024 | **Venue:** ACL 2024 | **Citations:** N/A | **Score:** 0.300

[PDF](https://aclanthology.org/2024.findings-acl.240.pdf) | > Autonomous artificial intelligence (AI) agents have emerged as promising protocols for automatically understanding the language-based environment, particularly with the exponential development of large language models (LLMs). However, a fine-grained, comprehensive understanding of multimodal environments remains under-explored. This work designs an autonomous workflow tailored for integrating AI a...

---

## 216. Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning

**Authors:** Lasa Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 47 | **Score:** 0.285

[DOI](https://doi.org/10.48550/arXiv.2506.07044)

> Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing...

---

## 217. MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant

**Authors:** Chenlu Zhan, Yu Lin, Gaoang Wang, Hongwei Wang, Jian Wu

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 30 | **Score:** 0.268

[PDF](http://arxiv.org/pdf/2403.04290) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01093)

> Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for dis-tinct medical tasks and are restricted to inadequate medi-cal multimodal knowledge, constraining medical compre-hensive diagnosis. In this paper, we propose MedM2G, a Medic...

---

## 218. UniMedVL: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis

**Authors:** Junzhi Ning (Raymond) Ning, Wei Li, Cheng Tang, Jiashi Lin, Chenglong Ma

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 2 | **Score:** 0.265

[DOI](https://doi.org/10.48550/arXiv.2510.15710)

> Medical diagnostic applications require models that can process multimodal medical inputs (images, patient histories, lab results) and generate diverse outputs including both textual reports and visual content (annotations, segmentation masks, and images). Despite this need, existing medical AI systems disrupt this unified process: medical image understanding models interpret images but cannot gen...

---

## 219. SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation

**Authors:** Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 229 | **Score:** 0.265

[DOI](https://doi.org/10.48550/arXiv.2404.14396)

> The rapid evolution of multimodal foundation model has demonstrated significant progresses in vision-language understanding and generation, e.g., our previous work SEED-LLaMA. However, there remains a gap between its capability and the real-world applicability, primarily due to the model's limited capacity to effectively respond to various user instructions and interact with diverse visual data. I...

---

## 220. AI agents

**Authors:** Association for Artificial Intelligence 2025, Kasirzadeh, Atoosa, Rossi, Francesca, Talamadupula, Kartik, Wooldridge, Michael

**Year:** 2025 | **Venue:** Underline Science Inc. | **Citations:** N/A | **Score:** 0.254

[PDF](https://doi.org/10.48448/zat6-pn10) | [DOI](https://doi.org/10.48448/zat6-pn10)

> Agents and multi-agent systems (MAS) have evolved from autonomous problem-solving entities to integrating generative AI and LLMs, ultimately leading to cooperative AI frameworks that enhance adaptability, scalability, and collaboration.<br><br> Main Takeaways * Multi-agent systems have evolved from rule-based autonomy to cooperative AI, emphasizing collaboration, negotiation, and ethical alignment...

---

## 221. MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation

**Authors:** Junjie Yang, Yuhao Yan, Gang Wu, Yuxuan Wang, Rachel Liang

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.233

[PDF](https://arxiv.org/pdf/2511.13135) | [DOI](https://doi.org/10.48550/arxiv.2511.13135)

> As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on amb...

---

## 222. Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis

**Authors:** Praveenbalaji Rajendran, Mojtaba Safari, Wenfeng He, Mingzhe Hu, Shansong Wang

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.229

[PDF](https://arxiv.org/pdf/2510.16973) | [DOI](https://doi.org/10.48550/arxiv.2510.16973)

> Recent advancements in artificial intelligence (AI), particularly foundation models (FMs), have revolutionized medical image analysis, demonstrating strong zero- and few-shot performance across diverse medical imaging tasks, from segmentation to report generation. Unlike traditional task-specific AI models, FMs leverage large corpora of labeled and unlabeled multimodal datasets to learn generalize...

---

## 223. Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning

**Authors:** Guoxin Wang, Jun Zhao, Xinyi Liu, Yanbo Liu, Xudong Cao

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.226

[PDF](https://arxiv.org/pdf/2509.19090) | [DOI](https://doi.org/10.48550/arxiv.2509.19090)

> Medical imaging provides critical evidence for clinical diagnosis, treatment planning, and surgical decisions, yet most existing imaging models are narrowly focused and require multiple specialized networks, limiting their generalization. Although large-scale language and multimodal models exhibit strong reasoning and multi-task capabilities, real-world clinical applications demand precise visual ...

---

## 224. Multimodal fusion: advancing medical visual question-answering

**Authors:** Anjali Mudgal, Udbhav Kush, Aditya Kumar, Amir Jafari

**Year:** 2024 | **Venue:** Neural computing & applications (Print) | **Citations:** 3 | **Score:** 0.224

[DOI](https://doi.org/10.1007/s00521-024-10318-8)

> ...

---

## 225. A Unified Agentic Framework for Medical VLMs: Case Retrieval, Reporting, and Visual Question Answering

**Authors:** Andy Xu Wang

**Year:** 2025 | **Venue:** Highlights in Science Engineering and Technology | **Citations:** N/A | **Score:** 0.221

[PDF](https://datahset.com/index.php/ojs/article/download/71/64) | [DOI](https://doi.org/10.54097/swf82j79)

> Clinical decision-making often requires physicians to iteratively generate hypotheses, gather multimodal evidence, and refine diagnoses under conditions of time pressure and information overload. While medical vision–language models (VLMs) have shown promise in visual question answering, report generation, and image-based diagnosis, most remain standalone tools without integration into broader dia...

---

## 226. TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation

**Authors:** Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 107 | **Score:** 0.221

[DOI](https://doi.org/10.1109/CVPR52734.2025.00243)

> We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation. Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ) encoder for unifying these two tasks. We observe that understanding and generation require fundamentally different granularities of visual information. This leads to a cri...

---

## 227. A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning

**Authors:** Liuyi Jin, Runzhi Wang, Sang‐Woo Lee, Radu Stoleru, Michael Middleton

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.218

[PDF](https://arxiv.org/pdf/2511.13078) | [DOI](https://doi.org/10.48550/arxiv.2511.13078)

> Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates t...

---

## 228. Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining

**Authors:** Bingqian Lin, Zicong Chen, Mingjie Li, Haokun Lin, Hang Xu

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 17 | **Score:** 0.215

[PDF](http://arxiv.org/pdf/2304.14204) | [DOI](https://doi.org/10.48550/arXiv.2304.14204)

> Medical artificial general intelligence (MAGI) enables one foundation model to solve different medical tasks, which is very practical in the medical domain. It can significantly reduce the requirement of large amounts of task-specific data by sufficiently sharing medical knowledge among different tasks. However, due to the challenges of designing strongly generalizable models with limited and comp...

---

## 229. Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding

**Authors:** Songtao Jiang, Yuhui Yuan, Sibo Song, Tianxiang Hu, Chenyi Zhou

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.213

[PDF](https://arxiv.org/pdf/2510.08668) | [DOI](https://doi.org/10.48550/arxiv.2510.08668)

> Real-world clinical decision-making requires integrating heterogeneous data, including medical text, 2D images, 3D volumes, and videos, while existing AI systems fail to unify all these signals, limiting their utility. In this paper, we introduce Hulu-Med, a transparent, generalist medical Vision-Language Model (VLM) designed to unify language-only, 2D/3D vision-language, and video understanding w...

---

## 230. SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding

**Authors:** Tae-Min Choi, Tae Kyeong Jeong, Garam Kim, Jaemin Lee, Yeongyoon Koh

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.210

> Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimoda...

---

## 231. Vision-Language Transformer Framework for Automated Medical Imaging Reporting

**Authors:** Viet-Hang Duong, Minh-Quan Tran, Nguyen-Khang Nguyen, Duc-Hoang Pham, Manh-Quan Bui

**Year:** 2025 | **Venue:** International Conference on Multimedia Analysis and Pattern Recognition | **Citations:** N/A | **Score:** 0.210

[DOI](https://doi.org/10.1109/MAPR67746.2025.11133782)

> Automated medical imaging report generation is an emerging challenge in clinical AI, aiming to bridge visual data interpretation with natural language understanding. While conventional approaches typically rely on Convolutional Neural Networks (CNNs) for visual feature extraction, recent progress in vision transformers has introduced more powerful alternatives for representation learning. In this ...

---

## 232. MedQuery: A Graph-Driven Medical Literature-Enhanced Query Answering System

**Authors:** Chenhan Fu, Yuanfang Xia, Guoming Wang, Rongxing Lu, Siliang Tang

**Year:** 2025 | **Venue:** International Conference on Multimedia Retrieval | **Citations:** 1 | **Score:** 0.209

[DOI](https://doi.org/10.1145/3731715.3733383)

> ...

---

## 233. Qualitative Research in an Era of AI: A Pragmatic Approach to Data Analysis, Workflow, and Computation

**Authors:** Corey M. Abramson, Zhuofan Li, Tara Prendergast, Daniel Dohan

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.208

[PDF](https://osf.io/7bsgy_v1/download) | [DOI](https://doi.org/10.31235/osf.io/7bsgy_v1)

> Rapid computational developments—particularly the proliferation of artificial intelligence (AI)—increasingly shape social scientific research while raising new questions about in-depth qualitative methods such as ethnography and interviewing. Building on classic debates about using computers to analyze qualitative data, we revisit longstanding concerns and assess possibilities and dangers in an er...

---

## 234. MedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data understanding and generation

**Authors:** Lijian Xu, Hao Sun, Ziyu Ni, Hongsheng Li, Shaoting Zhang

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 6 | **Score:** 0.206

[DOI](https://doi.org/10.48550/arXiv.2409.19684)

> Medicine is inherently multimodal and multitask, with diverse data modalities spanning text, imaging. However, most models in medical field are unimodal single tasks and lack good generalizability and explainability. In this study, we introduce MedViLaM, a unified vision-language model towards a generalist model for medical data that can flexibly encode and interpret various forms of medical data,...

---

## 235. MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations

**Authors:** Ziyang Zhang, Yang Yu, Yucheng Chen, Xulei Yang, Si Yong Yeo

**Year:** 2025 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 9 | **Score:** 0.206

[DOI](https://doi.org/10.1109/CVPR52734.2025.02769)

> Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content. This gap hinders the model’s ability to synthesize coherent and novel visual representations from textual prompts, thereby reducing the effectiveness of multi-modal learn...

---

## 236. IMGEF: integrated multimodal graph-enhanced framework for radiology report generation

**Authors:** Muhammad Usman, Xiaodi Hou, Yi Guo, Zonglin Liang, Yijia Zhang

**Year:** 2025 | **Venue:** Multimedia Systems | **Citations:** 1 | **Score:** 0.205

[DOI](https://doi.org/10.1007/s00530-025-01858-7)

> ...

---

## 237. Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning

**Authors:** Xiaotian Zhang, Yuan Wang, Zhaopeng Feng, Ruizhe Chen, Zhijie Zhou

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 4 | **Score:** 0.204

[DOI](https://doi.org/10.48550/arXiv.2506.12307)

> Medical Question-Answering (QA) encompasses a broad spectrum of tasks, including multiple choice questions (MCQ), open-ended text generation, and complex computational reasoning. Despite this variety, a unified framework for delivering high-quality medical QA has yet to emerge. Although recent progress in reasoning-augmented large language models (LLMs) has shown promise, their ability to achieve ...

---

## 238. MMORE: Massive Multimodal Open RAG & Extraction

**Authors:** Alexandre Sallinen, Stefan Krsteski, Paul Teiletche, Marc-Antoine Allard, Baptiste Lecoeur

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** N/A | **Score:** 0.204

[DOI](https://doi.org/10.48550/arXiv.2509.11937)

> We introduce MMORE, an open-source pipeline for Massive Multimodal Open RetrievalAugmented Generation and Extraction, designed to ingest, transform, and retrieve knowledge from heterogeneous document formats at scale. MMORE supports more than fifteen file types, including text, tables, images, emails, audio, and video, and processes them into a unified format to enable downstream applications for ...

---

## 239. Developing a general AI model for integrating diverse genomic modalities and comprehensive genomic knowledge

**Authors:** Zhenhao Zhang, Xinyu Bao, Linghua Jiang, Xin Luo, Yichun Wang

**Year:** 2025 | **Venue:** bioRxiv | **Citations:** 1 | **Score:** 0.204

[PDF](https://doi.org/10.1101/2025.05.08.652986) | [DOI](https://doi.org/10.1101/2025.05.08.652986)

> Advances in next-generation sequencing technologies have vastly expanded the availability of diverse genomic, epigenomic and transcriptomic data, presenting the opportunity to develop a general AI model that integrates comprehensive genomic knowledge into a unified model. Unlike previous predictive models, which are typically specialized to certain tasks, our general AI model unifies a wide range ...

---

## 240. Overview of MiReportor: Generating Reports for Multimodal Medical Images

**Authors:** Xuwen Wang, Hetong Ma, Zhen Guo, Jiao Li

**Year:** 2023 | **Venue:** International Conference on Natural Language Generation | **Citations:** N/A | **Score:** 0.204

> ...

---

## 241. A "Wenlu" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge

**Authors:** Liang Geng

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** N/A | **Score:** 0.204

[DOI](https://doi.org/10.48550/arXiv.2506.00570)

> With the rapid penetration of artificial intelligence across industries and scenarios, a key challenge in building the next-generation intelligent core lies in effectively integrating the language understanding capabilities of foundation models with domain-specific knowledge bases in complex real-world applications. This paper proposes a multimodal cognition and embodied decision-making brain syst...

---

## 242. U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding

**Authors:** Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 4 | **Score:** 0.203

[DOI](https://doi.org/10.48550/arXiv.2505.17779)

> Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We i...

---

## 243. MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata

**Authors:** Yihao Liu, Chenyu Gao, Lianrui Zuo, Michael E. Kim, Brian D. Boyd

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.202

[PDF](https://arxiv.org/pdf/2512.10041) | [DOI](https://doi.org/10.48550/arxiv.2512.10041)

> Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling frame...

---

## 244. MIGRATING LEGACY HEALTHCARE SYSTEMS TO CLOUD-NATIVE MICROSERVICES WITH AI: BEST PRACTICES AND PITFALLS

**Authors:** Sai Rupesh Kagga

**Year:** 2025 | **Venue:** International Journal of Apllied Mathematics | **Citations:** N/A | **Score:** 0.202

[PDF](https://ijamjournal.org/ijam/publication/index.php/ijam/article/download/123/119) | [DOI](https://doi.org/10.12732/ijam.v38i2s.123)

> This paper discusses the challenge and opportunity of migrating legacy health care information systems to cloud-native microservices designs with additional artificial intelligence capabilities. Healthcare organizations have unique constraints including regulatory compliance requirements, data sensitivity concerns, and the high-value nature of the requirement for continuous availability of service...

---

## 245. Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs

**Authors:** Zheng Sun, Yi Wei, Long Yu

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 1 | **Score:** 0.202

[DOI](https://doi.org/10.48550/arXiv.2505.23265)

> Multimodal Large Language Models (MLLMs) are of great application across many domains, such as multimodal understanding and generation. With the development of diffusion models (DM) and unified MLLMs, the performance of image generation has been significantly improved, however, the study of image screening is rare and its performance with MLLMs is unsatisfactory due to the lack of data and the wee...

---

## 246. Unified multimodal conditional framework for unsupervised anomaly detection

**Authors:** Najeh Nafti, Olfa Besbes, A. Abdallah, M. Bedoui

**Year:** 2025 | **Venue:** International Journal of Data Science and Analysis | **Citations:** N/A | **Score:** 0.202

[DOI](https://doi.org/10.1007/s41060-025-00745-8)

> ...

---

## 247. Analysis of a Multimodal Ocular Ultrasound Image Classification Algorithm Based on YOLO and VanillaNet

**Authors:** Yilu Zhang, Yanzhu Zhang, Tingxue Li, Mingyu Shi, Yue Huang

**Year:** 2025 | **Venue:** IEEE Access | **Citations:** N/A | **Score:** 0.202

[DOI](https://doi.org/10.1109/ACCESS.2025.3598747)

> The generation of traditional ophthalmic ultrasound examination reports primarily relies on clinicians manually entering information during procedures. This process is not only inefficient and highly subjective but also heavily dependent on the clinician’s expertise. Training skilled ophthalmologists is time-consuming and costly, and with limited medical resources at the primary care level, diagno...

---

## 248. A Comprehensive Survey of Research Trends in mmWave Technologies for Medical Applications

**Authors:** Xiaoyu Zhang, Chuhui Liu, Yanda Cheng, Zhengxiong Li, Chenhan Xu

**Year:** 2025 | **Venue:** Italian National Conference on Sensors | **Citations:** 2 | **Score:** 0.200

[DOI](https://doi.org/10.3390/s25123706)

> Millimeter-wave (mmWave) sensing has emerged as a promising technology for non-contact health monitoring, offering high spatial resolution, material sensitivity, and integration potential with wireless platforms. While prior work has focused on specific applications or signal processing methods, a unified understanding of how mmWave signals map to clinically relevant biomarkers remains lacking. Th...

---

## 249. Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset

**Authors:** Ziye Deng, Ruihan He, Jiaxiang Liu, Yuan Wang, Zijie Meng

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 2 | **Score:** 0.200

[DOI](https://doi.org/10.48550/arXiv.2508.10528)

> Medical image grounding aims to align natural language phrases with specific regions in medical images, serving as a foundational task for intelligent diagnosis, visual question answering (VQA), and automated report generation (MRG). However, existing research is constrained by limited modality coverage, coarse-grained annotations, and the absence of a unified, generalizable grounding framework. T...

---

## 250. GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing

**Authors:** Zhenyu Wang, Aoxue Li, Zhenguo Li, Xihui Liu

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 80 | **Score:** 0.200

[DOI](https://doi.org/10.48550/arXiv.2407.05600)

> Despite the success achieved by existing image generation and editing methods, current models still struggle with complex problems including intricate text prompts, and the absence of verification and self-correction mechanisms makes the generated images unreliable. Meanwhile, a single model tends to specialize in particular tasks and possess the corresponding capabilities, making it inadequate fo...

---

## 251. Computer-aided design of personalized occlusal positioning splints using multimodal 3D data

**Authors:** Agnieszka Anna Tomaka, Leszek Luchowski, Michał Tarnawski, Dariusz Pojda

**Year:** 2025 | **Venue:** Computer Vision and Image Understanding | **Citations:** N/A | **Score:** 0.200

[PDF](https://arxiv.org/pdf/2504.12868) | [DOI](https://doi.org/10.1016/j.cviu.2025.104527)

> ...

---

## 252. Dolphin v1.0 Technical Report

**Authors:** T.-C. Weng, Chi Zhang, Chuanzhu Yan, S.Y. Liu, Xiaoyang Liu

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.199

[PDF](https://arxiv.org/pdf/2509.25748) | [DOI](https://doi.org/10.48550/arxiv.2509.25748)

> Ultrasound is crucial in modern medicine but faces challenges like operator dependence, image noise, and real-time scanning, hindering AI integration. While large multimodal models excel in other medical imaging areas, they struggle with ultrasound's complexities. To address this, we introduce Dolphin v1.0 (V1) and its reasoning-augmented version, Dolphin R1-the first large-scale multimodal ultras...

---

## 253. A User-Centric Context-Aware Framework for Real-Time Optimisation of Multimedia Data Privacy Protection, and Information Retention Within Multimodal AI Systems

**Authors:** Ndricim Topalli, Atta Badii

**Year:** 2025 | **Venue:** Preprints.org | **Citations:** 1 | **Score:** 0.198

[PDF](https://www.preprints.org/frontend/manuscript/81e473db5748b988abe160da2016056b/download_pub) | [DOI](https://doi.org/10.20944/preprints202508.2085.v2)

> The increasing use of AI systems for face, object, action, scene and emotion recognition raises significant privacy risks, particularly when processing Personally Identifiable Information (PII). Current privacy-preserving methods lack adaptability to users’ preferences and contextual requirements, and obfuscate user faces uniformly. This research proposes a user-centric, context-aware, and ontolog...

---

## 254. Artificial Intelligence in Traditional Chinese Medicine: Multimodal Fusion and Machine Learning for Enhanced Diagnosis and Treatment Efficacy

**Authors:** Jie Wang, Yong-mei Liu, Jun Li, Hao-Qiang He, Chao Liu

**Year:** 2025 | **Venue:** Current Medical Science | **Citations:** N/A | **Score:** 0.197

[DOI](https://doi.org/10.1007/s11596-025-00103-6)

> ...

---

## 255. spEMO: Leveraging Multi-Modal Foundation Models for Analyzing Spatial Multi-Omic and Histopathology Data

**Authors:** Tianyu Liu, Tinglin Huang, Tong Ding, Hao Wu, Peter Humphrey

**Year:** 2025 | **Venue:** bioRxiv | **Citations:** 3 | **Score:** 0.197

[PDF](https://doi.org/10.1101/2025.01.13.632818) | [DOI](https://doi.org/10.1101/2025.01.13.632818)

> Recent advances in pathology foundation models (PFMs), which are pretrained on large-scale histopathological images, have significantly accelerated progress in disease-centered applications. In parallel, spatial multi-omic technologies collect gene and protein expression levels at high spatial resolution, offering rich understanding of tissue context. However, current models fall short in effectiv...

---

## 256. Cyclic Contrastive Representation Learning for Incomplete Multi-modal Medical Image Segmentation.

**Authors:** Shihuan He, Zongbao Yang, Jianbo Zhao, Hao Zhang, Ruxin Wang

**Year:** 2025 | **Venue:** IEEE journal of biomedical and health informatics | **Citations:** N/A | **Score:** 0.197

[DOI](https://doi.org/10.1109/JBHI.2025.3637570)

> Accurate segmentation of multimodal medical images with missing modalities remains a critical challenge due to incomplete data often encountered in clinical practice. Lack of modality-specific information often leads to significant performance degradation in scenarios with severely missing modalities. To address this problem, we focus on modeling the relationships between modality-specific feature...

---

## 257. Medical Education Amidst Conflict: A National Assessment of Sudanese Medical Students’ Perceptions Using the DREEM Tool

**Authors:** Norita Hussein, Malaz M Abdalmotalib, Muaiad Adil A. Abdelmotalab, Mona Aly Mohammed, Ola Yaser Mohammed Yassen

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.197

[PDF](https://www.researchsquare.com/article/rs-7329997/latest.pdf) | [DOI](https://doi.org/10.21203/rs.3.rs-7329997/v1)

> <title>Abstract</title> <bold>Background</bold> The ongoing conflict in Sudan has severely disrupted medical education, urging the need to assess the educational environment experienced by medical students across the country. <bold>Methods</bold> A cross-sectional study was conducted between December 2024 and February 2025, involving 2,714 medical students from 46 Sudanese universities. A convenie...

---

## 258. From Garden to Clinic: Plant&amp;#x2011;Derived Exosome&amp;#x2011;Like Nanovesicles for Precision Oxidative Stress Therapy

**Authors:** Tianhang Yang, Mengjia He, Jinxi Huang, Dan Zhang, Tao Song

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.196

[PDF](https://www.dovepress.com/from-garden-to-clinic-plantderived-exosomelike-nanovesicles-for-precis-peer-reviewed-fulltext-article-IJN) | > Tianhang Yang,1,2,&amp;ast; Mengjia He,1,2,&amp;ast; Jinxi Huang,1,2 Dan Zhang,3 Tao Song,1,2 Jun Tan,4 Xianyao Wang,1,2 Yanxin Lu,5 Qinghong Kong,6 Jidong Zhang1,2,7 1Department of Immunology, Zunyi Medical University, Zunyi, Peopleâs Republic of China; 2Key Laboratory of Cancer Prevention and Treatment of Guizhou Province, Zunyi Medical University, Zunyi, Peopleâs Republic of China; 3Library...

---

## 259. UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion

**Authors:** Zihan Liang, Yufei Ma, Zhipeng Qian, Huangyu Dai, Zihan Wang

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.195

[PDF](https://arxiv.org/pdf/2508.13843) | [DOI](https://doi.org/10.1145/3746252.3761170)

> Current e-commerce multimodal retrieval systems face two key limitations: they optimize for specific tasks with fixed modality pairings, and lack comprehensive benchmarks for evaluating unified retrieval approaches. To address these challenges, we introduce UniECS, a unified multimodal e-commerce search framework that handles all retrieval scenarios across image, text, and their combinations. Our ...

---

## 260. Foundation Models for Music: A Survey

**Authors:** Ying-Chao Ma, Anders Oland, Anton Ragni, B. M. D. Sette, C. Saitis

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 45 | **Score:** 0.195

[DOI](https://doi.org/10.48550/arXiv.2408.14340)

> In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the signifi...

---

## 261. The Global Importance of Machine Learning-Based Wearables and Digital Twins for Rehabilitation: A Review of Data Collection, Security, Edge Intelligence, Federated Learning, and Generative AI

**Authors:** Jolanta Masiak, Izabela Rojek

**Year:** 2025 | **Venue:** Electronics | **Citations:** N/A | **Score:** 0.195

[PDF](https://doi.org/10.3390/electronics14234699) | [DOI](https://doi.org/10.3390/electronics14234699)

> The convergence of wearable technologies and digital twin (DT) systems is transforming rehabilitation engineering, enabling continuous monitoring, personalized therapeutic interventions, and predictive modeling of patient recovery pathways. This review examines the growing role of machine learning (ML) in the development and integration of DTs frameworks in rehabilitation, with a focus on wearable...

---

## 262. AI-Based Healthcare Diagnostics Using CNN and Reinforcement Learning

**Authors:** Kumar, Pawan, Sharma, Mudit, JECRC University

**Year:** 2025 | **Venue:** Zenodo (CERN European Organization for Nuclear Research) | **Citations:** N/A | **Score:** 0.194

[PDF](https://doi.org/10.5281/zenodo.17980033) | [DOI](https://doi.org/10.5281/zenodo.17980033)

> This research presents a hybrid artificial intelligence framework designed to improve the accuracy, efficiency, and reliability of automated healthcare diagnostics. The proposed system integrates Convolutional Neural Networks (CNNs) for spatial feature extraction, Transformer models for global contextual representation learning, and Reinforcement Learning (RL) for decision optimization. By combini...

---

## 263. Hypergraph Mamba for Efficient Whole Slide Image Understanding

**Authors:** Jiaxuan Lu, Yuhui Lin, Junyan Shi, Fang Yan, Dongzhan Zhou

**Year:** 2025 | **Venue:**  | **Citations:** 1 | **Score:** 0.194

> Whole Slide Images (WSIs) in histopathology pose a significant challenge for extensive medical image analysis due to their ultra-high resolution, massive scale, and intricate spatial relationships. Although existing Multiple Instance Learning (MIL) approaches like Graph Neural Networks (GNNs) and Transformers demonstrate strong instance-level modeling capabilities, they encounter constraints regar...

---

## 264. A Cross‐Scale Memristor‐Based Neuromorphic Platform Bridging Physical Encoding and Medical Inference

**Authors:** Xiaobing Ren, Zhirong Liu, Song Wang, Zelin Cao, Kaikai Gao

**Year:** 2025 | **Venue:** Advanced Functional Materials | **Citations:** N/A | **Score:** 0.194

[PDF](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/adfm.202522229) | [DOI](https://doi.org/10.1002/adfm.202522229)

> Abstract Despite the progress in machine vision, existing image recognition systems still operate in isolation from the physical world, lacking dynamic temporal encoding, in‐memory processing, and intrinsic interpretability. Here, a memristive vision architecture that integrates multimodal sensing, physical memory, and medical inference within a unified device‐level framework is developed. Specifi...

---

## 265. Multimodal MRI–HSI synthetic brain tissue dataset based on agar phantoms

**Authors:** Villa, Manuel, Sancho Jaime, Rosa-Olmeda, Gonzalo, Enkaoua, Aure, Moccia, Sara

**Year:** 2025 | **Venue:** Zenodo (CERN European Organization for Nuclear Research) | **Citations:** N/A | **Score:** 0.192

[PDF](https://doi.org/10.5281/zenodo.17830112) | [DOI](https://doi.org/10.5281/zenodo.17830112)

> The Multimodal Agar Brain Phantom Dataset provides a collection of co-registered imaging data acquired from brain-mimicking phantoms designed for research in multimodal medical image registration, tissue characterization, and surgical guidance. The dataset includes two phantom generations (Phantom v1 and Phantom v2), comprising 13 phantoms in total with different tissue configurations and acquisit...

---

## 266. Enhancing Clinical Named Entity Recognition via Fine-Tuned BERT and Dictionary-Infused Retrieval-Augmented Generation

**Authors:** S. Sreenivas, S. P. Chowdhury, Mohammad Masum

**Year:** 2025 | **Venue:** Electronics | **Citations:** N/A | **Score:** 0.192

[PDF](https://www.mdpi.com/2079-9292/14/18/3676/pdf?version=1758106004) | [DOI](https://doi.org/10.3390/electronics14183676)

> Clinical notes often contain unstructured text filled with abbreviations, non-standard terminology, and inconsistent phrasing, which pose significant challenges for automated medical information extraction. Named Entity Recognition (NER) plays a crucial role in structuring this data by identifying and categorizing key clinical entities such as symptoms, medications, and diagnoses. However, traditi...

---

## 267. A Generative Expert-Narrated Simplification Model for Enhancing Health Literacy Among the Older Population

**Authors:** Akmalbek Abdusalomov, Sabina Umirzakova, Sanjar Mirzakhalilov, Alpamis Kutlimuratov, Rashid Nasimov

**Year:** 2025 | **Venue:** Bioengineering | **Citations:** N/A | **Score:** 0.192

[PDF](https://www.mdpi.com/2306-5354/12/10/1066/pdf?version=1759251918) | [DOI](https://doi.org/10.3390/bioengineering12101066)

> Older adults often face significant challenges in understanding medical information due to cognitive aging and limited health literacy. Existing simplification models, while effective in general domains, cannot adapt content for elderly users, frequently overlooking narrative tone, readability constraints, and semantic fidelity. In this work, we propose GENSIM—a Generative Expert-Narrated Simplifi...

---

## 268. MedDiTPro: A Prompt-Guided Diffusion Transformer for Multimodal Longitudinal Medical Data Synthesis

**Authors:** Yuan Zhong, Xiaochen Wang, Jiaqi Wang, Xiaokun Zhang, Fenglong Ma

**Year:** 2025 | **Venue:** Knowledge Discovery and Data Mining | **Citations:** N/A | **Score:** 0.192

[DOI](https://doi.org/10.1145/3711896.3737045)

> Diffusion models have recently emerged as a state-of-the-art approach for synthetic Electronic Health Record (EHR) generation, offering superior fidelity and diversity over traditional generative models. However, existing diffusion-based methods struggle with unique challenges: limited representation learning and modality utilization, where they fail to explicitly capture inter-modality dependenci...

---

## 269. Real-Time Mobile Video Analytics for Pre-arrival Emergency Medical Services

**Authors:** Jin, Liuyi, Haroon, Amran, Stoleru Radu, Gunawardena, Pasan, Middleton, Michael

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.191

[PDF](https://doi.org/10.48550/arxiv.2511.14119) | [DOI](https://doi.org/10.48550/arxiv.2511.14119)

> Timely and accurate pre-arrival video streaming and analytics are critical for emergency medical services (EMS) to deliver life-saving interventions. Yet, current-generation EMS infrastructure remains constrained by one-to-one video streaming and limited analytics capabilities, leaving dispatchers and EMTs to manually interpret overwhelming, often noisy or redundant information in high-stress envi...

---

## 270. Natural Language Processing Models and Applications

**Authors:** Yawen Zhang

**Year:** 2025 | **Venue:** Science and Technology of Engineering Chemistry and Environmental Protection | **Citations:** N/A | **Score:** 0.190

[PDF](https://lseee.net/index.php/te/article/download/1509/TE012692.pdf) | [DOI](https://doi.org/10.61173/zgja0n61)

> This paper reviews the technological evolution of Natural Language Processing (NLP), tracing its development from traditional rule-based and statistical methods to modern deep learning paradigms. It particularly emphasizes the profound impact of neural models. By systematically examining NLP’s significantly enhanced capabilities in understanding, generating, and integrating natural language, this ...

---

## 271. Omni-LLaMA-AD: A Unified Model for Open-Set Visual Anomaly Detection

**Authors:** Rongyu Zhang, Zhanbin Hu, Jiamu Wang, Qiang Zhu

**Year:** 2025 | **Venue:** Proceedings of the 33rd ACM International Conference on Multimedia | **Citations:** N/A | **Score:** 0.189

[DOI](https://doi.org/10.1145/3746027.3754465)

> Visual anomaly detection (VAD) aims to identify image regions that deviate from established normal patterns. Existing methods often rely on domain-specific training and follow a ''one-class-one-model'' paradigm, limiting scalability. We propose Omni-LLaMA-AD, the first unified multimodal large language model for open-set anomaly detection, capable of handling diverse domains with minimal supervisi...

---

## 272. Solar-Powered UAVs Architecture Design for GIG/Medical Deliveries

**Authors:** Mahama Dauda

**Year:** 2025 | **Venue:** Preprints.org | **Citations:** N/A | **Score:** 0.189

[PDF](https://www.preprints.org/frontend/manuscript/445bc55814cb5439e42feb233530ed4e/download_pub) | [DOI](https://doi.org/10.20944/preprints202509.1325.v1)

> The increasing demand for reliable, rapid, and sustainable last-mile medical deliveries has accelerated the adoption of unmanned aerial vehicle (UAV) technologies. This study proposes a solar-powered UAV system architecture that integrates Global Information Grid (GIG) network layers and cold-chain logistics for the safe transportation of blood, vaccines, biologics, and emergency medication. The a...

---

## 273. Hybrid AI-Driven Computer-Aided Engineering Optimization: Large Language Models Versus Regression-Based Models Validated Through Finite-Element Analysis

**Authors:** Che Ting Chien, Chi‐Wen Chien

**Year:** 2025 | **Venue:** Applied Sciences | **Citations:** N/A | **Score:** 0.188

[PDF](https://www.mdpi.com/2076-3417/15/18/10123/pdf?version=1758095133) | [DOI](https://doi.org/10.3390/app151810123)

> This study investigates the application potential of large language models (LLMs), particularly GPT-4o, in generating geometric parameter suggestions during the early stages of structural design. Design recommendations from the LLM are validated using a finite-element solver (FFE Plus solver), forming the core workflow of the proposed approach. To assess its effectiveness, the LLM’s performance is...

---

## 274. A Semantic Framework for Patient Digital Twins in Chronic Care

**Authors:** Elgammal, Amal, Krämer, Bernd, J., Papazoglou, Michael P., Raheem, Mira

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.188

[PDF](https://arxiv.org/pdf/2510.09134) | [DOI](https://doi.org/10.48550/arxiv.2510.09134)

> Personalized chronic care requires the integration of multimodal health data to enable precise, adaptive, and preventive decision-making. Yet most current digital twin (DT) applications remain organ-specific or tied to isolated data types, lacking a unified and privacy-preserving foundation. This paper introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in silico patient framewo...

---

## 275. [Expert consensus on etiologic diagnosis and clinical management of acute respiratory tract infection in children (2025 edition)].

**Authors:** 

**Year:** 2025 | **Venue:** PubMed | **Citations:** N/A | **Score:** 0.187

[DOI](https://doi.org/10.3760/cma.j.cn112137-20250613-01440)

> Acute respiratory tract infection (ARTI) is a common and prevalent disease in children. Defining its etiology is essential for precise and standardized prevention and treatment. There exists a disparity in the understanding of pathogen detection and clinical management of childhood ARTI among pediatricians at all levels, particularly regarding the lack of a unified definition and diagnostic and th...

---

## 276. Neonatal Training across Union of European Medical Specialists Member Countries: A Survey from the Next Generation of Neonatologists

**Authors:** E. Williams, T. Dassios, Almudena Alonso-Ojembarrena, Charles C. Roehr, Jasper V. Been

**Year:** 2025 | **Venue:** Neonatology | **Citations:** N/A | **Score:** 0.186

[DOI](https://doi.org/10.1159/000547673)

> Abstract Introduction: Taking care of critically ill newborn infants is challenging, even more so in the current era of greater use of non-invasive support, leaving limited exposure to learn critical skills. Supporting a growing workforce of neonatologists requires training of doctors with formal curriculums and assessment of skills ensuring delivery of high-quality care. Our aim was to gain an un...

---

## 277. Understanding the academic use of KM-driven Metaverse technology: insights from medical colleges

**Authors:** Amir A. Abdulmuhsin, Haitham O. Owain, Abeer F. Alkhwaldi

**Year:** 2024 | **Venue:** Journal of Science and Technology Policy Management | **Citations:** 27 | **Score:** 0.186

[DOI](https://doi.org/10.1108/jstpm-12-2023-0229)

> 
Purpose
This study delves into the behavioural intentions of educators within medical colleges at Mosul Universities concerning the adoption of Knowledge Management-Driven Metaverse technology (KM-D-MT). Rooted in an adapted Unified Theory of Acceptance and Use of Technology 2 (UTAUT2) model, the research aims to enrich the understanding of Metaverse adoption factors, exploring correlations among...

---

## 278. Editorial: Antimicrobial Resistance (AMR): Challenges and Opportunities, A Special Issue of the Jordan Medical Journal

**Authors:** Al‐Tamimi Mohammad, Maisam Akroush, Faris G Albakri

**Year:** 2025 | **Venue:** Jordan Medical Journal | **Citations:** N/A | **Score:** 0.186

[PDF](https://jjournals.ju.edu.jo/index.php/JMJ/article/download/5350/1279) | [DOI](https://doi.org/10.35516/jmj.v59i5.5350)

> A Brief View of the Global Situation of Antimicrobial Resistance (AMR) Antimicrobial resistance (AMR) is undermining the effectiveness of life-saving treatments and placing populations at heightened risk, whether from common infections or even routine medical interventions [1]. In 2014, the “Review on AMR” projected that 10 million deaths caused by AMR could occur by 2050. This helped position AMR...

---

## 279. ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction

**Authors:** Md Zabirul Islam, Md Motaleb Hossen Manik, Ge Wang

**Year:** 2025 | **Venue:** arXiv (Cornell University) | **Citations:** N/A | **Score:** 0.186

[PDF](https://doi.org/10.48550/arxiv.2512.20858) | [DOI](https://doi.org/10.48550/arxiv.2512.20858)

> Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delive...

---

## 280. Generative Classifiers Avoid Shortcut Solutions

**Authors:** Alexander C. Li, Ananya Kumar, Deepak Pathak

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.184

[PDF](https://arxiv.org/pdf/2512.25034v1) | > Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, ins...

---

## 281. Thermodynamic Characterization of Superconducting Materials

**Authors:** Department of Physics at NIILM University, Kaithal, Haryana, Ishaan Pathak

**Year:** 2025 | **Venue:** International Journal of Applied and Behavioral Sciences | **Citations:** N/A | **Score:** 0.184

[PDF](https://ijabs.niilmuniversity.ac.in/wp-content/uploads/2025/12/67.-Thermodynamic-Characterization-of-Superconducting-Materials-11.pdf) | [DOI](https://doi.org/10.70388/ijabs250155)

> Superconducting materials, characterized by their ability to conduct electric current with zero resistance below a critical temperature, have revolutionized various technological applications, from medical imaging to quantum computing. The thermodynamic characterization of these materials is pivotal in understanding their fundamental properties, optimizing their performance, and facilitating the d...

---

## 282. UniECG: Understanding and Generating ECG in One Unified Model

**Authors:** Jiarui Jin, Haoyu Wang, Xiang Lan, Jun Li, Gaofeng Cheng

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 1 | **Score:** 0.184

[DOI](https://doi.org/10.48550/arXiv.2509.18588)

> Recent unified models such as GPT-5 have achieved encouraging progress on vision-language tasks. However, these unified models typically fail to correctly understand ECG signals and provide accurate medical diagnoses, nor can they correctly generate ECG signals. To address these limitations, we propose UniECG, the first unified model for ECG capable of concurrently performing evidence-based ECG in...

---

## 283. Coordinated Humanoid Manipulation with Choice Policies

**Authors:** Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.184

[PDF](https://arxiv.org/pdf/2512.25072v1) | > Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which i...

---

## 284. Redefining knowledge-generation-driven blockchain for healthcare use: Insights from medical institutions.

**Authors:** Amir A. Abdulmuhsin, Abdulkareem H. Dbesan, S. Rehman, Alhamzah Alnoor, Abeer F. Alkhwaldi

**Year:** 2025 | **Venue:** Health Services Management Research | **Citations:** 1 | **Score:** 0.184

[DOI](https://doi.org/10.1177/09514848251358332)

> This study explores the factors influencing healthcare professionals' willingness to adopt knowledge-generation-driven Blockchain technology (KGDBT) in government healthcare facilities, using the Unified Theory of Acceptance and Use of Technology 2 (UTAUT2) framework. It introduces transparency as an independent variable and examines the mediating role of knowledge generation in the relationship b...

---

## 285. Technological Advances in Orthopedic Implants and Their Impact on Surgical Practice: An Integrative Review

**Authors:** Fieschi da Silva, Ícaro

**Year:** 2025 | **Venue:** Zenodo (CERN European Organization for Nuclear Research) | **Citations:** N/A | **Score:** 0.184

[PDF](https://doi.org/10.5281/zenodo.17809057) | [DOI](https://doi.org/10.5281/zenodo.17809057)

> Technological Advances in Orthopedic Implants and Their Impact on Surgical Practice: An Integrative Review Ícaro Fieschi da Silva Abstract The evolution of orthopedic implants has progressed at an accelerated pace in recent decades, driven by a combination of new metal alloys, bioactive biomaterials, 3D printing, and digital technologies such as navigation and robotics. These advances have profoun...

---

## 286. Digital Twin-Driven Sorting System for 3D Printing Farm

**Authors:** Zeyan Wang, Fei Xie, Zhiyuan Wang, Yijian Liu, Qi Mao

**Year:** 2025 | **Venue:** Applied Sciences | **Citations:** 1 | **Score:** 0.183

[PDF](https://www.mdpi.com/2076-3417/15/18/10222/pdf?version=1758283020) | [DOI](https://doi.org/10.3390/app151810222)

> Modern agricultural intelligent manufacturing faces critical challenges including low automation levels, safety hazards in high-temperature processing, and insufficient production data integration. Digital twin technology and 3D printing offer promising solutions through real-time virtual–physical synchronization and customized equipment manufacturing, respectively. However, existing research exhi...

---

## 287. Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer - Current Trends and Research Perspectives

**Authors:** Karolina Seweryn, Anna Wróblewska, Szymon Łukasik

**Year:** 2025 | **Venue:** ACM Transactions on Intelligent Systems and Technology | **Citations:** N/A | **Score:** 0.183

[PDF](https://arxiv.org/pdf/2309.12067) | [DOI](https://doi.org/10.1145/3776541)

> Analysing action scenes in soccer is a challenging task due to the complex and dynamic nature of the game, as well as the interactions between players. This article provides a comprehensive overview of this task, divided into action recognition, spotting key moments, and identifying actions in both time and space (spatio-temporal action localization) in soccer. We explore publicly available data s...

---

## 288. Extreme nonlinear optics in optical fibers

**Authors:** Mario Ferraro, Bertrand Kibler, Pierre Béjot, Frédéric Gérome, Benoit Debord

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.183

[PDF](https://arxiv.org/pdf/2512.25046v1) | > This paper reviews the field of extreme nonlinear optics in optical fibers, highlighting key phenomena and advancements. It discusses multiple ionization effects caused by femtosecond laser pulses that generate plasma and induce permanent material modifications, as well as plasma luminescence and its dependence on material imperfections. The formation and dynamics of plasma filaments, including he...

---

## 289. Next-generation biomedical sensing through photoacoustic–electrochemical synergy

**Authors:** Mingxi Chen, Junyu Zhou, Keying Guo, Valery V. Tuchin, Xunbin Wei

**Year:** 2025 | **Venue:** Medical Review | **Citations:** N/A | **Score:** 0.183

[DOI](https://doi.org/10.1515/mr-2025-0071)

> Abstract Modern biomedical sensing increasingly demands technologies capable of capturing structural, functional, and molecular information simultaneously. Photoacoustic (PA) and electrochemical (EC) sensing individually address these needs but exhibit inherent limitations when used alone. PA imaging offers deep-tissue, label-free visualization with high spatiotemporal resolution, yet lacks molecu...

---

## 290. The treatment characteristics of oral antipsychotics in treatment of adolescents with schizophrenia in China

**Authors:** Huaning Wang, Yongjing Zhang, Tao Wu, Sijia Dong, Rui Chi

**Year:** 2025 | **Venue:** BMC Psychiatry | **Citations:** N/A | **Score:** 0.182

[DOI](https://doi.org/10.1186/s12888-025-07113-7)

> Evidence on treatment for adolescent with schizophrenia in China is limited. Understanding the current practice of antipsychotics utilization is imperative to inform appropriate use in this population. This retrospective cohort study used electronic medical records (2018–2022) from two hospitals in China. Adolescents (12-17y) with schizophrenia (ICD-10: F20.x) who were prescribed oral antipsychoti...

---

## 291. The End of Particles & Beginning of the Wave Field Dark Matter, the Waveon, Waveon Interactions, and Multidimensional Wave Mechanics

**Authors:** Ean Mikale

**Year:** 2025 | **Venue:** Engineering and Applied Sciences Journal | **Citations:** N/A | **Score:** 0.182

[DOI](https://doi.org/10.64030/3067-8005.02.01.04)

> This research investigates the Waveon, a Dark Matter Candidate, and the second released by Infinite 8 Industries; a fundamental
quantum entity that revolutionizes our understanding of light, energy, and matter. Identified through advanced wave interference
techniques, the Waveon represents the irreducible building block of light waves, distinct from the photon, and embodies pure
wave properties. I...

---

## 292. Climate Change and AMR: Interconnected Threats and One Health Solutions

**Authors:** Bilal Aslam, Sulaiman F. Aljasir

**Year:** 2025 | **Venue:** Antibiotics | **Citations:** 3 | **Score:** 0.181

[PDF](https://www.mdpi.com/2079-6382/14/9/946/pdf?version=1758201393) | [DOI](https://doi.org/10.3390/antibiotics14090946)

> Climate change is a significant driver of antimicrobial resistance (AMR) and infectious disease dynamics, presenting urgent and interconnected global health challenges. Rising temperatures, ecosystem alterations, and extreme weather events amplify the global spread of resistant pathogens, zoonotic infections, and vector-borne diseases. These impacts disproportionately affect low- and middle-income...

---

## 293. A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences

**Authors:** Gabriel Lino Garcia, J. R. Manesco, P. H. Paiola, Lucas Miranda, Maria Paola de Salvo

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 9 | **Score:** 0.181

[DOI](https://doi.org/10.48550/arXiv.2412.03531)

> The rapid advancement of large language models (LLMs) has opened new boundaries in the extraction and synthesis of medical knowledge, particularly within evidence synthesis. This paper reviews the state-of-the-art applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of docume...

---

## 294. Polyvagal theory: a journey from physiological observation to neural innervation and clinical insight

**Authors:** Stephen W. Porges

**Year:** 2025 | **Venue:** Frontiers in Behavioral Neuroscience | **Citations:** 1 | **Score:** 0.180

[PDF](https://www.frontiersin.org/journals/behavioral-neuroscience/articles/10.3389/fnbeh.2025.1659083/pdf) | [DOI](https://doi.org/10.3389/fnbeh.2025.1659083)

> Polyvagal theory (PVT) offers an integrative model of autonomic regulation that accounts for the evolution, neuroanatomy, and functional organization of the vagus nerve in relation to behavioral and emotional processes. This article revisits PVT by synthesizing its scientific foundations with recent advancements in transcriptomics, neurophysiology, and clinical application. Particular emphasis is ...

---

## 295. Knowledge and Perceptions of Stillbirth Among Postgraduate trainees in Gynecology: A Cross-Sectional KAP Study (Preprint)

**Authors:** Vidisha Khanna, Smriti Agrawal, Rachael Cherubin

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.180

[PDF](https://doi.org/10.2196/preprints.87891) | [DOI](https://doi.org/10.2196/preprints.87891)

> <sec> <title>BACKGROUND</title> With advances in antenatal care and prenatal diagnosis, occurrence of adverse pregnancy outcomes, especially stillbirths continue to pose a clinical challenge. These present a difficult situation wherein parents rightfully have questions which the caregiver is often unable to answer satisfactorily. There also lies the burden of predicting recurrence, advising on pre...

---

## 296. Core Models: One Code to Rule Them All! How the Same Loops Govern Your Body, Nature, and the Marketplace

**Authors:** Mihăilă, Robert Ioan

**Year:** 2025 | **Venue:** Zenodo (CERN European Organization for Nuclear Research) | **Citations:** N/A | **Score:** 0.180

[PDF](https://doi.org/10.5281/zenodo.17562888) | [DOI](https://doi.org/10.5281/zenodo.17562888)

> Abstract for Future Scientists Calling all curious minds! Get ready to become a codebreaker. Did you know your body, a lion chasing a gazelle, and the price of a video game all run on the same SECRET CODE? This guide reveals this powerful secret: complex systems in your body, in nature, and in our economy are all governed by the same simple, repeating pattern—the balancing loop. We call them “Secr...

---

## 297. AudioSet-R: A Refined AudioSet with Multi-Stage LLM Label Reannotation

**Authors:** Yining Sun, Qisheng Xu, Yi Su, Qian Zhu, Yong Dou

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2508.15429) | [DOI](https://doi.org/10.1145/3746027.3758260)

> AudioSet is a widely used benchmark in the audio research community and has significantly advanced various audio-related tasks. However, persistent issues with label accuracy and completeness remain critical bottlenecks that limit performance in downstream applications.To address the aforementioned challenges, we propose a three-stage reannotation framework that harnesses general-purpose audio-lan...

---

## 298. Mini-Batch Robustness Verification of Deep Neural Networks

**Authors:** Saar Tzour-Shaday, Dana Drachsler-Cohen

**Year:** 2025 | **Venue:** Proceedings of the ACM on Programming Languages | **Citations:** N/A | **Score:** 0.180

[PDF](https://doi.org/10.1145/3763150) | [DOI](https://doi.org/10.1145/3763150)

> Neural network image classifiers are ubiquitous in many safety-critical applications. However, they are susceptible to adversarial attacks. To understand their robustness to attacks, many local robustness verifiers have been proposed to analyze є-balls of inputs. Yet, existing verifiers introduce a long analysis time or lose too much precision, making them less effective for a large set of inputs....

---

## 299. Finance: A Relational Perspective

**Authors:** A.L. Bovenberg

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.180

[PDF](http://hdl.handle.net/1765/132022) | [DOI](https://doi.org/10.5117/9789463727914-11)

> In this chapter Lans Bovenberg argues that things already go wrong in secondary-level education. If we teach the youth about a world of markets and competition, it should be no surprise that when they reach adulthood, they will interact with the world according to these principles. Prof. Bovenberg suggests that we approach economics and the financial sector from a relational perspective....

---

## 300. Cross-Reality Lifestyle: Integrating Physical and Virtual Lives Through Multiplatform Metaverse

**Authors:** Yuichi Hiroi, Yuji Hatada, Takefumi Hiraki

**Year:** 2025 | **Venue:** IEEE Pervasive Computing | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2504.21337) | [DOI](https://doi.org/10.1109/mprv.2025.3610749)

> Technological advances are redefining the relationship between physical and virtual spaces. Traditionally, when users engage in virtual reality, they are completely cutoff from the physical space. Similarly, they are unable to access virtual experiences while engaged in physical activities. However, modern multiplatform metaverse environments allow simultaneous participation through mobile devices...

---

## 301. Crying in the algorithm: modeling academic stress via multilayer topic construction and ERA effect

**Authors:** Liwei Ding, Hongfeng Zhang, Jinqiao Zhou

**Year:** 2025 | **Venue:** Frontiers in Psychology | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1673559/pdf) | [DOI](https://doi.org/10.3389/fpsyg.2025.1673559)

> Amid intensifying educational competition and societal expectations, academic stress has emerged as a multidimensional force influencing student mental health. While prior research has explored individual and institutional factors, limited attention has been paid to how learners semantically construct and express academic stress in digital environments. Addressing this gap, this study introduces a...

---

## 302. Machine Learning in Near-Field Communication for 6G: A Survey

**Authors:** Amjad Iqbal, Ala’a Al-Habashna, Gabriel Wainer, Gary Boudreau

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.175696105.52959598/v2) | [DOI](https://doi.org/10.36227/techrxiv.175696105.52959598/v2)

> ...

---

## 303. Climate justice, speciesism, and total liberation in age of the anthropocene

**Authors:** Steve Best

**Year:** 2025 | **Venue:** Frontiers in Communication | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2024.1484643/pdf) | [DOI](https://doi.org/10.3389/fcomm.2024.1484643)

> This essay provides a critical overview of climate justice discourse, while examining a key deficit that stems from humanist/speciesist biases and a failure to incorporate nonhuman animals (hereafter “animals”) into social analyses, ethics, politics, and visions of a just transition to an ecological society. This deficit, I argue, has serious consequence for understanding the roots and driving for...

---

## 304. Contextual Object Grouping (COG): A Specialized Framework for Dynamic Symbol Interpretation in Technical Security Diagrams

**Authors:** Jan Kapusta, Waldemar Bauer, Jerzy Baranowski

**Year:** 2025 | **Venue:** Preprints.org | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.preprints.org/frontend/manuscript/2d2b3994b4fdf640f3429201e278dbc2/download_pub) | [DOI](https://doi.org/10.20944/preprints202509.1632.v1)

> This paper introduces Contextual Object Grouping (COG), a specific computer vision framework that enables automatic interpretation of technical security diagrams through dynamic legend learning for intelligent sensing applications. Unlike traditional object detection approaches that rely on post-processing heuristics to establish relationships between detected elements, COG embeds contextual under...

---

## 305. An Interdisciplinary Review on Application of Graph Theory

**Authors:** Poonam Chawla

**Year:** 2025 | **Venue:** Turkish Journal of Computer and Mathematics Education (TURCOMAT). | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.turcomat.org/index.php/turkbilmat/article/download/15333/10930) | [DOI](https://doi.org/10.61841/turcomat.v9i1.15333)

> Graph theory is a branch of discrete mathematics that is used to model and analyze interconnected systems. This paper presents a review on application of its concepts and algorithms that support diverse applications in computer science, biology, neuroscience, social sciences, engineering and geosciences. It is also reviewedthat graph theory helped in network optimization, clustering, molecular mod...

---

## 306. Digital transitions at the port: human capital, technology adoption and operational efficiency among independent truckers

**Authors:** George Acheampong, Oliver Kwabena Aggrey, Alfred Kwadwo Yamoah Djan, Annette Skovsted Hansen, Mmakgabo Pinkie Segodi

**Year:** 2025 | **Venue:** Innovation & Management Review | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.emerald.com/inmr/article-pdf/doi/10.1108/INMR-04-2022-0054/10296050/inmr-04-2022-0054en.pdf) | [DOI](https://doi.org/10.1108/inmr-04-2022-0054)

> Purpose In this study we seek to understand the relationship between a trucker’s level of human capital, their ease of use of new technology innovations and how this translates into overall port efficiency at the Port of Tema. Design/methodology/approach To perform this inquiry, we develop a five-construct framework with trucker’s self-efficacy and mentorship moderating relationship between human ...

---

## 307. Azacitidine as therapy for VEXAS syndrome

**Authors:** David P. Steensma, Mrinal M. Patnaik

**Year:** 2025 | **Venue:** Blood | **Citations:** N/A | **Score:** 0.180

[PDF](https://ashpublications.org/blood/article-pdf/146/12/1380/2412661/blood_bld-2025-029732-c-main.pdf) | [DOI](https://doi.org/10.1182/blood.2025029732)

> ...

---

## 308. The Meanings of a Publication in the Humanities. Meaning, text, and authorship in critical perspective

**Authors:** Marcel Knöchelmann

**Year:** 2025 | **Venue:**  | **Citations:** N/A | **Score:** 0.180

[PDF](https://osf.io/uzq7m_v1/download) | [DOI](https://doi.org/10.31235/osf.io/uzq7m_v1)

> A publication is meaningful in different ways. It is more than objective materiality. A writer sees in a book a great achievement within a tradition of thinking. The corresponding reader finds in the same publication a hermeneutic device. For public discourse, the latest bestselling crossover book provides handy information to special interest readers. To philistine management, a publication is fo...

---

## 309. Two-Step Forward Modeling for GPR Data of Metal Pipes Based on Image Translation and Style Transfer

**Authors:** Zhishun Guo, Yesheng Gao, Zicheng Huang, Mengyang Shi, Xingzhao Liu

**Year:** 2025 | **Venue:** Remote Sensing | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.mdpi.com/2072-4292/17/18/3215/pdf?version=1758115071) | [DOI](https://doi.org/10.3390/rs17183215)

> Ground-penetrating radar (GPR) is an important geophysical technique in subsurface detection. However, traditional numerical simulation methods such as finite-difference time-domain (FDTD) face challenges in accurately simulating complex heterogeneous mediums in real-world scenarios due to the difficulty of obtaining precise medium distribution information and high computational costs. Meanwhile, ...

---

## 310. <i>AACR Cancer Progress Report 2025</i>: Unifying Cancer Science and Medicine: A Continuum of Innovation for Impact

**Authors:** Brett M. Sansbury, Elizabeth Blackman, Kristin Primm, Sayyed K. Zaidi, Rajarshi Sengupta

**Year:** 2025 | **Venue:** Clinical Cancer Research | **Citations:** N/A | **Score:** 0.180

[PDF](https://aacrjournals.org/clincancerres/article-pdf/doi/10.1158/1078-0432.CCR-25-3366/3651523/ccr-25-3366.pdf) | [DOI](https://doi.org/10.1158/1078-0432.ccr-25-3366)

> ...

---

## 311. An Innovative Retrieval-Augmented Generation Framework for Stage-Specific Knowledge Translation in Biomimicry Design

**Authors:** Hsueh-Kuan Chen, Hung-Hsiang Wang

**Year:** 2025 | **Venue:** Biomimetics | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.mdpi.com/2313-7673/10/9/626/pdf?version=1758108308) | [DOI](https://doi.org/10.3390/biomimetics10090626)

> Converting biological strategies into practical design principles during the Discover–Abstract phase of the Biomimicry Design Spiral (BSD) presents a considerable obstacle, particularly for designers lacking a biological background. This research introduces a Retrieval-Augmented Generation (RAG) framework that combines a specialized AskNature database of 2106 documents with a locally executed Llam...

---

## 312. HUMANITARIAN LESSONS IDENTIFIED IN THE CONDUCT OF THE ACTUAL CONFLICT IN UKRAINE

**Authors:** Viorel ORDEANU, Andronic BENONI

**Year:** 2025 | **Venue:** Strategic Impact | **Citations:** N/A | **Score:** 0.180

[PDF](https://revista.unap.ro/index.php/Impact_en/article/download/2208/2152) | [DOI](https://doi.org/10.53477/1842-9904-25-22)

> The authors of this study, conducted a comprehensive analysis of information from various domestic and international sources concerning the humanitarian dimensions of the ongoing conflict initiated by Russia in Ukraine. Their study revealed numerous inconsistencies and inaccuracies regarding the health and social aspects of the parties involved, and conclude on several lessons that should be consi...

---

## 313. Gal3‐CaN‐Smurf1 Complex Sequestrates FLCN‐FNIPs to Facilitate TFEB Activation in Response to Endomembrane Damage

**Authors:** Xia Qin, Z.F. Liu, Gaoqing Feng, Wanting Xu, H. Wang

**Year:** 2025 | **Venue:** Advanced Science | **Citations:** N/A | **Score:** 0.180

[PDF](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/advs.202413241) | [DOI](https://doi.org/10.1002/advs.202413241)

> Abstract Smurf1 mediates lysosomal biogenesis upon endomembrane damage by interacting with lysosomal injury sensor Gal3 and phosphatase CaN to form Gal3‐CaN‐Smurf1 complex, which is critical for TFEB dephosphorylation. However, whether Smurf1 plays a role in the inhibition of mTOR‐mediated TFEB phosphorylation is still unclear. TFEB phosphorylation by mTORC1 is strictly dependent on RagC/D GTPase ...

---

## 314. Adversarial Integration of LLM and Logic Program

**Authors:** Boris Galitsky

**Year:** 2025 | **Venue:** Preprints.org | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.preprints.org/frontend/manuscript/1832afc738971a7e6cf5d49bcd9d7566/download_pub) | [DOI](https://doi.org/10.20944/preprints202509.1484.v1)

> We introduce an innovative method that combines Large Language Models (LLMs) with Logic Programming (LP) to address complex reasoning tasks. This approach leverages the formal structure of LP to enhance the consistency of problem-solving by LLMs. In our framework, the LLM operates independently to generate reasoning steps and constructs a corresponding LP representation. The LP module then process...

---

## 315. Research on Enterprise Cross-border Data Compliance

**Authors:** Yuexuan He

**Year:** 2025 | **Venue:** Lecture Notes in Education Psychology and Public Media | **Citations:** N/A | **Score:** 0.180

[PDF](https://www.ewadirect.com/proceedings/lnep/article/view/26817/pdf) | [DOI](https://doi.org/10.54254/2753-7048/2025.br26817)

> With the continuous development of the global digital economy, cross-border data flows have become a critical component of businesses' international operations. At the same time, cross-border data compliance has become a major challenge facing businesses. The development of China's cross-border data compliance policy can be divided into three stages: initial exploration, system development, and re...

---

## 316. UniMRE: a unified framework for zero-shot medicial relation extraction with large language models

**Authors:** Yunlong Li, Pengcheng Wu, Aoze Zheng, Yuting Li, Hongying Zan

**Year:** 2025 | **Venue:** Health Information Science and Systems | **Citations:** N/A | **Score:** 0.180

[DOI](https://doi.org/10.1007/s13755-025-00359-1)

> ...

---

## 317. SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time

**Authors:** Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25075v1) | > We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animati...

---

## 318. Randomization Times under Quantum Chaotic Hamiltonian Evolution

**Authors:** Souradeep Ghosh, Nicholas Hunter-Jones, Joaquin F. Rodriguez-Nieva

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25074v1) | > Randomness generation through quantum-chaotic evolution underpins foundational questions in statistical mechanics and applications across quantum information science, including benchmarking, tomography, metrology, and demonstrations of quantum computational advantage. While statistical mechanics successfully captures the temporal averages of local observables, understanding randomness at the level...

---

## 319. GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction

**Authors:** Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25073v1) | > Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements b...

---

## 320. Edit3r: Instant 3D Scene Editing from Sparse Unposed Images

**Authors:** Jiageng Liu, Weijie Lyu, Xueting Li, Yejie Guo, Ming-Hsuan Yang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25071v1) | > We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies i...

---

## 321. Scaling Open-Ended Reasoning to Predict the Future

**Authors:** Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25070v1) | > High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenFore...

---

## 322. Classification of Interacting Topological Crystalline Superconductors in Three Dimensions and Beyond

**Authors:** Shang-Qiang Ning, Xing-Yu Ren, Qing-Rui Wang, Yang Qi, Zheng-Cheng Gu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25069v1) | > Although classification for free-fermion topological superconductors (TSC) is established, systematically understanding the classification of 3D interacting TSCs remains difficult, especially those protected by crystalline symmetries like the 230 space groups. We build up a general framework for systematically classifying 3D interacting TSCs protected by crystalline symmetries together with discre...

---

## 323. No-cost Bell Nonlocality Certification from Quantum Tomography and Its Applications in Quantum Magic Witnessing

**Authors:** Pawel Cieslinski, Lukas Knips, Harald Weinfurter, Wieslaw Laskowski

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25068v1) | > Tomographic measurements are the standard tool for characterizing quantum states, yet they are usually regarded only as means for state reconstruction or fidelity measurement. Here, we show that the same Pauli-basis measurements (X, Y, Z) can be directly employed for the certification of nonlocality at no additional experimental cost. Our framework allows any tomographic data - including archival ...

---

## 324. FineTec: Fine-Grained Action Recognition Under Temporal Corruption via Skeleton Decomposition and Sequence Completion

**Authors:** Dian Shao, Mingfei Shi, Like Liu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25067v1) | > Recognizing fine-grained actions from temporally corrupted skeleton sequences remains a significant challenge, particularly in real-world scenarios where online pose estimation often yields substantial missing data. Existing methods often struggle to accurately recover temporal dynamics and fine-grained spatial structures, resulting in the loss of subtle motion cues crucial for distinguishing simi...

---

## 325. From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing

**Authors:** Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25066v1) | > Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simulta...

---

## 326. Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search

**Authors:** Rohit Dwivedula, Divyanshu Saxena, Sujay Yadalam, Daehyeok Kim, Aditya Akella

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25065v1) | > Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.
  We propose a new alter...

---

## 327. Many Minds from One Model: Bayesian Transformers for Population Intelligence

**Authors:** Diji Yang, Yi Zhang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25063v1) | > Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into...

---

## 328. Melting curve of correlated iron at Earth's core conditions from machine-learned DFT+DMFT

**Authors:** Rishi Rao, Li Zhu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25061v1) | > Reliable constraints on iron's melting curve at Earth's inner-core boundary require accurate finite-temperature electronic correlations, yet DFT+DMFT calculations remain too costly for large-scale thermodynamic sampling. Here, we develop a machine-learning accelerator for charge self-consistent DFT+DMFT by training E(3)-equivariant graph neural networks to predict the local self-energy and Fermi l...

---

## 329. The variety of orthogonal frames

**Authors:** Laura Casabella, Alessio Sammartano

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25058v1) | > An orthogonal n-frame is an ordered set of n pairwise orthogonal vectors. The set of all orthogonal n-frames in a d-dimensional quadratic vector space is an algebraic variety V(d,n). In this paper, we investigate the variety V(d,n) as well as the quadratic ideal I(d,n) generated by the orthogonality relations, which cuts out V(d,n). We classify the irreducible components of V(d,n), give criteria f...

---

## 330. The Logical Structure of Physical Laws: A Fixed Point Reconstruction

**Authors:** Eren Volkan Küçük

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25057v1) | > We formalise the self referential definition of physical laws using monotone operators on a lattice of theories, resolving the pathologies of naive set theoretic formulations. By invoking Tarski fixed point theorem, we identify physical theories as least fixed points of admissibility constraints derived from Galois connections. We demonstrate that QED and General Relativity can be represented in s...

---

## 331. Sequential Bayesian parameter-state estimation in dynamical systems with noisy and incomplete observations via a variational framework

**Authors:** Liliang Wang, Alex Gorodetsky

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25056v1) | > Online joint estimation of unknown parameters and states in a dynamical system with uncertainty quantification is crucial in many applications. For example, digital twins dynamically update their knowledge of model parameters and states to support prediction and decision-making. Reliability and computational speed are vital for DTs. Online parameter-state estimation ensures computational efficienc...

---

## 332. AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG

**Authors:** Chao Peng, Bin Wang, Zhilei Long, Jinfang Sheng

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25052v1) | > Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-s...

---

## 333. Bilinear tau forms of quantum Painlevé equations and $\mathbb{C}^2/\mathbb{Z}_2$ blowup relations in SUSY gauge theories

**Authors:** Giulio Bonelli, Anton Shchechkin, Alessandro Tanzini

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25051v1) | > We derive bilinear tau forms of the canonically quantized Painlevé equations, thereby relating them to those previously obtained from the $\mathbb{C}^2/\mathbb{Z}_2$ blowup relations for the $\mathcal{N}=2$ supersymmetric gauge theory partition functions on a general $Ω$-background. We fully fix the refined Painlevé/gauge theory dictionary by formulating the proper equations for the quantum nonaut...

---

## 334. The PDE-ODI principle and cylindrical mean curvature flows

**Authors:** Richard H. Bamler, Yi Lai

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25050v1) | > We introduce a new approach for analyzing ancient solutions and singularities of mean curvature flow that are locally modeled on a cylinder. Its key ingredient is a general mechanism, called the \emph{PDE--ODI principle}, which converts a broad class of parabolic differential equations into systems of ordinary differential inequalities. This principle bypasses many delicate analytic estimates used...

---

## 335. All optical Lithography for Spatiotemporal Patterning of Azopolymer Microreliefs

**Authors:** I Komang Januariyasa, Francesco Reda, Nikolai Liubimtsev, Marina Saphiannikova, Fabio Borbone

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25048v1) | > Microstructured surfaces are central to photonics, biointerfaces, and functional coatings, yet they are typically fabricated through multi-step lithographic workflows requiring masks or molds and post-processing. Azopolymers provide an alternative route by converting structured optical fields into surface reliefs via light-induced mass migration, but existing approaches have been limited to smooth...

---

## 336. Bayesian Elastic Net Regression with Structured Prior Dependence

**Authors:** Christopher M. Hans, Ningyi Liu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25045v1) | > Many regularization priors for Bayesian regression assume the regression coefficients are a priori independent. In particular this is the case for standard Bayesian treatments of the lasso and the elastic net. While independence may be reasonable in some data-analytic settings, incorporating dependence in these prior distributions provides greater modeling flexibility. This paper introduces the or...

---

## 337. Primordial black hole dark matter from ultra-slow-roll inflation in Horndeski gravity

**Authors:** Despina Totolou, Theodoros Papanikolaou, Emmanuel N. Saridakis

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25044v1) | > Primordial black holes (PBHs) provide a well-motivated non-particle candidate for dark matter, requiring an enhancement of curvature perturbations on small inflationary scales consistent with observational constraints. In this work we study PBH production within Horndeski gravity, accounting for compatibility with the GW170817 constraint on the gravitational-wave speed and imposing a constant coup...

---

## 338. On exact Observability for Compactly perturbed infinite dimension system

**Authors:** Nisrine Charaf, Faouzi Triki

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25041v1) | > In this paper, we study the observability of compactly perturbed infinite dimensional systems. Assuming that a given infinite-dimensional system with self-adjoint generator is exactly observable we derive sufficient conditions on a compact self adjoint perturbation to guarantee that the perturbed system stays exactly observable. The analysis is based on a careful asymptotic estimation of the spect...

---

## 339. Towards precision cosmology with Voids x CMB correlations (I): Roman-Agora mock catalogs and pipeline validation

**Authors:** Mar Pérez Sar, Carlos Hernández Monteagudo, András Kovács, Alice Pisani

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25040v1) | > We construct and validate a set of multi-purpose mock galaxy catalogs designed to capture, to different degrees of accuracy, the main characteristics of the Nancy Grace Roman Space Telescope survey. These catalogs provide a foundation for void statistics and various CMB cross-correlation analyses. Our approach differs from traditional halo occupation or abundance matching methods by directly trans...

---

## 340. The Hochschild homology of a noncommutative symmetric quotient stack

**Authors:** Rina Anno, Vladimir Baranovsky, Timothy Logvinenko

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25039v1) | > We prove an orbifold type decomposition theorem for the Hochschild homology of the symmetric powers of a small DG category $\mathcal{A}$. In noncommutative geometry, these can be viewed as the noncommutative symmetric quotient stacks of $\mathcal{A}$. We use this decomposition to show that the total Hochschild homology of the symmetric powers of $\mathcal{A}$ is isomorphic to the symmetric algebra...

---

## 341. Universal polar dual pairs of spherical codes found in $E_8$ and $Λ_{24}$

**Authors:** S. V. Borodachov, P. G. Boyvalenkov, P. D. Dragnev, D. P. Hardin, E. B. Saff

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25037v1) | > We identify universal polar dual pairs of spherical codes $C$ and $D$ such that for a large class of potential functions $h$ the minima of the discrete $h$-potential of $C$ on the sphere occur at the points of $D$ and vice versa. Moreover, the minimal values of their normalized potentials are equal. These codes arise from the known sharp codes embedded in the even unimodular extremal lattices $E_8...

---

## 342. Perturbative Kondo destruction and global phase diagram of heavy fermion metals

**Authors:** Yiming Wang, Shouvik Sur, Chia-Chuan Liu, Qimiao Si

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25036v1) | > Strange metals represent a foundational problem in quantum condensed matter physics, and heavy fermion systems provide a canonical setting to advance a general understanding. The concept of a Kondo destruction quantum critical point is widely invoked to describe the competition of the Kondo effect and the local-moment magnetism. Here, we develop a unified field-theoretic approach, analyzing this c...

---

## 343. Large Neutrino-Dark Matter Interactions: From Effective Field Theory to Ultraviolet Completions

**Authors:** K. S. Babu, P. S. Bhupal Dev, Anil Thapa

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25035v1) | > We develop a general effective field theory (EFT) framework for neutrino-dark matter (DM) interactions, and apply it to systematically find all possible gauge-invariant ultraviolet (UV) completions at a given EFT operator dimension. Our goal here is to find simple UV-complete models that can realize potentially large neutrino-DM interactions, while being consistent with all existing theoretical an...

---

## 344. Testing Monotonicity in a Finite Population

**Authors:** Jiafeng Chen, Jonathan Roth, Jann Spiess

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25032v1) | > We consider the extent to which we can learn from a completely randomized experiment whether everyone has treatment effects that are weakly of the same sign, a condition we call monotonicity. From a classical sampling perspective, it is well-known that monotonicity is untestable. By contrast, we show from the design-based perspective -- in which the units in the population are fixed and only treat...

---

## 345. Multivariate Generalized Counting Process via Gamma Subordination

**Authors:** Manisha Dhillon, Kuldeep Kumar Kataria, Shyan Ghosh

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25030v1) | > In this paper, we study a multivariate gamma subordinator whose components are independent gamma processes subject to a random time governed by an independent negative binomial process. We derive the explicit expressions for its joint Laplace-Stieltjes transform, its probability density function and the associated governing differential equations. Also, we study a time-changed variant of the multi...

---

## 346. Mod $p$ Poincaré duality for $p$-adic period domains

**Authors:** Guillaume Pignon-Ywanne

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25029v1) | > In this article, we introduce a new class of smooth partially proper rigid analytic varieties over a $p$-adic field that satisfy Poincaré duality for étale cohomology with mod $p$-coefficients : the varieties satisfying "primitive comparison with compact support". We show that almost proper varieties, as well as p-adic (weakly admissible) period domains in the sense of Rappoport-Zink belong to thi...

---

## 347. Universal Seesaw Pati-Salam Model with P for Strong CP

**Authors:** K. S. Babu, Sumit Biswas

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25028v1) | > We develop a universal seesaw version of the Pati-Salam model wherein quarks and leptons of each family are unified into common multiplets transforming as $ψ_L(2,1,4))+ ψ_R((1,2,4)$ under the $SU(2)_L \times SU(2)_R \times SU(4)_c$ gauge symmetry. Parity symmetry is spontaneously broken in the model, which helps in solving the strong CP problem without the axion. The Higgs sector of the model is v...

---

## 348. Computational Analysis of Disease Progression in Pediatric Pulmonary Arterial Hypertension

**Authors:** Omar Said, Christopher Tossas-Betancourt, Mary K. Olive, Jimmy C. Lu, Adam Dorfman

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25027v1) | > Pulmonary arterial hypertension (PAH) is a progressive cardiopulmonary disease that leads to increased pulmonary pressures, vascular remodeling, and eventual right ventricular (RV) failure. Pediatric PAH remains understudied due to limited data and the lack of targeted diagnostic and therapeutic strategies. In this study, we developed and calibrated multi-scale, patient-specific cardiovascular mod...

---

## 349. Modeling Language as a Sequence of Thoughts

**Authors:** Nasim Borazjanizadeh, James McClelland

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25026v1) | > Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficienc...

---

## 350. On Nonlinear Inertial Transformations

**Authors:** Nicholas Agia

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25024v1) | > It is often assumed that the most general transformation between two inertial reference frames is affine linear in their Cartesian coordinates, an assumption which is however not true. We provide a complete derivation of the most general inertial frame transformation, which is indeed nonlinear; along the way, we shall find that the conditions of preserving the Law of Inertia take the form of Schwa...

---

## 351. ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning

**Authors:** Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke Hüllermeier

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25023v1) | > Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-an...

---

## 352. Convergence of the generalization error for deep gradient flow methods for PDEs

**Authors:** Chenguang Liu, Antonis Papapantoleon, Jasper Rou

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25017v1) | > The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approx...

---

## 353. MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes

**Authors:** Siddhant Agarwal, Adya Dhuler, Polly Ruhnke, Melvin Speisman, Md Shad Akhtar

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25015v1) | > Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a v...

---

## 354. Diffusion Language Models are Provably Optimal Parallel Samplers

**Authors:** Haozhe Jiang, Nika Haghtalab, Lijie Chen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25014v1) | > Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequ...

---

## 355. Bounding regularity of $\mathrm{VI}^m$-modules

**Authors:** Wee Liang Gan, Khoa Ta

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25010v1) | > Fix a finite field $\mathbb{F}$. Let $\mathrm{VI}$ be a skeleton of the category of finite dimensional $\mathbb{F}$-vector spaces and injective $\mathbb{F}$-linear maps. We study $\mathrm{VI}^m$-modules over a noetherian commutative ring in the nondescribing characteristic case. We prove that if a finitely generated $\mathrm{VI}^m$-module is generated in degree $\leqslant d$ and related in degree ...

---

## 356. The splitting field and generators of the elliptic surface $Y^2=X^3 +t^{360} +1$

**Authors:** Sajad Salami

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25009v1) | > The splitting field of an elliptic surface $\mathcal{E}/\mathbb{Q}(t)$ is the smallest finite extension $\mathcal{K} \subset \mathbb{C}$ such that all $\mathbb{C}(t)$-rational points are defined over $\mathcal{K}(t)$. In this paper, we provide a symbolic algorithmic approach to determine the splitting field and a set of $68$ linearly independent generators for the Mordell--Weil lattice of Shioda's...

---

## 357. FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM

**Authors:** Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25008v1) | > We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geome...

---

## 358. Fast Poisson brackets and constraint algebras in canonical gravity

**Authors:** Will Barker

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25007v1) | > In the study of alternative or extended theories of gravity, Dirac's Hamiltonian constraint algorithm is invaluable for enumerating the propagating modes and gauge symmetries. For gravity, this canonical approach is frequently applied as a means for finding pathologies such as strongly coupled modes; more generally it facilitates the reconstruction of gauge symmetries and the quantization of gauge...

---

## 359. Grassmannian Geometries for Non-Planar On-Shell Diagrams

**Authors:** Artyom Lisitsyn, Umut Oktem, Melissa Sherman-Bennett, Jaroslav Trnka

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25005v1) | > On-shell diagrams are gauge invariant quantities which play an important role in the description of scattering amplitudes. Based on the principles of generalized unitarity, they are given by products of elementary three-point amplitudes where the kinematics of internal on-shell legs are determined by cut conditions. In the ${\cal N}=4$ Super Yang-Mills (SYM) theory, the dual formulation for on-she...

---

## 360. The local limit of weighted spanning trees on balanced networks

**Authors:** Ágnes Kúsz

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.25001v1) | > We prove that the local limit of the weighted spanning trees on any simple connected high degree almost regular sequence of electric networks is the Poisson(1) branching process conditioned to survive forever, by generalizing [NP22] and closing a gap in their proof. We also study the local statistics of the WST's on high degree almost balanced sequences, which is interesting even for the uniform s...

---

## 361. Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis

**Authors:** Seunghoon Paik, Kangjie Zhou, Matus Telgarsky, Ryan J. Tibshirani

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.24999v1) | > We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Gi...

---

## 362. Manifold classification from the descriptive viewpoint

**Authors:** Jeffrey Bergfalk, Iian B. Smythe

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.24996v1) | > We consider classification problems for manifolds and discrete subgroups of Lie groups from a descriptive set-theoretic point of view. This work is largely foundational in conception and character, recording both a framework for general study and Borel complexity computations for some of the most fundamental classes of manifolds. We show, for example, that for all $n\geq 0$, the homeomorphism prob...

---

## 363. Strong Gravitational Lensing by a Black Hole with a Global Monopole in Kalb-Ramond Bumblebee Gravity

**Authors:** Bijendra Kumar Vishvakarma, Shubham Kala

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.180

[PDF](https://arxiv.org/pdf/2512.24995v1) | > We investigate the strong gravitational lensing and shadow properties of the black hole in the context of bumblebee gravity, characterized by a global monopole charge $κη^2$ and a Lorentz symmetry breaking parameter $γ$. We compute the deflection angles of light passing near the black hole in strong deflection limit, and estimate key lensing observables, including relativistic Einstein rings, abso...

---

## 364. Explainable Artificial Intelligence for Multi-Modal Cancer Analysis: From Genomics to Immunology

**Authors:** Min Zhang

**Year:** 2025 | **Venue:** Critical Reviews in Oncology/Hematology | **Citations:** N/A | **Score:** 0.180

[DOI](https://doi.org/10.1016/j.critrevonc.2025.105040)

> ...

---

## 365. MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants

**Authors:** Hritik Bansal, Daniel Israel, Siyan Zhao, Shufan Li, Tung Nguyen

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 5 | **Score:** 0.180

[DOI](https://doi.org/10.48550/arXiv.2412.12661)

> Recent advancements in mixed-modal generative have opened new avenues for developing unified biomedical assistants capable of analyzing biomedical images, answering complex questions about them, and generating multimodal patient reports. However, existing datasets face challenges such as small sizes, limited coverage of biomedical tasks and domains, and a reliance on narrow sources. To address the...

---

## 366. Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark

**Authors:** Li Xu, Bo Liu, Ameer Hamza Khan, Lu Fan, Xiao-Ming Wu

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 10 | **Score:** 0.176

[PDF](https://arxiv.org/pdf/2306.06494) | [DOI](https://doi.org/10.48550/arXiv.2306.06494)

> With the availability of large-scale, comprehensive, and general-purpose vision-language (VL) datasets such as MSCOCO, vision-language pre-training (VLP) has become an active area of research and proven to be effective for various VL tasks such as visual-question answering. However, studies on VLP in the medical domain have so far been scanty. To provide a comprehensive perspective on VLP for medi...

---

## 367. A Survey on Vision Autoregressive Model

**Authors:** Kai Jiang, Jiaxing Huang

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 2 | **Score:** 0.174

[DOI](https://doi.org/10.48550/arXiv.2411.08666)

> Autoregressive models have demonstrated great performance in natural language processing (NLP) with impressive scalability, adaptability and generalizability. Inspired by their notable success in NLP field, autoregressive models have been intensively investigated recently for computer vision, which perform next-token predictions by representing visual data as visual tokens and enables autoregressi...

---

## 368. Medxchat: A Unified Multimodal Large Language Model Framework Towards CXRS Understanding and Generation

**Authors:** Ling Yang, Zhanyu Wang, Zhenghao Chen, Xinyu Liang, Luping Zhou

**Year:** 2023 | **Venue:** IEEE International Symposium on Biomedical Imaging | **Citations:** 8 | **Score:** 0.172

[DOI](https://doi.org/10.1109/ISBI60581.2025.10981215)

> Multimodal Large Language Models (MLLMs) have shown promise in general image tasks, but their application in medical imaging remains limited. This study explores MLLMs for Chest X-Rays (CXRs) through MedXChat, a unified framework enabling seamless interaction for tasks such as text report generation, visual question-answering (VQA), and Text-to-CXR generation. Our MLLM, which uses natural language...

---

## 369. Document-level Clinical Entity and Relation extraction via Knowledge Base-Guided Generation

**Authors:** Kriti Bhattarai, Inez Y. Oh, Zach Abrams, Albert M Lai

**Year:** 2024 | **Venue:** Workshop on Biomedical Natural Language Processing | **Citations:** 7 | **Score:** 0.171

[DOI](https://doi.org/10.48550/arXiv.2407.10021)

> Generative pre-trained transformer (GPT) models have shown promise in clinical entity and relation extraction tasks because of their precise extraction and contextual understanding capability. In this work, we further leverage the Unified Medical Language System (UMLS) knowledge base to accurately identify medical concepts and improve clinical entity and relation extraction at the document level. ...

---

## 370. Biomedical text readability after hypernym substitution with fine-tuned large language models

**Authors:** Karl Swanson, Shuhan He, Josh Calvano, David Chen, Talar Telvizian

**Year:** 2024 | **Venue:** PLOS Digital Health | **Citations:** 7 | **Score:** 0.169

[PDF](https://journals.plos.org/digitalhealth/article/file?id=10.1371/journal.pdig.0000489&type=printable) | [DOI](https://doi.org/10.1371/journal.pdig.0000489)

> The advent of patient access to complex medical information online has highlighted the need for simplification of biomedical text to improve patient understanding and engagement in taking ownership of their health. However, comprehension of biomedical text remains a difficult task due to the need for domain-specific expertise. We aimed to study the simplification of biomedical text via large langu...

---

## 371. Inspiring the Next Generation of Segment Anything Models: Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards Context-Dependent Concepts under Different Scenes

**Authors:** Xiaoqi Zhao, Youwei Pang, Shijie Chang, Yuan Zhao, Lihe Zhang

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 8 | **Score:** 0.169

[DOI](https://doi.org/10.48550/arXiv.2412.01240)

> As large-scale foundation models trained on billions of image--mask pairs covering a vast diversity of scenes, objects, and contexts, SAM and its upgraded version, SAM~2, have significantly influenced multiple fields within computer vision. Leveraging such unprecedented data diversity, they exhibit strong open-world segmentation capabilities, with SAM~2 further enhancing these capabilities to supp...

---

## 372. CATD: Unified Representation Learning for EEG-to-fMRI Cross-Modal Generation

**Authors:** Weiheng Yao, Zhihan Lyu, Mufti Mahmud, Ning Zhong, Baiying Lei

**Year:** 2024 | **Venue:** IEEE Transactions on Medical Imaging | **Citations:** 11 | **Score:** 0.168

[PDF](http://arxiv.org/pdf/2408.00777) | [DOI](https://doi.org/10.1109/TMI.2025.3550206)

> Multi-modal neuroimaging analysis is crucial for a comprehensive understanding of brain function and pathology, as it allows for the integration of different imaging techniques, thus overcoming the limitations of individual modalities. However, the high costs and limited availability of certain modalities pose significant challenges. To address these issues, this paper proposes the Condition-Align...

---

## 373. Behavioral Intention to Use Telemedicine Applications Among Millennial and Generation Z in Indonesia

**Authors:** N. Hidayah, Meinarini Catur Utami, Irfan Nur Rizki, Aang Subyakto, Evy Nurmiati

**Year:** 2024 | **Venue:** 2024 12th International Conference on Cyber and IT Service Management (CITSM) | **Citations:** 3 | **Score:** 0.164

[DOI](https://doi.org/10.1109/CITSM64103.2024.10775387)

> Telemedicine applications such as Halodoc, Alodokter, and KlikDokter serve medical consultation and treatment which are vital aspects of human life. However, in practice there are still users who experience problems in using the application. Based on reviews on the Google Play Store related to the telemedicine application, there are users who complain about the length of response time, application...

---

## 374. Data Lakehouse: Next Generation Information System

**Authors:** Mohamed Cherradi

**Year:** 2024 | **Venue:** Seminars in Medical Writing and Education | **Citations:** 7 | **Score:** 0.163

[PDF](https://mw.saludcyt.ar/index.php/mw/article/download/67/158) | [DOI](https://doi.org/10.56294/mw202467)

> This paper introduces the Data Lakehouse Architecture, a transformative model in data architecture that seamlessly integrates the analytical strengths of traditional data warehouses with the schema flexibility inherent in data lakes. Departing from current frameworks, this comprehensive approach establishes a unified platform, overcoming limitations of conventional data management. Addressing the ...

---

## 375. [Chinese expert consensus on the analytical validation of tumor comprehensive genomic profiling next generation sequencing testing (2024 edition)].

**Authors:** 

**Year:** 2024 | **Venue:** Zhonghua zhong liu za zhi [Chinese journal of oncology] | **Citations:** N/A | **Score:** 0.160

[DOI](https://doi.org/10.3760/cma.j.cn112152-20231027-00277)

> ...

---

## 376. Enabling the Informed Patient Paradigm with Secure and Personalized Medical Question Answering

**Authors:** Joel Oduro-Afriyie, Hasan M. Jamil

**Year:** 2023 | **Venue:** ACM International Conference on Bioinformatics, Computational Biology and Biomedicine | **Citations:** 7 | **Score:** 0.155

[PDF](https://dl.acm.org/doi/pdf/10.1145/3584371.3613016) | [DOI](https://doi.org/10.1145/3584371.3613016)

> Quality patient care is a complex and multifaceted problem requiring the integration of data from multiple sources. We propose Medicient, a knowledge-graph-based question answering system that processes heterogeneous data sources, including patient health records, drug databases, and medical literature, into a unified knowledge graph with zero training. The knowledge graph is then utilized to prov...

---

## 377. A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction

**Authors:** Zefa Hu, Ziyi Ni, Jing Shi, Shuang Xu, Bo Xu

**Year:** 2023 | **Venue:** Machine Intelligence Research | **Citations:** 5 | **Score:** 0.153

[PDF](http://arxiv.org/pdf/2307.16200) | [DOI](https://doi.org/10.1007/s11633-023-1461-5)

> This paper focuses on term-status pair extraction from medical dialogues (MD-TSPE), which is essential in diagnosis dialogue systems and the automatic scribe of electronic medical records (EMRs). In the past few years, works on MD-TSPE have attracted increasing research attention, especially after the remarkable progress made by generative methods. However, these generative methods output a whole ...

---

## 378. The Generation of a Lung Cancer Health Factor Distribution Using Patient Graphs Constructed From Electronic Medical Records: Retrospective Study

**Authors:** Anjun Chen, Ran Huang, Erman Wu, Ruobing Han, Jianpei Wen

**Year:** 2022 | **Venue:** Journal of Medical Internet Research | **Citations:** 5 | **Score:** 0.129

[PDF](https://www.jmir.org/2022/11/e40361/PDF) | [DOI](https://doi.org/10.2196/40361)

> Background Electronic medical records (EMRs) of patients with lung cancer (LC) capture a variety of health factors. Understanding the distribution of these factors will help identify key factors for risk prediction in preventive screening for LC. Objective We aimed to generate an integrated biomedical graph from EMR data and Unified Medical Language System (UMLS) ontology for LC, and to generate a...

---

## 379. Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical Procedures

**Authors:** Ege Özsoy, Evin Pınar Örnek, U. Eck, F. Tombari, N. Navab

**Year:** 2021 | **Venue:** arXiv.org | **Citations:** 7 | **Score:** 0.114

> From a computer science viewpoint, a surgical domain model needs to be a conceptual one incorporating both behavior and data. It should therefore model actors, devices, tools, their complex interactions and data flow. To capture and model these, we take advantage of the latest computer vision methodologies for generating 3D scene graphs from camera views. We then introduce the Multimodal Semantic ...

---

## 380. A Unifying Theory of Aging between Modern Medicine and Traditional Chinese Medicine

**Authors:** Kai-Chit Cheung, H. Leung, K. Ko

**Year:** 2020 | **Venue:**  | **Citations:** 4 | **Score:** 0.087

[PDF](http://www.scirp.org/journal/PaperDownload.aspx?paperID=100921) | [DOI](https://doi.org/10.4236/cm.2020.112006)

> With the increasing aging population around the 
world as a result of birth rates and advances in medical technologies, there is 
an urgent need to unravel the primary cause of aging, in the hope of developing 
a rational approach to retard the aging process. This is crucial to reduce the 
societal impact of aging. Although modern medicine and traditional Chinese 
medicine view the process of agin...

---

