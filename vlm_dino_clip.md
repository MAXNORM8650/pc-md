# Research Papers: Vision-Language Models

Updated: 2026-01-26 10:46
Total: 880 papers

---

## 1. Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection

**Authors:** Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang

**Year:** 2023 | **Venue:** European Conference on Computer Vision | **Citations:** 3320 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2303.05499) | [DOI](https://doi.org/10.48550/arXiv.2303.05499)

> In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively f...

---

## 2. DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection

**Authors:** Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su

**Year:** 2022 | **Venue:** International Conference on Learning Representations | **Citations:** 2230 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2203.03605) | [DOI](https://doi.org/10.48550/arXiv.2203.03605)

> We present DINO (\textbf{D}ETR with \textbf{I}mproved de\textbf{N}oising anch\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DIN...

---

## 3. DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning

**Authors:** Gaoyue Zhou, Hengkai Pan, Yann LeCun, Lerrel Pinto

**Year:** 2024 | **Venue:** International Conference on Machine Learning | **Citations:** 104 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2411.04983)

> The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, remains challenging to learn and are typically developed for task-specific solutions with online policy learning. To unlock world models' true potential, we argue that they should 1) be trainable on offline, pre-collected trajectories, 2) s...

---

## 4. Back to the Features: DINO as a Foundation for Video World Models

**Authors:** Federico Baldassarre, Marc Szafraniec, Basile Terver, Vasil Khalidov, Francisco Massa

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 26 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2507.19468)

> We present DINO-world, a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on a large-scale uncurated video dataset, DINO-world learns the temporal dynamics of diverse scenes, from driving and indoor scenes to simulated environments. We show that DINO-world outperforms prev...

---

## 5. Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation

**Authors:** Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang

**Year:** 2022 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 527 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2206.02777) | [DOI](https://doi.org/10.1109/CVPR52729.2023.00297)

> In this paper we present Mask DINO, a unified object detection and segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). It makes use of the query embeddings from DINO to dot-product a high-resolution pixel embedding map to predict a set of binary ...

---

## 6. Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection

**Authors:** Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 95 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.10300)

> This paper introduces Grounding DINO 1.5, a suite of advanced open-set object detection models developed by IDEA Research, which aims to advance the"Edge"of open-set object detection. The suite encompasses two models: Grounding DINO 1.5 Pro, a high-performance model designed for stronger generalization capability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an efficient model opt...

---

## 7. RDB-DINO: An Improved End-to-End Transformer With Refined De-Noising and Boxes for Small-Scale Ship Detection in SAR Images

**Authors:** Chuan Qin, Linping Zhang, Xueqian Wang, Gang Li, You He

**Year:** 2025 | **Venue:** IEEE Transactions on Geoscience and Remote Sensing | **Citations:** 17 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TGRS.2024.3515150)

> Recently, convolution neural networks (CNNs) have been extensively utilized in synthetic aperture radar (SAR) ship detection owing to their strong feature extraction and representation capability. However, existing CNN-based SAR ship detectors often suffer from poor sensitivity to small-scale ship targets due to the limited extractable features, especially in complex inshore scenarios. Moreover, t...

---

## 8. A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence

**Authors:** Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani

**Year:** 2023 | **Venue:** Neural Information Processing Systems | **Citations:** 288 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2305.15347) | [DOI](https://doi.org/10.48550/arXiv.2305.15347)

> Text-to-image diffusion models have made significant advances in generating and editing high-quality images. As a result, numerous approaches have explored the ability of diffusion model features to understand and process single images for downstream tasks, e.g., classification, semantic segmentation, and stylization. However, significantly less is known about what these features reveal across mul...

---

## 9. DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding

**Authors:** Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 62 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2411.14347)

> In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extend...

---

## 10. Simplifying DINO via Coding Rate Regularization

**Authors:** Ziyang Wu, Jingyuan Zhang, Druv Pai, XuDong Wang, Chandan Singh

**Year:** 2025 | **Venue:** International Conference on Machine Learning | **Citations:** 11 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2502.10385)

> DINO and DINOv2 are two model families being widely used to learn representations from unlabeled imagery data at large scales. Their learned representations often enable state-of-the-art performance for downstream tasks, such as image classification and segmentation. However, they employ many empirically motivated design choices and their training pipelines are highly complex and unstable -- many ...

---

## 11. Frozen CLIP-DINO: A Strong Backbone for Weakly Supervised Semantic Segmentation

**Authors:** Bingfeng Zhang, Siyue Yu, Jimin Xiao, Yunchao Wei, Yao Zhao

**Year:** 2025 | **Venue:** IEEE Transactions on Pattern Analysis and Machine Intelligence | **Citations:** 13 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TPAMI.2025.3543191)

> Weakly supervised semantic segmentation has witnessed great achievements with image-level labels. Several recent approaches use the CLIP model to generate pseudo labels for training an individual segmentation model, while there is no attempt to apply the CLIP model as the backbone to directly segment objects with image-level labels. In this paper, we propose WeCLIP and its advanced version WeCLIP+...

---

## 12. DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video

**Authors:** Narek Tumanyan, Assaf Singer, Shai Bagon, Tali Dekel

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 61 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.14548)

> We present DINO-Tracker -- a new framework for long-term dense tracking in video. The pillar of our approach is combining test-time training on a single video, with the powerful localized semantic features learned by a pre-trained DINO-ViT model. Specifically, our framework simultaneously adopts DINO's features to fit to the motion observations of the test video, while training a tracker that dire...

---

## 13. Surgical-DINO: adapter learning of foundation models for depth estimation in endoscopic surgery

**Authors:** Beilei Cui, Mobarak Islam Hoque, Long Bai, Hongliang Ren

**Year:** 2024 | **Venue:** International Journal of Computer Assisted Radiology and Surgery | **Citations:** 56 | **Score:** 0.000

[PDF](https://link.springer.com/content/pdf/10.1007/s11548-024-03083-5.pdf) | [DOI](https://doi.org/10.1007/s11548-024-03083-5)

> Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the...

---

## 14. DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation

**Authors:** Karim Knaebel, Kadir Yilmaz, Daan de Geus, Alexander Hermans, David B. Adrian

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 13 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2503.18944)

> Vision foundation models (VFMs) trained on large-scale image datasets provide high-quality features that have significantly advanced 2D visual recognition. However, their potential in 3D scene segmentation remains largely untapped, despite the common availability of 2D images alongside 3D point cloud datasets. While significant research has been dedicated to 2D-3D fusion, recent state-of-the-art 3...

---

## 15. DINO-Reg: Efficient Multimodal Image Registration With Distilled Features

**Authors:** Xin Song, Xuanang Xu, Jiajin Zhang, D. Machado Reyes, Pingkun Yan

**Year:** 2025 | **Venue:** IEEE Transactions on Medical Imaging | **Citations:** 11 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TMI.2025.3567247)

> Medical image registration is a crucial process for aligning anatomical structures, enabling applications such as atlas mapping, longitudinal analysis, and multimodal data fusion. This paper introduces DINO-Reg, an adaptation-free registration method leveraging the vision foundation model, DINOv2, to extract features for deformable 3D medical image alignment. Although DINOv2 was originally trained...

---

## 16. DINO-MOT: 3D Multi-Object Tracking With Visual Foundation Model for Pedestrian Re-Identification Using Visual Memory Mechanism

**Authors:** Min Young Lee, Christina Dao Wen Lee, Jianghao Li, Marcelo H. Ang

**Year:** 2025 | **Venue:** IEEE Robotics and Automation Letters | **Citations:** 10 | **Score:** 0.000

[DOI](https://doi.org/10.1109/lra.2024.3500882)

> In the advancing domain of autonomous driving, this research focuses on enhancing 3D Multi-Object Tracking (3D-MOT). Pedestrians are particularly vulnerable in urban environments, and robust tracking methodologies are required to understand their movements. Prevalent Tracking-By-Detection (TBD) frameworks often underutilize the rich visual data from sensors such as cameras. This study leverages th...

---

## 17. DOGAN: DINO-Based Optical-Prior-Driven GAN for SAR-to-Optical Image Translation

**Authors:** Jingfei He, Liang Chen, Hao Shi, Yuhang Chen, Jingyi Yang

**Year:** 2025 | **Venue:** IEEE Transactions on Geoscience and Remote Sensing | **Citations:** 6 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TGRS.2025.3606979)

> To leverage the complementary advantages of synthetic aperture radar’s (SAR) all-weather and all-day imaging capability and optical imagery’s intuitive visualization, SAR-to-optical image translation (S2OIT) has emerged as a promising solution to mitigate the interpretability challenges posed by SAR’s speckle noise and geometric distortions. However, the scale of high-quality registered SAR-optica...

---

## 18. Efficient Grounding DINO: Efficient Cross-Modality Fusion and Efficient Label Assignment for Visual Grounding in Remote Sensing

**Authors:** Zibo Hu, Kun Gao, Xiaodian Zhang, Zhijia Yang, Mingfeng Cai

**Year:** 2025 | **Venue:** IEEE Transactions on Geoscience and Remote Sensing | **Citations:** 8 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TGRS.2025.3536015)

> Visual grounding for remote sensing (RSVG) aims to detect objects in remote sensing scenes based on textual descriptions. While existing methods perform well on RSVG datasets, they are limited to single-object predictions, making them unsuitable for multi-object candidate category datasets. Open-set methods can be applied to both RSVG and candidate datasets, but their use in remote sensing remains...

---

## 19. SegDINO: An Efficient Design for Medical and Natural Image Segmentation with DINO-V3

**Authors:** Sicheng Yang, Hongqiu Wang, Zhaohu Xing, Sixiang Chen, Lei Zhu

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 6 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2509.00833)

> The DINO family of self-supervised vision models has shown remarkable transferability, yet effectively adapting their representations for segmentation remains challenging. Existing approaches often rely on heavy decoders with multi-scale fusion or complex upsampling, which introduce substantial parameter overhead and computational cost. In this work, we propose SegDINO, an efficient segmentation f...

---

## 20. OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective Fusion

**Authors:** Hao Wang, Pengzhen Ren, Zequn Jie, Xiao Dong, Chengjian Feng

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 21 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.07844)

> Open-vocabulary detection is a challenging task due to the requirement of detecting objects based on class names, including those not encountered during training. Existing methods have shown strong zero-shot detection capabilities through pre-training and pseudo-labeling on diverse large-scale datasets. However, these approaches encounter two main challenges: (i) how to effectively eliminate data ...

---

## 21. Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation

**Authors:** Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 4 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2508.20909)

> Foundation models pre-trained on large-scale natural image datasets offer a powerful paradigm for medical image segmentation. However, effectively transferring their learned representations for precise clinical applications remains a challenge. In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundat...

---

## 22. PL-DINO: An Improved Transformer-Based Method for Plant Leaf Disease Detection

**Authors:** Wei Li, Lizhou Zhu, Jun Liu

**Year:** 2024 | **Venue:** Agriculture | **Citations:** 18 | **Score:** 0.000

[DOI](https://doi.org/10.3390/agriculture14050691)

> Agriculture is important for ecology. The early detection and treatment of agricultural crop diseases are meaningful and challenging tasks in agriculture. Currently, the identification of plant diseases relies on manual detection, which has the disadvantages of long operation time and low efficiency, ultimately impacting the crop yield and quality. To overcome these disadvantages, we propose a new...

---

## 23. Enhancing Human Detection in Occlusion-Heavy Disaster Scenarios: A Visibility-Enhanced DINO (VE-DINO) Model with Reassembled Occlusion Dataset

**Authors:** Zian Zhao, Shidan Wang, Minxin Chen, Ye-Jiao Mao, Andy Chi-Ho Chan

**Year:** 2025 | **Venue:** Smart Cities | **Citations:** 3 | **Score:** 0.000

[DOI](https://doi.org/10.3390/smartcities8010012)

> Natural disasters create complex environments where effective human detection is both critical and challenging, especially when individuals are partially occluded. While recent advancements in computer vision have improved detection capabilities, there remains a significant need for efficient solutions that can enhance search-and-rescue (SAR) operations in resource-constrained disaster scenarios. ...

---

## 24. Few-Shot Adaptation of Grounding DINO for Agricultural Domain

**Authors:** Rajhans Singh, Rafael Bidese Puhl, Kshitiz Dhakal, Sudhir Sornapudi

**Year:** 2025 | **Venue:** 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) | **Citations:** 3 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPRW67362.2025.00530)

> Deep learning models are transforming agricultural applications by enabling automated phenotyping, monitoring, and yield estimation. However, their effectiveness heavily depends on large amounts of annotated training data, which can be labor and time intensive. Recent advances in open-set object detection, particularly with models like Grounding-DINO, offer a potential solution to detect regions o...

---

## 25. GDPGO-SAM: An Unsupervised Fine Segmentation of Desert Vegetation Driven by Grounding DINO Prompt Generation and Optimization Segment Anything Model

**Authors:** Shuzhen Hua, Biao Yang, Xinchang Zhang, Ji Qi, Fengxi Su

**Year:** 2025 | **Venue:** Remote Sensing | **Citations:** 2 | **Score:** 0.000

[PDF](https://doi.org/10.3390/rs17040691) | [DOI](https://doi.org/10.3390/rs17040691)

> Desert encroachment significantly threatens the living and activity space of humanity, and undertaking human-directed vegetation restoration is one of the effective ways to prevent desert expansion. In the process of desert vegetation restoration, counting the number of tree saplings for rapidly assessing the survival rate of vegetation (such as Haloxylon ammodendron) is a critical task within the...

---

## 26. DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models

**Authors:** Chenbin Pan, Wenbin He, Zhengzhong Tu, Liu Ren

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2505.24025)

> The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representati...

---

## 27. Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection

**Authors:** Yehao Lu, Minghe Weng, Zekang Xiao, Rui Jiang, Wei Su

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2507.17436)

> The Mixture of Experts (MoE) architecture has excelled in Large Vision-Language Models (LVLMs), yet its potential in real-time open-vocabulary object detectors, which also leverage large-scale vision-language datasets but smaller models, remains unexplored. This work investigates this domain, revealing intriguing insights. In the shallow layers, experts tend to cooperate with diverse peers to expa...

---

## 28. Zero-Shot Day–Night Domain Adaptation for Face Detection Based on DAl-CLIP-Dino

**Authors:** Huadong Sun, Yinghui Liu, Ziyang Chen, Pengyi Zhang

**Year:** 2025 | **Venue:** Electronics | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.3390/electronics14010143)

> Two challenges in computer vision (CV) related to face detection are the difficulty of acquisition in the target domain and the degradation of image quality. Especially in low-light situations, the poor visibility of images is difficult to label, which results in detectors trained under well-lit conditions exhibiting reduced performance in low-light environments. Conventional works image enhanceme...

---

## 29. Enhancing nuclear cross-section predictions with deep learning: the DINo algorithm

**Authors:** L. Gesson, Greg Henning, Jonathan Collin, M. Vanstalle

**Year:** 2025 | **Venue:** The European Physical Journal Plus | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.1140/epjp/s13360-025-06562-z)

> Accurate modeling of nuclear reaction cross-sections is crucial for applications such as hadron therapy, radiation protection, and nuclear reactor design. Despite continuous advancements in nuclear physics, significant discrepancies persist between experimental data and evaluated nuclear data libraries such as TENDL (TALYS-based Evaluated Nuclear Data Library) and ENDF/B (Evaluated Nuclear Data Fi...

---

## 30. Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation

**Authors:** Luca Barsellotti, Lorenzo Bianchi, Nicola Messina, F. Carrara, Marcella Cornia

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 20 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2411.19331)

> Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form textual concepts without predefined training classes. While existing vision-language models such as CLIP can generate segmentation masks by leveraging coarse spatial information from Vision Transformers, they face challenges in spatial localization due to their global alignment of image and text features. Conversely, self-...

---

## 31. DINO-VO: A Feature-Based Visual Odometry Leveraging a Visual Foundation Model

**Authors:** Maulana Bisyir Azhari, D. Shim

**Year:** 2025 | **Venue:** IEEE Robotics and Automation Letters | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.1109/LRA.2025.3592065)

> Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this letter, we present DINO-VO, a feature-based VO system leveraging...

---

## 32. Data or Language Supervision: What Makes CLIP Better than DINO?

**Authors:** Yiming Liu, Yuhui Zhang, D. Ghosh, Ludwig Schmidt, S. Yeung-Levy

**Year:** 2025 | **Venue:** Conference on Empirical Methods in Natural Language Processing | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.18653/v1/2025.findings-emnlp.98)

> CLIP outperforms self-supervised models like DINO as vision encoders for vision-language models (VLMs), but it remains unclear whether this advantage stems from CLIP's language supervision or its much larger training data. To disentangle these factors, we pre-train CLIP and DINO under controlled settings -- using the same architecture, dataset, and training configuration -- achieving similar Image...

---

## 33. DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning

**Authors:** Mahmut S. Gokmen, C. Bumgardner

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2511.01610)

> Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv...

---

## 34. DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations

**Authors:** Ziren Gong, Xiaohan Li, Fabio Tosi, Youmin Zhang, Stefano Mattoccia

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2507.19474)

> This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene element...

---

## 35. VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging

**Authors:** Andre Dourson, Kylie Taylor, Xiaoli Qiao, Michael Fitzke

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2505.15248)

> Self-supervised learning has emerged as a powerful paradigm for training deep neural networks, particularly in medical imaging where labeled data is scarce. While current approaches typically rely on synthetic augmentations of single images, we propose VET-DINO, a framework that leverages a unique characteristic of medical imaging: the availability of multiple standardized views from the same stud...

---

## 36. Text-guided Visual Prompt DINO for Generic Segmentation

**Authors:** Yuchen Guan, Chong Sun, Canmiao Fu, Zhipeng Huang, Chun Yuan

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2508.06146)

> Recent advancements in multimodal vision models have highlighted limitations in late-stage feature fusion and suboptimal query selection for hybrid prompts open-world segmentation, alongside constraints from caption-derived vocabularies. To address these challenges, we propose Prompt-DINO, a text-guided visual Prompt DINO framework featuring three key innovations. First, we introduce an early fusi...

---

## 37. Optimizing zero-shot text-based segmentation of remote sensing imagery using SAM and Grounding DINO

**Authors:** Mohanad Diab, Pol Kolokoussis, M. Brovelli

**Year:** 2025 | **Venue:** Artificial Intelligence in Geosciences | **Citations:** 5 | **Score:** 0.000

[DOI](https://doi.org/10.1016/j.aiig.2025.100105)

> ...

---

## 38. CAFF-DINO: Multi-spectral object detection transformers with cross-attention features fusion

**Authors:** Kevin Helvig, Baptiste Abeloos, P. Trouvé-Peloux

**Year:** 2024 | **Venue:** 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) | **Citations:** 16 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPRW63382.2024.00309)

> Object detection on images can find benefit from coupling multiple spectra, each presenting specific useful features. However, building an efficient architecture coupling the different modalities is a complex task. Transformers, due to their ability to extract meaningful correlations between the different regions of the inputs appear as a promising way to perform features fusion across different s...

---

## 39. DINO as a von Mises-Fisher mixture model

**Authors:** Hariprasath Govindarajan, Per Sidén, Jacob Roll, F. Lindsten

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 17 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.10939)

> Self-distillation methods using Siamese networks are popular for self-supervised pre-training. DINO is one such method based on a cross-entropy loss between $K$-dimensional probability vectors, obtained by applying a softmax function to the dot product between representations and learnt prototypes. Given the fact that the learned representations are $L^2$-normalized, we show that DINO and its deri...

---

## 40. DINO-Foresight: Looking into the Future with DINO

**Authors:** Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 15 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2412.11673)

> Predicting future dynamics is crucial for applications like autonomous driving and robotics, where understanding the environment is key. Existing pixel-level methods are computationally expensive and often focus on irrelevant details. To address these challenges, we introduce DINO-Foresight, a novel framework that operates in the semantic feature space of pretrained Vision Foundation Models (VFMs)...

---

## 41. Deformable One-Shot Face Stylization via DINO Semantic Guidance

**Authors:** Yang Zhou, Zichong Chen, Hui Huang

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 15 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2403.00459) | [DOI](https://doi.org/10.1109/CVPR52733.2024.00744)

> This paper addresses the complex issue of one-shot face stylization, focusing on the simultaneous consideration of appearance and structure, where previous methods have fallen short. We explore deformation-aware face stylization that diverges from traditional single-image style reference, opting for a real-style image pair instead. The cornerstone of our method is the utilization of a self-supervi...

---

## 42. Bentuk Dan Fungsi Ornamen Pada Kain Tenun (Puta Dino) Di Kota Tidore Kepulauan Provinsi Maluku Utara

**Authors:** Bentuk dan, Fungsi Ornamen, Pada Kain, Tenun Puta, Dino Di

**Year:** 2025 | **Venue:** Jambura: Jurnal Seni dan Desain | **Citations:** N/A | **Score:** 0.000

[DOI](https://doi.org/10.37905/jjsd.v4i2.24075)

> Penelitian ini bertujuan untuk mengungkap dan menjelaskan bentuk dan fungsi ornamen pada kain tenun (puta dino) di Kota Tidore Kepulauan Provinsi Maluku Utara. Penelitian menggunakan metode kualitatif deskriptif. Data dikumpulkan dengan cara observasi, wawancara, dan studi dokumen. Analisis data dilakukan melalui, verifikasi data, reduksi data, penyajian data, pembahasan, dan penarikan kesimpulan....

---

## 43. From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models

**Authors:** Dongsheng Jiang, Yuchen Liu, Songlin Liu, Jin'e Zhao, Hao Zhang

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 70 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2310.08825) | [DOI](https://doi.org/10.48550/arXiv.2310.08825)

> Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract feature...

---

## 44. DIG: Improved DINO for Graffiti Detection

**Authors:** Bingshu Wang, Qianchen Mao, Ieee C. L. Philip Chen Life Fellow, Aifei Liu, Long Chen

**Year:** 2025 | **Venue:** IEEE Transactions on Systems, Man, and Cybernetics: Systems | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TSMC.2025.3541795)

> Graffiti detection is essential in historic building protection and urban neighborhood management. Graffiti detection has made significant progress in recent years based on the development of deep learning. However, small-scale graffiti, interference from the background, and the false detection of word parts in graffiti make it a challenging problem. This article proposes a Transformer-based high-...

---

## 45. DINO-Mix enhancing visual place recognition with foundational vision model and feature mixing

**Authors:** Gaoshuang Huang, Yang Zhou, Xiaofei Hu, Chenglong Zhang, Luying Zhao

**Year:** 2024 | **Venue:** Scientific Reports | **Citations:** 12 | **Score:** 0.000

[PDF](https://doi.org/10.1038/s41598-024-73853-3) | [DOI](https://doi.org/10.1038/s41598-024-73853-3)

> Using visual place recognition (VPR) technology to ascertain the geographical location of publicly available images is a pressing issue. Although most current VPR methods achieve favorable results under ideal conditions, their performance in complex environments, characterized by lighting variations, seasonal changes, and occlusions, is generally unsatisfactory. Therefore, obtaining efficient and ...

---

## 46. An Improved Normalized Difference Vegetation Index (NDVI) Estimation Using Grounded Dino and Segment Anything Model for Plant Health Classification

**Authors:** A. Balasundaram, Alabhya Sharma, Swaathy Kumaravelan, Ayesha Shaik, M. Kavitha

**Year:** 2024 | **Venue:** IEEE Access | **Citations:** 12 | **Score:** 0.000

[PDF](https://ieeexplore.ieee.org/ielx7/6287639/6514899/10535513.pdf) | [DOI](https://doi.org/10.1109/ACCESS.2024.3403520)

> Ensuring sustainable and profitable agriculture is critical for addressing global food security challenges. This has resulted in the need for automation in plant health identification. However, this objective is hampered by the lack of efficient image-processing methods and the need for extensive datasets for training deep learning models for plant disease diagnosis. To overcome the need for exten...

---

## 47. DINO is Also a Semantic Guider: Exploiting Class-aware Affinity for Weakly Supervised Semantic Segmentation

**Authors:** Yuanchen Wu, Xiaoqiang Li, Jide Li, Kequan Yang, Pinpin Zhu

**Year:** 2024 | **Venue:** ACM Multimedia | **Citations:** 11 | **Score:** 0.000

[DOI](https://doi.org/10.1145/3664647.3681710)

> Weakly supervised semantic segmentation (WSSS) using image-level labels is a challenging task, with relying on Class Activation Map (CAM) to derive segmentation supervision. Although many efficient single-stage solutions have been proposed, their performance is hindered by the inherent ambiguity of CAM. This paper introduces a new approach, dubbed ECA, to Exploit the self-supervised Vision Transfo...

---

## 48. CR-DINO: A Novel Camera-Radar Fusion 2-D Object Detection Model Based on Transformer

**Authors:** Yuhao Jin, Xiaohui Zhu, Yong Yue, Eng Gee Lim, Wei Wang

**Year:** 2024 | **Venue:** IEEE Sensors Journal | **Citations:** 11 | **Score:** 0.000

[DOI](https://doi.org/10.1109/JSEN.2024.3357775)

> Due to millimeter-wave (MMW) radar’s ability to directly acquire spatial positions and velocity information of objects, as well as its robust performance in adverse weather conditions, it has been widely employed in autonomous driving. However, radar lacks specific semantic information. To address this limitation, we take the complementary strengths of camera and radar by feature-level fusion and ...

---

## 49. Teacher–Student Model Using Grounding DINO and You Only Look Once for Multi-Sensor-Based Object Detection

**Authors:** Jinhwan Son, Heechul Jung

**Year:** 2024 | **Venue:** Applied Sciences | **Citations:** 10 | **Score:** 0.000

[PDF](https://www.mdpi.com/2076-3417/14/6/2232/pdf?version=1709799294) | [DOI](https://doi.org/10.3390/app14062232)

> Object detection is a crucial research topic in the fields of computer vision and artificial intelligence, involving the identification and classification of objects within images. Recent advancements in deep learning technologies, such as YOLO (You Only Look Once), Faster-R-CNN, and SSDs (Single Shot Detectors), have demonstrated high performance in object detection. This study utilizes the YOLOv...

---

## 50. DINO-Reg: General Purpose Image Encoder for Training-Free Multi-modal Deformable Medical Image Registration

**Authors:** Xin Song, Xuanang Xu, Pingkun Yan

**Year:** 2024 | **Venue:** International Conference on Medical Image Computing and Computer-Assisted Intervention | **Citations:** 15 | **Score:** 0.000

[DOI](https://doi.org/10.1007/978-3-031-72069-7_57)

> ...

---

## 51. DIVE: Taming DINO for Subject-Driven Video Editing

**Authors:** Yi Huang, Wei Xiong, He Zhang, Chaoqi Chen, Jianzhuang Liu

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 7 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2412.03347)

> Building on the success of diffusion models in image generation and editing, video editing has recently gained substantial attention. However, maintaining temporal consistency and motion alignment still remains challenging. To address these issues, this paper proposes DINO-guided Video Editing (DIVE), a framework designed to facilitate subject-driven editing in source videos conditioned on either ...

---

## 52. DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised Pre-Training

**Authors:** Guillermo Jiménez-Pérez, Pedro Osório, Josef Cersovsky, Javier Montalt-Tordera, Jens Hooge

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 4 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.11594)

> Diffusion models (DMs) have emerged as powerful foundation models for a variety of tasks, with a large focus in synthetic image generation. However, their requirement of large annotated datasets for training limits their applicability in medical imaging, where datasets are typically smaller and sparsely annotated. We introduce DiNO-Diffusion, a self-supervised method for training latent diffusion ...

---

## 53. On Partial Prototype Collapse in the DINO Family of Self-Supervised Methods

**Authors:** Hariprasath Govindarajan, Per Sid'en, Jacob Roll, Fredrik Lindsten

**Year:** 2024 | **Venue:** British Machine Vision Conference | **Citations:** 4 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2410.14060)

> A prominent self-supervised learning paradigm is to model the representations as clusters, or more generally as a mixture model. Learning to map the data samples to compact representations and fitting the mixture model simultaneously leads to the representation collapse problem. Regularizing the distribution of data points over the clusters is the prevalent strategy to avoid this issue. While this...

---

## 54. Brain Tumor Images Class-Based and Prompt-Based Detectors and Segmenter: Performance Evaluation of YOLO, SAM and Grounding DINO

**Authors:** Eduardo H. Teixeira, Mateus C. Melo, Wellington B. C. Junior, Elaine C. C. Silva, Mateus R. Cruz.

**Year:** 2024 | **Venue:** 2024 International Conference on Artificial Intelligence, Blockchain, Cloud Computing, and Data Analytics (ICoABCD) | **Citations:** 3 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICoABCD63526.2024.10704507)

> Early tumor diagnosis is crucial for effective treatment and improved clinical outcomes in oncology. However, detecting and segmenting tumors in magnetic resonance imaging (MRI) poses significant challenges due to the complexity of the data and the increasing volume of medical imaging. This study analyzes machine learning-based computer vision techniques to automate these processes. We evaluate th...

---

## 55. Oriented-DINO: Angle Decoupling Prediction and Consistency Optimizing for Oriented Detection Transformer

**Authors:** Minjian Zhang, Heqian Qiu, Lanxiao Wang, Haoyang Cheng, Taijin Zhao

**Year:** 2024 | **Venue:** IEEE Transactions on Geoscience and Remote Sensing | **Citations:** 4 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TGRS.2024.3450200)

> Considering the arbitrary orientation of remote sensing objects, accurate angle prediction plays a crucial role in achieving precise oriented object detection (OOD) of aerial scenes. Existing transformer-based methods typically adopt an iterative refinement mechanism to update angle prediction and perform bipartite graph matching based on the combined matching costs. However, these methods may suf...

---

## 56. Compact DINO-ViT: Feature Reduction for Visual Transformer

**Authors:** Didih Rizki Chandranegara, Przemysław Niedziela, Bogusław Cyganek

**Year:** 2024 | **Venue:** Electronics | **Citations:** 3 | **Score:** 0.000

[PDF](https://doi.org/10.3390/electronics13234694) | [DOI](https://doi.org/10.3390/electronics13234694)

> Research has been ongoing for years to discover image features that enable their best classification. One of the latest developments in this area is the Self-Distillation with No Labels Vision Transformer—DINO-ViT features. However, even for a single image, their volume is significant. Therefore, for this article we proposed to substantially reduce their size, using two methods: Principal Componen...

---

## 57. Intelligent Auto Annotation System for Identifying Vehicles without Number Plates Using Grounding DINO and Segment Anything Model

**Authors:** R. V, Swetha Jaganmohan, K. H., Varsha Thomas, Senthil Pandi S

**Year:** 2024 | **Venue:** International Conference on Advanced Infocomm Technology | **Citations:** 5 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICAIT61638.2024.10690781)

> It is difficult to automatically annotate things in pictures and movies, especially when the items have different properties, such as cars without license plates. In this study, we offer a unique method that combines the Segments Anything Model (SAM) with Grounding DINO (the identification of noisy objects) for the auto-annotation of automobiles without number plates. Modern object identification ...

---

## 58. DINO Pre-training for Vision-based End-to-end Autonomous Driving

**Authors:** Shubham Juneja, P. Daniušis, Virginijus Marcinkevičius

**Year:** 2024 | **Venue:** Baltic Journal of Modern Computing | **Citations:** 3 | **Score:** 0.000

[PDF](https://doi.org/10.22364/bjmc.2024.12.4.02) | [DOI](https://doi.org/10.22364/bjmc.2024.12.4.02)

> In this article, we focus on the pre-training of visual autonomous driving agents in the context of imitation learning. Current methods often rely on a classification-based pre-training, which we hypothesise to be holding back from extending capabilities of implicit image understanding. We propose pre-training the visual encoder of a driving agent using the self-distillation with no labels (DINO) ...

---

## 59. WEA-DINO: An Improved DINO With Word Embedding Alignment for Remote Scene Zero-Shot Object Detection

**Authors:** Guangbiao Wang, Hongbo Zhao, Qing Chang, Shuchang Lyu, Guangliang Cheng

**Year:** 2024 | **Venue:** IEEE Geoscience and Remote Sensing Letters | **Citations:** 3 | **Score:** 0.000

[DOI](https://doi.org/10.1109/LGRS.2024.3408875)

> Remote sensing scene zero-shot object detection (ZSD) aims to detect and recognize both seen and unseen categories of landscape elements with the guidance of the word embeddings. In this task, two primary challenges are identified. First, there exists considerable variability within categories of landscape elements, causing a misalignment between visual features and word embeddings, particularly n...

---

## 60. I-DINO: High-Quality Object Detection for Indoor Scenes

**Authors:** Zhipeng Fan, Wanglong Mei, Wei Liu, Ming Chen, Zeguo Qiu

**Year:** 2024 | **Venue:** Electronics | **Citations:** 4 | **Score:** 0.000

[DOI](https://doi.org/10.3390/electronics13224419)

> Object Detection in Complex Indoor Scenes is designed to identify and categorize objects in indoor settings, with applications in areas such as smart homes, security surveillance, and home service robots. It forms the basis for advanced visual tasks including visual question answering, video description generation, and instance segmentation. Nonetheless, the task faces substantial hurdles due to b...

---

## 61. MLP-DINO: Category Modeling and Query Graphing with Deep MLP for Object Detection

**Authors:** Guiping Cao, Wen-Fong Huang, Xiangyuan Lan, Jianguo Zhang, Dongmei Jiang

**Year:** 2024 | **Venue:** International Joint Conference on Artificial Intelligence | **Citations:** 4 | **Score:** 0.000

[DOI](https://doi.org/10.24963/ijcai.2024/67)

> Popular transformer-based detectors detect objects in a one-to-one manner, where both the bounding box and category of each object are predicted only by the single query, leading to the box-sensitive category predictions. Additionally, the initialization of positional queries solely based on the predicted confidence scores or learnable embeddings neglects the significant spatial interrelation betw...

---

## 62. CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections

**Authors:** Mohamed Fazli Mohamed Imam, Rufael Marew, Jameel Hassan, M. Fiaz, Alham Fikri Aji

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 5 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2411.19346)

> In the era of foundation models, CLIP has emerged as a powerful tool for aligning text&visual modalities into a common embedding space. However, the alignment objective used to train CLIP often results in subpar visual features for fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at extracting rich visual features due to their specialized training paradigm. Yet, these SSL mod...

---

## 63. MS-DINO: Masked Self-Supervised Distributed Learning Using Vision Transformer

**Authors:** Sangjoon Park, Ik-jae Lee, Jun Won Kim, Jong Chul Ye

**Year:** 2024 | **Venue:** IEEE journal of biomedical and health informatics | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.1109/JBHI.2024.3423797)

> Despite promising advancements in deep learning in medical domains, challenges still remain owing to data scarcity, compounded by privacy concerns and data ownership disputes. Recent explorations of distributed-learning paradigms, particularly federated learning, have aimed to mitigate these challenges. However, these approaches are often encumbered by substantial communication and computational o...

---

## 64. Cell DINO: End-to-End Cell Segmentation and Tracking with Transformer

**Authors:** Wei Liao, Lei Luo, Changjian Wang, Chunyuan Zhang

**Year:** 2024 | **Venue:** IEEE International Conference on Bioinformatics and Biomedicine | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.1109/BIBM62325.2024.10821971)

> Precise cell segmentation and tracking are essential in biomedical research, but current methods are often complex and inefficient. We propose Cell DINO, an extension of the Transformer-based Mask DINO, designed for cell segmentation and tracking. By introducing rotated bounding boxes, track queries, and mitosis queries, our model detects, segments, and tracks cells simultaneously. Benchmarked on ...

---

## 65. The Investigation of Performance Comparison for VGG, YOLO, and DINO in Image Classification

**Authors:** Yanqi Chen

**Year:** 2024 | **Venue:** Highlights in Science Engineering and Technology | **Citations:** 2 | **Score:** 0.000

[PDF](https://drpress.org/ojs/index.php/HSET/article/download/18546/18085) | [DOI](https://doi.org/10.54097/9bgem219)

> The rise of artificial intelligence has led to a proliferation of deep learning models, yet there remains a noticeable shortage of comparative analyses, particularly among computer vision models rooted in different design philosophies. As such, this study seeks to delve into the strengths of various models through an examination of their structural attributes, with the aim of offering insights tha...

---

## 66. Self-Supervised Learning With Cluster-Aware-DINO for High-Performance Robust Speaker Verification

**Authors:** Bing Han, Zhengyang Chen, Y. Qian

**Year:** 2023 | **Venue:** IEEE/ACM Transactions on Audio Speech and Language Processing | **Citations:** 36 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2304.05754) | [DOI](https://doi.org/10.1109/TASLP.2023.3331949)

> The automatic speaker verification task has achieved great success using deep learning approaches with a large-scale, manually annotated dataset. However, collecting a significant amount of well-labeled data for system building is very difficult and expensive. Recently, self-supervised speaker verification has attracted a lot of interest due to its no dependency on labeled data. In this article, w...

---

## 67. CLIP-DINOiser: Teaching CLIP a few DINO tricks

**Authors:** Monika Wysocza'nska, Oriane Siméoni, Michael Ramamonjisoa, Andrei Bursuc, Tomasz Trzci'nski

**Year:** 2023 | **Venue:** European Conference on Computer Vision | **Citations:** 64 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2312.12359)

> The popular CLIP model displays impressive zero-shot capabilities thanks to its seamless interaction with arbitrary text prompts. However, its lack of spatial awareness makes it unsuitable for dense computer vision tasks, e.g., semantic segmentation, without an additional fine-tuning step that often uses annotations and can potentially suppress its original open-vocabulary properties. Meanwhile, s...

---

## 68. DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring

**Authors:** Mahmut S. Gokmen, Caner Ozcan, Moneera N. Haque, Cody Bumgardner

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2411.07976)

> Coronary artery disease (CAD), one of the leading causes of mortality worldwide, necessitates effective risk assessment strategies, with coronary artery calcium (CAC) scoring via computed tomography (CT) being a key method for prevention. Traditional methods, primarily based on UNET architectures implemented on pre-built models, face challenges like the scarcity of annotated CT scans containing CA...

---

## 69. Cashmere and wool target detection algorithm based on Dino DETR and SVM

**Authors:** Huo Zhengtong Huo, Li Ziyin Li, He Jianjun He, Lu Yang Lu, Wei Shijie Wei

**Year:** 2024 | **Venue:** International Conference on Intelligent Human-Machine Systems and Cybernetics | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.1109/IHMSC62065.2024.00029)

> Wool and cashmere, as one of the important raw materials for clothing production, occupy an important position in industrial production. With the development of computer technology, the mainstream methods for detecting wool and cashmere fibers nowadays are one based on feature fusion set machine learning recognition method, and the other based on deep learning method. Although some algorithms can ...

---

## 70. Breaking Barriers with Enhanced DINO Framework and Score Normalization to Self-Supervised Speaker Verification

**Authors:** Xianmei Wan, Xiaosi Zhan, Na Li, Guihua Liao

**Year:** 2024 | **Venue:** International Conference on Digital Signal Processing | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.1145/3653876.3653878)

> Training robust speaker verification systems without speaker labels has long posed a significant challenge. Self-supervised learning (SSL) has garnered increased attention in the field of speech processing. However, prior researches have revealed a notable performance gap between self-supervised and fully supervised methods. This paper introduces an adaptive score normalization approach for self-s...

---

## 71. Change Dino: A Unified Transformer-Based Framework For Object-Level Change Detection and Segmentation in Remote Sensing Imagery

**Authors:** Yixin Chen, Ruiqian Zhang, Xiaogang Ning, Hanchao Zhang, Youxi He

**Year:** 2024 | **Venue:** IEEE International Geoscience and Remote Sensing Symposium | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.1109/IGARSS53475.2024.10642342)

> In the realm of remote sensing change detection, deep learning-based pixel-level methods have shown commendable accuracy and speed. However, due to the difficulty in distinguishing between each changed object and the high matching accuracy required, there are still limitations in practical applications. To address these issues, we propose Change DINO, a novel unified object-level change detection ...

---

## 72. EA-DINO: Improved method for unmanned aerial vehicle detection in airspace based on DINO

**Authors:** Hao Cai, Jinhong Zhang, Jianlong Xu

**Year:** 2024 | **Venue:** Engineering Research Express | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.1088/2631-8695/ad5f77)

> In recent years, the increase in drone traffic and the potential for unauthorized surveillance has underscored the urgent need for technological advances in drone detection. Despite the rapid advancements in deep learning that have significantly improved object detection tasks, air-to-air unmanned aerial vehicle (UAV) detection continues to pose significant challenges. Challenges such as complex b...

---

## 73. Generalized DINO: DINO via Multimodal Models for Generalized Object Detection

**Authors:** Kashing Yuen, Jianpeng Zou, Kaoru Uchida

**Year:** 2024 | **Venue:** Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.1145/3672758.3672887)

> Referring Expression Comprehension (REC) is a task in the realm of vision and language, aiming to identify objects in images based on provided descriptions. Classic REC methods, however, face challenges in handling expressions involving multiple targets or empty scenarios. In this paper, we study the limitations of existing REC methods, particularly in the context of Generalized Referring Expressi...

---

## 74. Interpretation of the category of the fantastic in the short stories of Dino Buzzati

**Authors:** A. N. Ushakova

**Year:** 2024 | **Venue:** Philology. Issues of Theory and Practice | **Citations:** 1 | **Score:** 0.000

[PDF](https://doi.org/10.30853/phil20240226) | [DOI](https://doi.org/10.30853/phil20240226)

> The purpose of the study is to explain the specifics of the category of the fantastic in the short stories of the 20th-century Italian writer Dino Buzzati. The article examines the features of the interpretation of the fantastic by a writer known for his special approach to the philosophical and aesthetic category that has remained relevant for several centuries. The choice of different collection...

---

## 75. Research on Marker Recognition Method for Substation Engineering Progress Monitoring Based on Grounding DINO

**Authors:** Li Ma, Ming Zhou, Qiang Wu, Tongyan Zhang, Hong Zhang

**Year:** 2024 | **Venue:** IEEE International Conference on Power and Renewable Energy | **Citations:** 1 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICPRE62586.2024.10768306)

> The traditional progress monitoring method for substation engineering mainly relies on manual work, which is time-consuming and laborious. Based on this, this paper proposes a new marker recognition method for substation engineering progress monitoring based on Grounding DINO. This method uses Grounding DINO to recognize the key markers of substation engineering and combines the weights of the mar...

---

## 76. DINO-MC: Self-supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops

**Authors:** Xinye Wanyan, Sachith Seneviratne, Shuchang Shen, Michael Kirley

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 24 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2303.06670) | [DOI](https://doi.org/10.48550/arXiv.2303.06670)

> ...

---

## 77. Finding Dino: A Plug-and-Play Framework for Zero-Shot Detection of Out-of-Distribution Objects Using Prototypes

**Authors:** Poulami Sinhamahapatra, Franziska Schwaiger, Shirsha Bose, Huiyu Wang, Karsten Roscher

**Year:** 2024 | **Venue:** IEEE Workshop/Winter Conference on Applications of Computer Vision | **Citations:** 5 | **Score:** 0.000

[DOI](https://doi.org/10.1109/WACV61041.2025.00821)

> Detecting and localising unknown or out-of-distribution (OOD) objects in any scene can be a challenging task in vision, particularly in safety-critical cases involving autonomous systems like automated vehicles or trains. Supervised anomaly segmentation or open-world object detection models depend on training on exhaustively annotated datasets for every domain and still struggle in distinguishing ...

---

## 78. Swiss DINO: Efficient and Versatile Vision Framework for On-device Personal Object Search

**Authors:** Kirill Paramonov, Jia-Xing Zhong, Umberto Michieli, J. Moon, Mete Ozay

**Year:** 2024 | **Venue:** IEEE/RJS International Conference on Intelligent RObots and Systems | **Citations:** 5 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2407.07541) | [DOI](https://doi.org/10.1109/IROS58592.2024.10802332)

> In this paper, we address a recent trend in robotic home appliances to include vision systems on personal devices, capable of personalizing the appliances on the fly. In particular, we formulate and address an important technical task of personal object search, which involves localization and identification of personal items of interest on images captured by robotic appliances, with each item refe...

---

## 79. Finding Dino: A plug-and-play framework for unsupervised detection of out-of-distribution objects using prototypes

**Authors:** Poulami Sinhamahapatra, Franziska Schwaiger, Shirsha Bose, Huiyu Wang, Karsten Roscher

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 4 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2404.07664)

> ...

---

## 80. Multi-task Image Restoration Guided By Robust DINO Features

**Authors:** Xin Lin, Jingtong Yue, Kelvin C. K. Chan, Lu Qi, Chao Ren

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 13 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2312.01677)

> Multi-task image restoration has gained significant interest due to its inherent versatility and efficiency compared to its single-task counterpart. However, performance decline is observed with an increase in the number of tasks, primarily attributed to the restoration model's challenge in handling different tasks with distinct natures at the same time. Thus, a perspective emerged aiming to explo...

---

## 81. IHRRB-DINO: Identifying High-Risk Regions of Breast Masses in Mammogram Images Using Data-Driven Instance Noise (DINO)

**Authors:** M. Kasem, Abdelrahman Abdallah, I. Abdelhalim, N. Alghamdi, S. Contractor

**Year:** 2024 | **Venue:** International Conference on Medical Image Computing and Computer-Assisted Intervention | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.1007/978-3-031-72378-0_11)

> ...

---

## 82. Re-Identification of Individual Kākā: An Explainable DINO-Based Model

**Authors:** Paula Maddigan, Oskar Ehrhardt, Andrew Lensen, Rachael C. Shaw

**Year:** 2024 | **Venue:** Image and Vision Computing New Zealand | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.1109/IVCNZ64857.2024.10794473)

> Recent advancements in vision transformers and self-supervised learning are expanding the capabilities of computer vision models. This study explores the application of a DINOv2-based unsupervised approach for the re-identification of kākā, a forest parrot endemic to New Zealand. We measure the performance of our vision transformer against a canonical SIFT-based method to establish its utility in ...

---

## 83. DINO-rPPG: Remote Photoplethysmography Measurement using Facial Representation from DINO Guidance

**Authors:** Jiho Choi, Sang Jun Lee

**Year:** 2024 | **Venue:** RePSS@IJCAI | **Citations:** 2 | **Score:** 0.000

> ...

---

## 84. VideoGrounding-DINO: Towards Open-Vocabulary Spatio- Temporal Video Grounding

**Authors:** Syed Talal Wasim, Muzammal Naseer, Salman H. Khan, Ming-Hsuan Yang, F. Khan

**Year:** 2023 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 28 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2401.00901) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01789)

> Video grounding aims to localize a spatio-temporal section in a video corresponding to an input text query. This paper addresses a critical limitation in current video grounding methodologies by introducing an Open-Vocabulary Spatio- Temporal Video Grounding task. Unlike prevalent closed-set approaches that struggle with open-vocabulary scenarios due to limited training data and pre-defined vocabu...

---

## 85. From Saliency to DINO: Saliency-guided Vision Transformer for Few-shot Keypoint Detection

**Authors:** Changsheng Lu, Hao Zhu, Piotr Koniusz

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 14 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2304.03140) | [DOI](https://doi.org/10.48550/arXiv.2304.03140)

> Unlike current deep keypoint detectors that are trained to recognize limited number of body parts, few-shot keypoint detection (FSKD) attempts to localize any keypoints, including novel or base keypoints, depending on the reference samples. FSKD requires the semantically meaningful relations for keypoint similarity learning to overcome the ubiquitous noise and ambiguous local patterns. One rescue ...

---

## 86. Entrevista Nicolao Dino

**Authors:** Nicolao Dino

**Year:** 2024 | **Venue:** Revista CNJ | **Citations:** N/A | **Score:** 0.000

[PDF](https://doi.org/10.54829/revistacnj.v8i2.704) | [DOI](https://doi.org/10.54829/revistacnj.v8i2.704)

> ...

---

## 87. Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction

**Authors:** Izzeddin Teeti, Rongali Sai Bhargav, Vivek Singh, Andrew Bradley, Biplab Banerjee

**Year:** 2023 | **Venue:** 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW) | **Citations:** 4 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2308.04589) | [DOI](https://doi.org/10.1109/ICCVW60793.2023.00352)

> The emerging field of action prediction - the task of forecasting action in a video sequence - plays a vital role in various computer vision applications such as autonomous driving, activity analysis and human-computer interaction. Despite significant advancements, accurately predicting future actions remains a challenging problem due to high dimensionality, complex dynamics and uncertainties inhe...

---

## 88. RS DINO: A Novel Panoptic Segmentation Algorithm for High Resolution Remote Sensing Images

**Authors:** Zizhen Li, Guangjun He, Han Fu, Qianqian Chen, Boyi Shangguan

**Year:** 2023 | **Venue:** International Conference on Agro-Geoinformatics | **Citations:** 4 | **Score:** 0.000

[DOI](https://doi.org/10.1109/Agro-Geoinformatics59224.2023.10233326)

> To address the suboptimal performance of current panoptic segmentation models in remote sensing image processing, this paper presents RS DINO(Remote Sensing Mask DETR with Improved Denoising Anchor Boxes), a novel algorithm for panoptic segmentation of multi-spectral high-resolution remote sensing images. First, a batch attention module was designed to calculate relationships between image patches...

---

## 89. FS-DINO: Improved Detection Method for Full-Scale Objects Based on DINO From High-Resolution Remote Sensing Imagery

**Authors:** Chang Guo, Bin Zhang

**Year:** 2023 | **Venue:** IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing | **Citations:** 4 | **Score:** 0.000

[PDF](https://ieeexplore.ieee.org/ielx7/4609443/9973430/10295978.pdf) | [DOI](https://doi.org/10.1109/JSTARS.2023.3327331)

> Object detection from high-resolution remote sensing (RS) imagery plays an important role in a wide range of applications, including traffic management, urban planning, and automatic surveying and mapping. Meanwhile, the end-to-end model for object detection is becoming a hot topic in computer vision area. However, owing to the complex background in the RS image, the results of object detection ba...

---

## 90. DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification

**Authors:** M. Shakouri, F. Iranmanesh, M. Eftekhari

**Year:** 2023 | **Venue:** International Symposium on Visual Computing | **Citations:** 6 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2308.00475) | [DOI](https://doi.org/10.48550/arXiv.2308.00475)

> The limited availability of labeled chest X-ray datasets is a significant bottleneck in the development of medical imaging methods. Self-supervised learning (SSL) can mitigate this problem by training models on unlabeled data. Furthermore, self-supervised pretraining has yielded promising results in visual recognition of natural images but has not been given much consideration in medical image ana...

---

## 91. DINO-VITS: Data-Efficient Zero-Shot TTS with Self-Supervised Speaker Verification Loss for Noise Robustness

**Authors:** Vikentii Pankov, Valeria Pronina, Alexander Kuzmin, Maksim Borisov, Nikita Usoltsev

**Year:** 2023 | **Venue:** Interspeech | **Citations:** 6 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2311.09770) | [DOI](https://doi.org/10.21437/interspeech.2024-549)

> We address zero-shot TTS systems' noise-robustness problem by proposing a dual-objective training for the speaker encoder using self-supervised DINO loss. This approach enhances the speaker encoder with the speech synthesis objective, capturing a wider range of speech characteristics beneficial for voice cloning. At the same time, the DINO objective improves speaker representation learning, ensuri...

---

## 92. Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery

**Authors:** J. A. Gallego-Mejia, Anna Jungbluth, Laura Mart'inez-Ferrer, Matt Allen, Francisco Dorr

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 4 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2310.03513) | [DOI](https://doi.org/10.48550/arXiv.2310.03513)

> Self-supervised learning (SSL) models have recently demonstrated remarkable performance across various tasks, including image segmentation. This study delves into the emergent characteristics of the Self-Distillation with No Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR) imagery. We pre-train a vision transformer (ViT)-based DINO model using unlabeled SAR data, and l...

---

## 93. Consistency and Ability of Students Using DINA and DINO Models

**Authors:** Mohammad Nasim Wafa, Z. Zia, Fazlahmad Frozan

**Year:** 2023 | **Venue:** European Journal of Mathematics and Statistics | **Citations:** 4 | **Score:** 0.000

[PDF](https://www.ej-math.org/index.php/ejmath/article/download/230/89) | [DOI](https://doi.org/10.24018/ejmath.2023.4.4.230)

> There are two prevalent models for studying cognitive diagnosis Models  for appraisement and diagnostics, each of these two models is the definite Input Noisy Output “and” gate  and the Deterministic Input Noisy Output “or” gate  models. The usage of them is to display various views of the ways cognitive skills are related and the probability of how an  responds correctly. The aim of  study is to ...

---

## 94. Review of Reinforcement Learning in Chrome Dino Game

**Authors:** Advait Kulkarni, Pranav Bapat, Tanvi Kulkarni, Rajendra Pawar

**Year:** 2023 | **Venue:** International Conference on Computing Communication Control and automation | **Citations:** 5 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICCUBEA58933.2023.10392095)

> The intention of this study is to study some implementation of reinforcement learning in the environment of Google Chrome's Dino Game. Different environments were tested on Deep Q network and proximal policy optimization models. Images were provided as inputs in all trials and the highest scores were recorded and analysed to check which algorithm and environment performed better. Further work aims...

---

## 95. Long non-coding RNA DINO promotes cisplatin sensitivity in lung adenocarcinoma via the p53-Bax axis

**Authors:** Zhi-le Liu, Qi Wang, Yue-Wei Bi, Alexey S. Chubarov, Ying Li

**Year:** 2023 | **Venue:** Journal of Thoracic Disease | **Citations:** 6 | **Score:** 0.000

[PDF](https://jtd.amegroups.org/article/viewFile/74764/pdf) | [DOI](https://doi.org/10.21037/jtd-23-465)

> Background The damage-induced non-coding (DINO) RNA is a newly identified long non-coding RNA (lncRNA) found in human cells with DNA damage. The treatment of tumors with cisplatin can induce DNA damage; however, whether the lncRNA DINO is involved in the treatment of non-small cell lung cancer (NSCLC) has not yet been elucidated. Methods The expression of the lncRNA DINO in lung adenocarcinoma cel...

---

## 96. Improving Dino-Based Self-Supervised Speaker Verification with Progressive Cluster-Aware Training

**Authors:** Bing Han, Wen-Chin Huang, Zhengyang Chen, Y. Qian

**Year:** 2023 | **Venue:** 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW) | **Citations:** 6 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICASSPW59220.2023.10192957)

> Self-supervised contrastive learning has recently emerged as one of the promising approaches in speaker verification task, due to its independence from labeled data. Among them, the DINO-based self-supervised framework, trained without exploiting negative pairs, is very popular and achieves excellent performance in the speaker verification task. However, limited by the duration of utterance, there...

---

## 97. High-Quality Object Detection Method for UAV Images Based on Improved DINO and Masked Image Modeling

**Authors:** Wanjie Lu, C. Niu, Chaozhen Lan, Wei Liu, Shiju Wang

**Year:** 2023 | **Venue:** Remote Sensing | **Citations:** 12 | **Score:** 0.000

[PDF](https://www.mdpi.com/2072-4292/15/19/4740/pdf?version=1695865992) | [DOI](https://doi.org/10.3390/rs15194740)

> The extensive application of unmanned aerial vehicle (UAV) technology has increased academic interest in object detection algorithms for UAV images. Nevertheless, these algorithms present issues such as low accuracy, inadequate stability, and insufficient pre-training model utilization. Therefore, a high-quality object detection method based on a performance-improved object detection baseline and ...

---

## 98. An Effective Representation Learning Approach: The Integrated Self-Supervised Pre-Training Models of StyleGAN2-ADA and DINO for Colon Polyp Images

**Authors:** Jong-Yeup Kim, Gayrat Tangriberganov, Woochul Jung, Dae Sung Kim, H. Koo

**Year:** 2023 | **Venue:** IEEE Access | **Citations:** 3 | **Score:** 0.000

[PDF](https://ieeexplore.ieee.org/ielx7/6287639/6514899/10359525.pdf) | [DOI](https://doi.org/10.1109/ACCESS.2023.3342838)

> In order to achieve optimal performance in the task of visual representation learning from image or video datasets, a significant quantity of annotated data is required. However, the process of collecting and annotating large-scale datasets is both costly and time-consuming, particularly in domains such as medicine where access to patient images is limited due to privacy concerns and the difficult...

---

## 99. Intelligent Recognition of Traffic Safety Facilities Using DINO Algorithm in Deep Learning

**Authors:** Lingxin Lu, Zhonglin Xiao, Lu Zhou

**Year:** 2023 | **Venue:** 2023 3rd International Conference on Electronic Information Engineering and Computer (EIECT) | **Citations:** 2 | **Score:** 0.000

[DOI](https://doi.org/10.1109/EIECT60552.2023.10442956)

> In an era of rapid technological advancement, the deep integration of computer technology with various fields, including traffic engineering, has become increasingly profound. Traditional methods of facility inspection and management suffer from inefficiencies and incomplete information, challenges that are effectively addressed by the intelligent and automated capabilities of computer technology....

---

## 100. Evaluating Large Vision-language Models for Surgical Tool Detection

**Authors:** Nakul Poudel, Richard Simon, Cristian A. Linte

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16895v1) | > Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the int...

---

## 101. RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture

**Authors:** Anas Anwarul Haq Khan, Mariam Husain, Kshitij Jadhav

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15891v1) | > Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Pre...

---

## 102. OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3

**Authors:** Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.13895v1) | > Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. ...

---

## 103. Revisiting Multi-Task Visual Representation Learning

**Authors:** Shangzhe Di, Zhonghua Zhai, Weidi Xie

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.13886v1) | > Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task...

---

## 104. DroneVLA: VLA based Aerial Manipulation

**Authors:** Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel Altamirano Cabrera

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.13809v2) | > As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system...

---

## 105. SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM

**Authors:** Xulei Shi, Maoyu Wang, Yuning Peng, Guanbo Wang, Xin Wang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11930v1) | > Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapp...

---

## 106. SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models

**Authors:** Turhan Can Kargin, Wojciech Jasiński, Adam Pardyl, Bartosz Zieliński, Marcin Przewięźlikowski

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11729v1) | > Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of ...

---

## 107. Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images

**Authors:** Pouya Afshin, David Helminiak, Tianling Niu, Julie M. Jorns, Tina Yen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.10917v1) | > Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided ...

---

## 108. Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure

**Authors:** Luxuan Fu, Chong Liu, Bisheng Yang, Zhen Dong

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.10551v1) | > Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unr...

---

## 109. Exploiting DINOv3-Based Self-Supervised Features for Robust Few-Shot Medical Image Segmentation

**Authors:** Guoping Xu, Jayaram K. Udupa, Weiguo Lu, You Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.08078v1) | > Deep learning-based automatic medical image segmentation plays a critical role in clinical diagnosis and treatment planning but remains challenging in few-shot scenarios due to the scarcity of annotated training data. Recently, self-supervised foundation models such as DINOv3, which were trained on large natural image datasets, have shown strong potential for dense feature extraction that can help...

---

## 110. Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training

**Authors:** Lingchen Sun, Rongyuan Wu, Zhengqiang Zhang, Ruibin Li, Yujing Sun

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.07773v1) | > Recent works such as REPA have shown that guiding diffusion models with external semantic features (e.g., DINO) can significantly accelerate the training of diffusion transformers (DiTs). However, this requires the use of pretrained external networks, introducing additional dependencies and reducing flexibility. In this work, we argue that DiTs actually have the power to guide the training of them...

---

## 111. VENUS: Visual Editing with Noise Inversion Using Scene Graphs

**Authors:** Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.07219v1) | > State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relation...

---

## 112. LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models

**Authors:** Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Yuhua Zhu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.06550v1) | > Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive...

---

## 113. Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery

**Authors:** Tom Burgert, Leonard Hackel, Paolo Rota, Begüm Demir

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.02289v1) | > Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive...

---

## 114. MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization

**Authors:** Zhexin Zhang, Yifeng Zhu, Yangyang Xu, Long Chen, Yong Du

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.01955v1) | > Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and se...

---

## 115. Noise-Robust Tiny Object Localization with Flows

**Authors:** Huixin Sun, Linlin Yang, Ronyu Chen, Kerui Gu, Baochang Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00617v1) | > Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framew...

---

## 116. What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?

**Authors:** Basile Terver, Tsung-Yen Yang, Jean Ponce, Adrien Bardes, Yann LeCun

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.24497v2) | > A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods ...

---

## 117. Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline for Foundation-Model Cancer Risk Prediction

**Authors:** Md. Enamul Hoq, Linda Larson-Prior, Fred Prior

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.24294v1) | > Robust preprocessing is rarely quantified in deep-learning pipelines for low-dose CT (LDCT) lung cancer screening. We develop and validate Virtual-Eyes, a clinically motivated 16-bit CT quality-control pipeline, and measure its differential impact on generalist foundation models versus specialist models. Virtual-Eyes enforces strict 512x512 in-plane resolution, rejects short or non-diagnostic seri...

---

## 118. ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation

**Authors:** Ziquan Liu, Zhewei Zhu, Xuyang Shi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.24224v1) | > Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP's internal features. These approaches are e...

---

## 119. Structural changes in the Lennard-Jones supercooled liquid and ideal glass: an improved integral equation for the replica method

**Authors:** Bomont Jean-Marc, Bretonnet Jean-Louis, Costa Dino, Pastore Giorgio

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.22548v1) | > Framing the glass formation within standard statistical mechanics is an outstanding problem of condensed matter theory. To provide new insight, we investigate the structural properties of the Lennard-Jones fluid in the very-low temperature regime, by using a replicated version of the refined HMSA theory of the liquid state, combined with an appropriate split of the pair potential [Bomont and Breto...

---

## 120. Break Out the Silverware -- Semantic Understanding of Stored Household Items

**Authors:** Michaela Levi-Richter, Reuth Mirsky, Oren Glickman

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.23739v1) | > ``Bring me a plate.'' For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating ser...

---

## 121. Deep learning-enhanced dual-mode multiplexed optical sensor for point-of-care diagnostics of cardiovascular diseases

**Authors:** Gyeo-Re Han, Merve Eryilmaz, Artem Goncharov, Yuzhu Li, Shun Ye

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.21389v1) | > Rapid and accessible cardiac biomarker testing is essential for the timely diagnosis and risk assessment of myocardial infarction (MI) and heart failure (HF), two interrelated conditions that frequently coexist and drive recurrent hospitalizations with high mortality. However, current laboratory and point-of-care testing systems are limited by long turnaround times, narrow dynamic ranges for the t...

---

## 122. Autonomous Uncertainty Quantification for Computational Point-of-care Sensors

**Authors:** Artem Goncharov, Rajesh Ghosh, Hyou-Arm Joung, Dino Di Carlo, Aydogan Ozcan

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.21335v1) | > Computational point-of-care (POC) sensors enable rapid, low-cost, and accessible diagnostics in emergency, remote and resource-limited areas that lack access to centralized medical facilities. These systems can utilize neural network-based algorithms to accurately infer a diagnosis from the signals generated by rapid diagnostic tests or sensors. However, neural network-based diagnostic models are ...

---

## 123. Mental Health Self-Disclosure on Social Media throughout the Pandemic Period

**Authors:** Dino Husnic, Stefan Cobeli, Shweta Yadav

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.20990v1) | > The COVID-19 pandemic has created many problems, especially in people's social lives. There has been increasing isolation and economic hardships since the beginning of the pandemic for people all over the world. Quarantines and lockdowns also took part in that, and so, people have been expressing their emotions throughout the pandemic period using social media platforms like Reddit, Twitter, Faceb...

---

## 124. Symmetric Superconducting Dome Accompanied by Non-Fermi Liquid Transport in Ionic Liquid Gated-MoS2

**Authors:** Qiao Chen, Changshuai Lan, Huiqin Jian, Yi Yan, Xinming Zhao

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.20207v1) | > In strongly correlated superconductors, the emergence of superconductivity is often accompanied by anomalous normal state. The connection between these two phenomena is considered crucial for understanding the underlying unconventional pairing mechanisms. In this study, we report analogous behavior in MoS2, a band insulator devoid of long-range magnetic order. Through ionic liquid gating, continuo...

---

## 125. DVGT: Driving Visual Geometry Transformer

**Authors:** Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16919v1) | > Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed mult...

---

## 126. CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies

**Authors:** John M. Statheros, Hairong Wang, Richard Klein

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16700v1) | > The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a ...

---

## 127. From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection

**Authors:** Manuel Nkegoum, Minh-Tan Pham, Élisa Fromont, Bruno Avignon, Sébastien Lefèvre

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.15971v1) | > Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable sour...

---

## 128. Multi-View Foundation Models

**Authors:** Leo Segre, Or Hirschorn, Shai Avidan

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.15708v1) | > Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Fou...

---

## 129. Route-DETR: Pairwise Query Routing in Transformers for Object Detection

**Authors:** Ye Zhang, Qi Chen, Wenyou Huang, Rui Liu, Zhengjian Kang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.13876v1) | > Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-att...

---

## 130. DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis

**Authors:** Felix J. Dorfner, Manon A. Dorster, Ryan Connolly, Oscar Gentilhomme, Edward Gibbs

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.13608v1) | > Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Sel...

---

## 131. Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"

**Authors:** Carla Monteiro, Valentina Corbetta, Regina Beets-Tan, Luís F. Teixeira, Wilson Silva

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.13376v2) | > Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific archit...

---

## 132. Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery

**Authors:** Zhimao Peng, Enguang Wang, Fei Yang, Xialei Liu, Ming-Ming Cheng

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.12925v1) | > Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as sup...

---

## 133. Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis

**Authors:** Valentina Lilova, Toyesh Chakravorty, Julian I. Bibo, Emma Boccaletti, Brandon Li

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.11574v2) | > Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream fine-tuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pre-trained encoders. In this work, we introduce a novel benchmark for in-context 3D sc...

---

## 134. SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection

**Authors:** Tianye Qi, Weihao Li, Nick Barnes

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.11215v1) | > Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based sm...

---

## 135. Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching

**Authors:** Javier Villena Toro, Mehdi Tarkian

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.10674v1) | > Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-devic...

---

## 136. Dichotomy results for classes of countable graphs

**Authors:** Vittorio Cipriani, Ekaterina Fokina, Matthew Harrison-Trainor, Liling Ko, Dino Rossegger

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.09832v1) | > We study classes of countable graphs where every member does not contain a given finite graph as an induced subgraph -- denoted by $\mathsf{Free}(\mathcal{G})$ for a given finite graph $\mathcal{G}$. Our main results establish a structural dichotomy for such classes: If $\mathcal{G}$ is not an induced subgraph of $\mathcal{P}_4$, then $\mathsf{Free}(\mathcal{G})$ is on top under effective bi-inter...

---

## 137. DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation

**Authors:** Jianwei Wang, Qing Wang, Menglan Ruan, Rongjun Ge, Chunfeng Yang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.08337v1) | > Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extra...

---

## 138. Relational Visual Similarity

**Authors:** Thao Nguyen, Sicheng Mo, Krishna Kumar Singh, Yilin Wang, Jing Shi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.07833v1) | > Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. ...

---

## 139. One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation

**Authors:** Yuan Gao, Chen Chen, Tianrong Chen, Jiatao Gu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.07829v2) | > Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fu...

---

## 140. The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers

**Authors:** Kanishk Awadhiya

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.07331v1) | > Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classificatio...

---

## 141. RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation

**Authors:** Nicolas Houdré, Diego Marcos, Hugo Riffaud de Turckheim, Dino Ienco, Laurent Wendling

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.05025v1) | > Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting gene...

---

## 142. Vision Foundry: A System for Training Foundational Vision AI Models

**Authors:** Mahmut S. Gokmen, Mitchell A. Klusty, Evan W. Damron, W. Vaiden Logan, Aaron D. Mullen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.11837v1) | > Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities w...

---

## 143. DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction

**Authors:** Kaichen Zhang, Tianxiang Sheng, Xuanming Shi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.03715v1) | > This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The
  method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and
  matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while
 ...

---

## 144. OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation

**Authors:** Zhishan Zhou, Siyuan Wei, Zengran Wang, Chunjie Wang, Xiaosheng Yan

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.03532v1) | > Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios an...

---

## 145. KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM

**Authors:** Zaid Nasser, Mikhail Iumanov, Tianhao Li, Maxim Popov, Jaafar Mahmoud

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.01889v1) | > We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO ...

---

## 146. Uniformity in learning structures

**Authors:** Vittorio Cipriani, Dino Rossegger

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.01867v1) | > The standard framework for studying learning problems on algebraic structures assumes that the structures in the target family are pairwise nonisomorphic. Under this assumption, the most widely investigated learning criterion--Ex-learning--becomes inherently equivalent to the well-known paradigm of Bc-learning. This paper explores what happens when the nonisomorphism requirement is removed and ana...

---

## 147. DINO-Tok: Adapting DINO for Visual Tokenizers

**Authors:** Mingkai Jia, Mingxiao Li, Liaoyuan Fan, Tianxing Shi, Jiaxin Guo

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.20565v1) | > Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a...

---

## 148. The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation

**Authors:** Weijia Mao, Hao Chen, Zhenheng Yang, Mike Zheng Shou

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.20256v1) | > A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we int...

---

## 149. Mechanical Design of the PIP-II ORBUMP Pulsed Dipole Magnet

**Authors:** D. Karas, K. Badgley, Z. Chen, V. Chernenok, M. Davidson

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.19658v1) | > The Proton Improvement Plan II (PIP-II) project is a vital upgrade to the Fermilab accelerator complex. The magnet pulse rate of the PIP-II Injection system requires an increase from the current rate of 15 Hz to 20 Hz as well as a roughly 30% increase in the magnetic field of the new Orbital Bump (ORBUMP) pulsed dipole magnets in the Booster. The ORBUMP magnet mechanical design is presented in thi...

---

## 150. Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens

**Authors:** Yiming Qin, Bomin Wei, Jiaxin Ge, Konstantinos Kallidromitis, Stephanie Fu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.19418v2) | > Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework th...

---

## 151. RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models

**Authors:** Timing Yang, Guoyizhe Wei, Alan Yuille, Feng Wang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.18380v1) | > Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as ...

---

## 152. Muskie: Multi-view Masked Image Modeling for 3D Vision Pre-training

**Authors:** Wenyu Li, Sidun Liu, Peng Qiao, Yong Dou, Tongrui Hu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.18115v1) | > We present Muskie, a native multi-view vision backbone designed for 3D vision tasks. Unlike existing models, which are frame-wise and exhibit limited multi-view consistency, Muskie is designed to process multiple views simultaneously and introduce multi-view consistency in pre-training stage. Muskie is trained to reconstruct heavily masked content in one view by finding and utilizing geometric cor...

---

## 153. Not Quite Anything: Overcoming SAMs Limitations for 3D Medical Imaging

**Authors:** Keith Moore

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.19471v1) | > Foundation segmentation models such as SAM and SAM-2 perform well on natural images but struggle with brain MRIs where structures like the caudate and thalamus lack sharp boundaries and have low contrast. Rather than fine tune these models (for example MedSAM), we propose a compositional alternative where the foundation model output is treated as an additional input channel and passed alongside th...

---

## 154. Loomis Painter: Reconstructing the Painting Process

**Authors:** Markus Pobitzer, Chang Liu, Chenyi Zhuang, Teng Long, Bin Ren

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.17344v2) | > Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address ...

---

## 155. Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers

**Authors:** Cris Claessens, Christiaan Viviers, Giacomo D'Amicantonio, Egor Bondarev, Fons van der Sommen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.17209v1) | > We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique ...

---

## 156. DReX: Pure Vision Fusion of Self-Supervised and Convolutional Representations for Image Complexity Prediction

**Authors:** Jonathan Skaza, Parsa Madinei, Ziqi Wen, Miguel Eckstein

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.16991v1) | > Visual complexity prediction is a fundamental problem in computer vision with applications in image compression, retrieval, and classification. Understanding what makes humans perceive an image as complex is also a long-standing question in cognitive science. Recent approaches have leveraged multimodal models that combine visual and linguistic representations, but it remains unclear whether langua...

---

## 157. Dataset Distillation for Pre-Trained Self-Supervised Vision Models

**Authors:** George Cazenavette, Antonio Torralba, Vincent Sitzmann

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.16674v1) | > The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on larg...

---

## 158. GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI

**Authors:** Naomi Simumba, Nils Lehmann, Paolo Fraccaro, Hamed Alemohammad, Geeth De Mel

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.15658v1) | > Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common c...

---

## 159. What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs

**Authors:** Zhihan Ren, Lijun He, Jiaxi Liang, Xinzhu Fu, Haixia Bi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.15316v1) | > Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA). Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage. To reveal the ...

---

## 160. MeanFlow Transformers with Representation Autoencoders

**Authors:** Zheyuan Hu, Chieh-Hsin Lai, Ge Wu, Yuki Mitsufuji, Stefano Ermon

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.13019v1) | > MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inf...

---

## 161. DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection

**Authors:** Jialiang Shen, Jiyang Zheng, Yunqi Xue, Huajie Chen, Yu Yao

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.12511v2) | > With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causin...

---

## 162. Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection

**Authors:** Tianxiang Zhang, Peipeng Yu, Zhihua Xia, Longchen Dai, Xiaoyu Zhou

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.12107v1) | > The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates light...

---

## 163. NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion

**Authors:** Chuheng Chen, Xiaofei Zhou, Geyuan Zhang, Yong Huang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.11051v1) | > Low-Rank Adaptation (LoRA) fusion has emerged as a key technique for reusing and composing learned subject and style representations for controllable generation without costly retraining. However, existing methods rely on weight-based merging, where one LoRA often dominates the other, leading to interference and degraded fidelity. This interference is structural: separately trained LoRAs occupy lo...

---

## 164. PhaseWin Search Framework Enable Efficient Object-Level Interpretation

**Authors:** Zihan Gu, Ruoyu Chen, Junchi Zhang, Yue Hu, Hua Zhang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.10914v1) | > Attribution is essential for interpreting object-level foundation models. Recent methods based on submodular subset selection have achieved high faithfulness, but their efficiency limitations hinder practical deployment in real-world scenarios. To address this, we propose PhaseWin, a novel phase-window search algorithm that enables faithful region attribution with near-linear complexity. PhaseWin ...

---

## 165. Depth Anything 3: Recovering the Visual Space from Any Views

**Authors:** Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.10647v1) | > We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates...

---

## 166. Feature Quality and Adaptability of Medical Foundation Models: A Comparative Evaluation for Radiographic Classification and Segmentation

**Authors:** Frank Li, Theo Dapamede, Mohammadreza Chavoshi, Young Seok Jeon, Bardia Khosravi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.09742v1) | > Foundation models (FMs) promise to generalize medical imaging, but their effectiveness varies. It remains unclear how pre-training domain (medical vs. general), paradigm (e.g., text-guided), and architecture influence embedding quality, hindering the selection of optimal encoders for specific radiology tasks. To address this, we evaluate vision encoders from eight medical and general-domain FMs fo...

---

## 167. Empowering DINO Representations for Underwater Instance Segmentation via Aligner and Prompter

**Authors:** Zhiyang Chen, Chen Zhang, Hao Fang, Runmin Cong

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.08334v1) | > Underwater instance segmentation (UIS), integrating pixel-level understanding and instance-level discrimination, is a pivotal technology in marine resource exploration and ecological protection. In recent years, large-scale pretrained visual foundation models, exemplified by DINO, have advanced rapidly and demonstrated remarkable performance on complex downstream tasks. In this paper, we demonstra...

---

## 168. High-Quality Proposal Encoding and Cascade Denoising for Imaginary Supervised Object Detection

**Authors:** Zhiyuan Chen, Yuelin Guo, Zitong Huang, Haoyu He, Renhao Lu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.08018v1) | > Object detection models demand large-scale annotated datasets, which are costly and labor-intensive to create. This motivated Imaginary Supervised Object Detection (ISOD), where models train on synthetic images and test on real images. However, existing methods face three limitations: (1) synthetic datasets suffer from simplistic prompts, poor image quality, and weak supervision; (2) DETR-based de...

---

## 169. Scene-Aware Urban Design: A Human-AI Recommendation Framework Using Co-Occurrence Embeddings and Vision-Language Models

**Authors:** Rodrigo Gallardo, Oz Fishman, Alexander Htet Kyaw

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.06201v1) | > This paper introduces a human-in-the-loop computer vision framework that uses generative AI to propose micro-scale design interventions in public space and support more continuous, local participation. Using Grounding DINO and a curated subset of the ADE20K dataset as a proxy for the urban built environment, the system detects urban objects and builds co-occurrence embeddings that reveal common sp...

---

## 170. Zero-Shot Multi-Animal Tracking in the Wild

**Authors:** Jan Frederik Meier, Timo Lüddecke

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.02591v1) | > Multi-animal tracking is crucial for understanding animal ecology and behavior. However, it remains a challenging task due to variations in habitat, motion patterns, and species appearance. Traditional approaches typically require extensive model fine-tuning and heuristic design for each application scenario. In this work, we explore the potential of recent vision foundation models for zero-shot m...

---

## 171. Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs

**Authors:** Arya Shah, Vaibhav Tripathi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.02404v1) | > Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic cats) have vertically elongated pupils linked to ambush predation; yet, how such specializations manifest in downstream visual representations remains incompletely understood. We present a unified, frozen-encoder benchmark that quantifies feline-human cross-species representational alignment in the wild, across convoluti...

---

## 172. Extremal Contours: Gradient-driven contours for compact visual attribution

**Authors:** Reza Karimzadeh, Albert Alonso, Frans Zdyb, Julius B. Kirkegaard, Bulat Ibragimov

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2511.01411v1) | > Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve...

---

## 173. DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications

**Authors:** Malaisree P, Youwai S, Kitkobsin T, Janrungautai S, Amorndechaphon D

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.25140v2) | > Object detection in civil engineering applications is constrained by limited annotated data in specialized domains. We introduce DINO-YOLO, a hybrid architecture combining YOLOv12 with DINOv3 self-supervised vision transformers for data-efficient detection. DINOv3 features are strategically integrated at two locations: input preprocessing (P0) and mid-backbone enhancement (P3). Experimental valida...

---

## 174. Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8

**Authors:** Zahra Ebrahimi Vargoorani, Amir Mohammad Ghoreyshi, Ching Yee Suen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.25032v1) | [DOI](https://doi.org/10.1109/MLSP62443.2025.11204315)

> Developing a highly accurate automatic license plate recognition system (ALPR) is challenging due to environmental factors such as lighting, rain, and dust. Additional difficulties include high vehicle speeds, varying camera angles, and low-quality or low-resolution images. ALPR is vital in traffic control, parking, vehicle tracking, toll collection, and law enforcement applications. This paper pr...

---

## 175. Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?

**Authors:** Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.24709v2) | > Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-...

---

## 176. Towards Generalisable Foundation Models for Brain MRI

**Authors:** Moona Mazher, Geoff J. M. Parker, Daniel C. Alexander

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.23415v3) | > Foundation models in artificial intelligence (AI) are transforming medical imaging by enabling general-purpose feature learning from large-scale, unlabeled datasets. In this work, we introduce BrainFound, a self-supervised foundation model for brain MRI, built by extending DINO-v2, a vision transformer originally designed for 2D natural images. BrainFound adapts DINO-v2 to model full 3D brain anat...

---

## 177. LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering

**Authors:** Wenkai Zhu, Xu Li, Qimin Xu, Benwu Wang, Kun Wei

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.22669v1) | > 3D Gaussian Splatting SLAM has emerged as a widely used technique for high-fidelity mapping in spatial intelligence. However, existing methods often rely on a single representation scheme, which limits their performance in large-scale dynamic outdoor scenes and leads to cumulative pose errors and scale ambiguity. To address these challenges, we propose \textbf{LVD-GS}, a novel LiDAR-Visual 3D Gaus...

---

## 178. S3OD: Towards Generalizable Salient Object Detection with Synthetic Data

**Authors:** Orest Kupyn, Hirokatsu Kataoka, Christian Rupprecht

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.21605v1) | > Salient object detection exemplifies data-bounded tasks where expensive pixel-precise annotations force separate model training for related subtasks like DIS and HR-SOD. We present a method that dramatically improves generalization through large-scale synthetic data generation and ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000 high-resolution images created through our ...

---

## 179. DiNo and RanBu: Lightweight Predictions from Shallow Random Forests

**Authors:** Tiago Mendonça dos Santos, Rafael Izbicki, Luís Gustavo Esteves

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.23624v1) | > Random Forest ensembles are a strong baseline for tabular prediction tasks, but their reliance on hundreds of deep trees often results in high inference latency and memory demands, limiting deployment in latency-sensitive or resource-constrained environments. We introduce DiNo (Distance with Nodes) and RanBu (Random Bushes), two shallow-forest methods that convert a small set of depth-limited tree...

---

## 180. Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking

**Authors:** Zixuan Wu, Hengyuan Zhang, Ting-Hsuan Chen, Yuliang Guo, David Paz

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.20335v1) | > Parking is a critical pillar of driving safety. While recent end-to-end (E2E) approaches have achieved promising in-domain results, robustness under domain shifts (e.g., weather and lighting changes) remains a key challenge. Rather than relying on additional data, in this paper, we propose Dino-Diffusion Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates visual foundation...

---

## 181. Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration

**Authors:** Francisco Mena, Dino Ienco, Cassio F. Dantas, Roberto Interdonato, Andreas Dengel

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.19579v1) | [DOI](https://doi.org/10.1007/s10994-025-06903-0)

> Multi-modal co-learning is emerging as an effective paradigm in machine learning, enabling models to collaboratively learn from different modalities to enhance single-modality predictions. Earth Observation (EO) represents a quintessential domain for multi-modal data analysis, wherein diverse remote sensors collect data to sense our planet. This unprecedented volume of data introduces novel challe...

---

## 182. Elastic ViTs from Pretrained Models without Retraining

**Authors:** Walter Simoncini, Michael Dorkenwald, Tijmen Blankevoort, Cees G. M. Snoek, Yuki M. Asano

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.17700v1) | > Vision foundation models achieve remarkable performance but are only available in a limited set of pre-determined sizes, forcing sub-optimal deployment choices under real-world constraints. We introduce SnapViT: Single-shot network approximation for pruned Vision Transformers, a new post-pretraining structured pruning method that enables elastic inference across a continuum of compute budgets. Our...

---

## 183. Mapping Hidden Heritage: Self-supervised Pre-training on High-Resolution LiDAR DEM Derivatives for Archaeological Stone Wall Detection

**Authors:** Zexian Huang, Mashnoon Islam, Brian Armstrong, Billy Bell, Kourosh Khoshelham

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.17644v3) | > Historic dry-stone walls hold significant cultural and environmental importance, serving as historical markers and contributing to ecosystem preservation and wildfire management during dry seasons in Australia. However, many of these stone structures in remote or vegetated landscapes remain undocumented due to limited accessibility and the high cost of manual mapping. Deep learning-based segmentat...

---

## 184. DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation

**Authors:** Pedram Fekri, Majid Roshanfar, Samuel Barbeau, Seyedfarzad Famouri, Thomas Looi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.17038v1) | > Cardiac catheterization remains a cornerstone of minimally invasive interventions, yet it continues to rely heavily on manual operation. Despite advances in robotic platforms, existing systems are predominantly follow-leader in nature, requiring continuous physician input and lacking intelligent autonomy. This dependency contributes to operator fatigue, more radiation exposure, and variability in ...

---

## 185. Latent Diffusion Model without Variational Autoencoder

**Authors:** Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.15301v3) | > Recent progress in diffusion-based visual generation has largely relied on latent diffusion models with variational autoencoders (VAEs). While effective for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited training efficiency, slow inference, and poor transferability to broader vision tasks. These issues stem from a key limitation of VAE latent spaces: the lack of clear se...

---

## 186. GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering

**Authors:** Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.14270v3) | > Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitation...

---

## 187. Detect Anything via Next Point Prediction

**Authors:** Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.12798v1) | > Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a 3B-scale MLLM that achieves state-of-...

---

## 188. AnyUp: Universal Feature Upsampling

**Authors:** Thomas Wimmer, Prune Truong, Marie-Julie Rakotosaona, Michael Oechsle, Federico Tombari

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.12764v1) | > We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic ups...

---

## 189. Evaluating the Explainability of Vision Transformers in Medical Imaging

**Authors:** Leili Barekatain, Ben Glocker

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.12021v1) | > Understanding model decisions is crucial in medical imaging, where interpretability directly impacts clinical trust and adoption. Vision Transformers (ViTs) have demonstrated state-of-the-art performance in diagnostic imaging; however, their complex attention mechanisms pose challenges to explainability. This study evaluates the explainability of different Vision Transformer architectures and pre-...

---

## 190. Diffusion Transformers with Representation Autoencoders

**Authors:** Boyang Zheng, Nanye Ma, Shengbang Tong, Saining Xie

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.11690v1) | > Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-d...

---

## 191. Deployment and Development of a Cognitive Teleoreactive Framework for Deep Sea Autonomy

**Authors:** Christopher Thierauf

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.10716v1) | > A new AUV mission planning and execution software has been tested on AUV Sentry. Dubbed DINOS-R, it draws inspiration from cognitive architectures and AUV control systems to replace the legacy MC architecture. Unlike these existing architectures, however, DINOS-R is built from the ground-up to unify symbolic decision making (for understandable, repeatable, provable behavior) with machine learning ...

---

## 192. Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation

**Authors:** Wenyao Zhang, Hongsi Liu, Bohan Li, Jiawei He, Zekun Qi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.09320v1) | > Current self-supervised monocular depth estimation (MDE) approaches encounter performance limitations due to insufficient semantic-spatial knowledge extraction. To address this challenge, we propose Hybrid-depth, a novel framework that systematically integrates foundation models (e.g., CLIP and DINO) to extract visual priors and acquire sufficient contextual information for MDE. Our approach intro...

---

## 193. Re-Identifying Kākā with AI-Automated Video Key Frame Extraction

**Authors:** Paula Maddigan, Andrew Lensen, Rachael C. Shaw

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.08775v1) | > Accurate recognition and re-identification of individual animals is essential for successful wildlife population monitoring. Traditional methods, such as leg banding of birds, are time consuming and invasive. Recent progress in artificial intelligence, particularly computer vision, offers encouraging solutions for smart conservation and efficient automation. This study presents a unique pipeline f...

---

## 194. Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry

**Authors:** Thomas Fel, Binxu Wang, Michael A. Lepori, Matthew Kowal, Andrew Lee

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.08638v1) | > DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet the nature of what it perceives remains unknown. As a working baseline, we adopt the Linear Representation Hypothesis (LRH) and operationalize it using SAEs, producing a 32,000-unit dictionary that serves as the interpretability backbone of our study, which unfolds in three parts.
  In the first part, we analyze how differ...

---

## 195. LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval

**Authors:** Avishree Khare, Hideki Okamoto, Bardh Hoxha, Georgios Fainekos, Rajeev Alur

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.06512v1) | > Neural models such as YOLO and HuBERT can be used to detect local properties such as objects ("car") and emotions ("angry") in individual frames of videos and audio clips respectively. The likelihood of these detections is indicated by scores in [0, 1]. Lifting these scores to temporal properties over sequences can be useful for several downstream applications such as query matching (e.g., "does t...

---

## 196. A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation

**Authors:** Alon Kaya, Igal Bilik, Inna Stainvas

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.04794v1) | > Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs) have reshaped computer vision through pretrained feature representations that enable strong transfer learning for diverse tasks. However, their efficiency as backbone architectures for geometric estimation tasks involving image deformations in low-data regimes remains an open question. This work considers two such tasks:...

---

## 197. Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops

**Authors:** Mattia Scardecchia

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.03606v1) | > Recent advances in self-supervised learning (SSL) have made it possible to learn general-purpose visual features that capture both the high-level semantics and the fine-grained spatial structure of images. Most notably, the recent DINOv2 has established a new state of the art by surpassing weakly supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we examine the core ideas b...

---

## 198. Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields

**Authors:** Zhiting Mei, Ola Shorinwa, Anirudha Majumdar

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.03104v1) | > Semantic distillation in radiance fields has spurred significant advances in open-vocabulary robot policies, e.g., in manipulation and navigation, founded on pretrained semantics from large vision models. While prior work has demonstrated the effectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit of geometry-grou...

---

## 199. One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework

**Authors:** Lorenzo Bianchi, Giacomo Pacini, Fabio Carrara, Nicola Messina, Giuseppe Amato

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.02898v2) | > Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present Patch-ioner, a unified framework for zero-shot captioni...

---

## 200. Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models

**Authors:** Lihua Zhou, Mao Ye, Shuaifeng Li, Nianxin Li, Jinlin Wu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.02750v1) | > Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved remarkable success in object recognition and detection. However, their performance often degrades under real-world distribution shifts. Test-time adaptation (TTA) aims to mitigate this issue by adapting models during inference. Existing methods either rely on computationally expensive backpropagation, which hinders real-ti...

---

## 201. A computational framework for quantifying route diversification in road networks

**Authors:** Giuliano Cornacchia, Luca Pappalardo, Mirco Nanni, Dino Pedreschi, Marta C. González

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.02582v1) | > The structure of road networks impacts various urban dynamics, from traffic congestion to environmental sustainability and access to essential services. Recent studies reveal that most roads are underutilized, faster alternative routes are often overlooked, and traffic is typically concentrated on a few corridors. In this article, we examine how road network structure, and in particular the presen...

---

## 202. Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring

**Authors:** Han-Jay Shu, Wei-Ning Chiu, Shun-Ting Chang, Meng-Ping Huang, Takeshi Tohyama

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.01683v1) | > Deep learning models achieve strong performance in chest radiograph (CXR) interpretation, yet fairness and reliability concerns persist. Models often show uneven accuracy across patient subgroups, leading to hidden failures not reflected in aggregate metrics. Existing error detection approaches -- based on confidence calibration or out-of-distribution (OOD) detection -- struggle with subtle within...

---

## 203. Excitons and Optical Response in Excitonic Insulator Candidate TiSe$_2$

**Authors:** Dino Novko

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.00556v1) | > The origin of the charge density wave (CDW) phase in TiSe$_2$ is a highly debated topic, with lattice and excitonic correlations proposed as the main driving mechanisms. One of the proposed scenarios is the excitonic insulator (EI) mechanism, where soft electronic mode drives the phase transition. However, the existence of this purely electronic mode is controversial. Here, we perform fully ab-ini...

---

## 204. VLOD-TTA: Test-Time Adaptation of Vision-Language Object Detectors

**Authors:** Atif Belal, Heitor R. Medeiros, Marco Pedersoli, Eric Granger

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2510.00458v1) | > Vision-language object detectors (VLODs) such as YOLO-World and Grounding DINO achieve impressive zero-shot recognition by aligning region proposals with text representations. However, their performance often degrades under domain shift. We introduce VLOD-TTA, a test-time adaptation (TTA) framework for VLODs that leverages dense proposal overlap and image-conditioned prompt scores. First, an IoU-w...

---

## 205. An Efficient Shift-and-Stack Algorithm Applied to Detection Catalogs

**Authors:** Steven Stetzler, Mario Jurić, Pedro H. Bernardinelli, Dino Bektešević, Colin Orion Chandler

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.26279v1) | [DOI](https://doi.org/10.3847/1538-3881/ae0e1a)

> The boundary of solar system object discovery lies in detecting its faintest members. However, their discovery in detection catalogs from imaging surveys is fundamentally limited by the practice of thresholding detections at signal-to-noise (SNR) $\geq 5$ to maintain catalog purity. Faint moving objects can be recovered from survey images using the shift-and-stack algorithm, which coadds pixels fr...

---

## 206. CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image Segmentation

**Authors:** Max Curie, Paulo da Costa

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.25016v2) | > We introduce CLASP (Clustering via Adaptive Spectral Processing), a lightweight framework for unsupervised image segmentation that operates without any labeled data or finetuning. CLASP first extracts per patch features using a self supervised ViT encoder (DINO); then, it builds an affinity matrix and applies spectral clustering. To avoid manual tuning, we select the segment count automatically wi...

---

## 207. Fire severity and recovery across Europe: insights from forest diversity and landscape metrics

**Authors:** Eatidal Amin, Cassio F. Dantas, Dino Ienco, Samuel Alleaume, Sandra Luque

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.24953v1) | > In recent decades, European forests have faced an increased incidence of fire disturbances. This phenomenon is likely to persist, given the rising frequency of extreme events expected in the future. Estimating canopy recovery time after disturbance serves as a critical assessment for understanding forest resilience, which can ultimately help determine the ability of forests to regain their capacit...

---

## 208. Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation

**Authors:** Lei Tong, Zhihua Liu, Chaochao Lu, Dino Oglic, Tom Diethe

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.24798v3) | > We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal stru...

---

## 209. ActiveCQ: Active Estimation of Causal Quantities

**Authors:** Erdun Gao, Dino Sejdinovic

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.24293v1) | > Estimating causal quantities (CQs) typically requires large datasets, which can be expensive to obtain, especially when measuring individual outcomes is costly. This challenge highlights the importance of sample-efficient active learning strategies. To address the narrow focus of prior work on the conditional average treatment effect, we formalize the broader task of Actively estimating Causal Qua...

---

## 210. Visual CoT Makes VLMs Smarter but More Fragile

**Authors:** Chunxue Xu, Yiwei Wang, Yujun Cai, Bryan Hooi, Songze Li

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.23789v1) | > Chain-of-Thought (CoT) techniques have significantly enhanced reasoning in Vision-Language Models (VLMs). Extending this paradigm, Visual CoT integrates explicit visual edits, such as cropping or annotating regions of interest, into the reasoning process, achieving superior multimodal performance. However, the robustness of Visual CoT-based VLMs against image-level noise remains unexplored. In thi...

---

## 211. Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation

**Authors:** Abdelrahman Eldesokey, Aleksandar Cvejic, Bernard Ghanem, Peter Wonka

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.21989v1) | > We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. Howev...

---

## 212. Causal-EPIG: A Prediction-Oriented Active Learning Framework for CATE Estimation

**Authors:** Erdun Gao, Jake Fawkes, Dino Sejdinovic

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.21866v1) | > Estimating the Conditional Average Treatment Effect (CATE) is often constrained by the high cost of obtaining outcome measurements, making active learning essential. However, conventional active learning strategies suffer from a fundamental objective mismatch. They are designed to reduce uncertainty in model parameters or in observable factual outcomes, failing to directly target the unobservable ...

---

## 213. Dense Semantic Matching with VGGT Prior

**Authors:** Songlin Yang, Tianyi Wei, Yushi Lan, Zeqi Xiao, Anyi Rao

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.21263v1) | > Semantic matching aims to establish pixel-level correspondences between instances of the same category and represents a fundamental task in computer vision. Existing approaches suffer from two limitations: (i) Geometric Ambiguity: Their reliance on 2D foundation model features (e.g., Stable Diffusion, DINO) often fails to disambiguate symmetric structures, requiring extra fine-tuning yet lacking g...

---

## 214. Vision Transformers: the threat of realistic adversarial patches

**Authors:** Kasper Cools, Clara Maathuis, Alexander M. van Oers, Claudia S. Hübner, Nikos Deligiannis

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.21084v1) | > The increasing reliance on machine learning systems has made their security a critical concern. Evasion attacks enable adversaries to manipulate the decision-making processes of AI systems, potentially causing security breaches or misclassification of targets. Vision Transformers (ViTs) have gained significant traction in modern machine learning due to increased 1) performance compared to Convolut...

---

## 215. Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture

**Authors:** Nico Schulthess, Ender Konukoglu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.19997v1) | > In this work, we leverage informative embeddings from foundational models for unsupervised anomaly detection in medical imaging. For small datasets, a memory-bank of normative features can directly be used for anomaly detection which has been demonstrated recently. However, this is unsuitable for large medical datasets as the computational burden increases substantially. Therefore, we propose to m...

---

## 216. A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring

**Authors:** Giovanni De Gasperis, Sante Dino Facchini

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.15848v1) | > Industrial monitoring systems, especially when deployed in Industry 4.0 environments, are experiencing a shift in paradigm from traditional rule-based architectures to data-driven approaches leveraging machine learning and artificial intelligence. This study presents a comparison between these two methodologies, analyzing their respective strengths, limitations, and application scenarios, and prop...

---

## 217. ORB: Operating Room Bot, Automating Operating Room Logistics through Mobile Manipulation

**Authors:** Jinkai Qiu, Yungjun Kim, Gaurav Sethia, Tanmay Agarwal, Siddharth Ghodasara

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.15600v1) | > Efficiently delivering items to an ongoing surgery in a hospital operating room can be a matter of life or death. In modern hospital settings, delivery robots have successfully transported bulk items between rooms and floors. However, automating item-level operating room logistics presents unique challenges in perception, efficiency, and maintaining sterility. We propose the Operating Room Bot (OR...

---

## 218. NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training

**Authors:** Moinak Bhattacharya, Angelica P. Kurtz, Fabio M. Iwamoto, Prateek Prasanna, Gagandeep Singh

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.15416v1) | > Neuro-oncology poses unique challenges for machine learning due to heterogeneous data and tumor complexity, limiting the ability of foundation models (FMs) to generalize across cohorts. Existing FMs also perform poorly in predicting uncommon molecular markers, which are essential for treatment response and risk stratification. To address these gaps, we developed a neuro-oncology specific FM with a...

---

## 219. DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images

**Authors:** Kazuma Nagata, Naoshi Kaneko

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.14685v2) | > Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models t...

---

## 220. Data Scaling Laws for Radiology Foundation Models

**Authors:** Maximilian Ilse, Harshita Sharma, Anton Schwaighofer, Sam Bond-Taylor, Fernando Pérez-García

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.12818v1) | > Foundation vision encoders such as CLIP and DINOv2, trained on web-scale data, exhibit strong transfer performance across tasks and datasets. However, medical imaging foundation models remain constrained by smaller datasets, limiting our understanding of how data scale and pretraining paradigms affect performance in this setting. In this work, we systematically study continual pretraining of two v...

---

## 221. ToMA: Token Merge with Attention for Diffusion Models

**Authors:** Wenbo Lu, Shaoyi Zheng, Yuxuan Xia, Shengjie Wang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.10918v3) | > Diffusion models excel in high-fidelity image generation but face scalability limits due to transformers' quadratic attention complexity. Plug-and-play token reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens in generated images but rely on GPU-inefficient operations (e.g., sorting, scattered writes), introducing overheads that negate theoretical speedups when paired w...

---

## 222. InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation

**Authors:** Sanyam Jain, Khuram Naveed, Illia Oleksiienko, Alexandros Iosifidis, Ruben Pauwels

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.12239v1) | > This work introduces InJecteD, a framework for interpreting Denoising Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during the denoising process of 2D point cloud generation. We apply this framework to three datasets from the Datasaurus Dozen bullseye, dino, and circle using a simplified DDPM architecture with customizable input and time embeddings. Our approach quantifie...

---

## 223. When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection

**Authors:** Rabin Dulal, Lihong Zheng, Muhammad Ashad Kabir

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.06427v2) | > Muzzle patterns are among the most effective biometric traits for cattle identification. Fast and accurate detection of the muzzle region as the region of interest is critical to automatic visual cattle identification.. Earlier approaches relied on manual detection, which is labor-intensive and inconsistent. Recently, automated methods using supervised models like YOLO have become popular for muzz...

---

## 224. Symbolic Graphics Programming with Large Language Models

**Authors:** Yamei Chen, Haoquan Zhang, Yangyi Huang, Zeju Qiu, Kaipeng Zhang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.05208v1) | > Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate...

---

## 225. Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning

**Authors:** Jasmine Shone, Zhening Li, Shaden Alshammari, Mark Hamilton, William Freeman

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.04734v2) | > The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We presen...

---

## 226. Efficient Odd-One-Out Anomaly Detection

**Authors:** Silvio Chito, Paolo Rabino, Tatiana Tommasi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.04326v1) | > The recently introduced odd-one-out anomaly detection task involves identifying the odd-looking instances within a multi-object scene. This problem presents several challenges for modern deep learning models, demanding spatial reasoning across multiple views and relational reasoning to understand context and generalize across varying object categories and layouts. We argue that these challenges mu...

---

## 227. VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision

**Authors:** Safouane El Ghazouali, Umberto Michelucci

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.04180v1) | > AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. ...

---

## 228. Enhancing Self-Supervised Speaker Verification Using Similarity-Connected Graphs and GCN

**Authors:** Zhaorui Sun, Yihao Chen, Jialong Wang, Minqiang Xu, Lei Fang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.04147v1) | > With the continuous development of speech recognition technology, speaker verification (SV) has become an important method for identity authentication. Traditional SV methods rely on handcrafted feature extraction, while deep learning has significantly improved system performance. However, the scarcity of labeled data still limits the widespread application of deep learning in SV. Self-supervised ...

---

## 229. 2D Gaussian Splatting with Semantic Alignment for Image Inpainting

**Authors:** Hongyu Li, Chaofeng Chen, Xiaoming Li, Guangming Lu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2509.01964v1) | > Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainti...

---

## 230. SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing

**Authors:** Jakub Straka, Ivan Gruber

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.21402v1) | > Self-supervised learning has emerged as a powerful tool for remote sensing, where large amounts of unlabeled data are available. In this work, we investigate the use of DINO, a contrastive self-supervised method, for pretraining on remote sensing imagery. We introduce SatDINO, a model tailored for representation learning in satellite imagery. Through extensive experiments on multiple datasets in m...

---

## 231. Generalizable Object Re-Identification via Visual In-Context Prompting

**Authors:** Zhizhong Huang, Xiaoming Liu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.21222v1) | > Current object re-identification (ReID) methods train domain-specific models (e.g., for persons or vehicles), which lack generalization and demand costly labeled data for new categories. While self-supervised learning reduces annotation needs by learning instance-wise invariance, it struggles to capture \textit{identity-sensitive} features critical for ReID. This paper proposes Visual In-Context P...

---

## 232. Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation

**Authors:** Zhixiang Chi, Yanan Wu, Li Gu, Huan Liu, Ziqiang Wang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.20265v1) | > CLIP exhibits strong visual-textual alignment but struggle with open-vocabulary segmentation due to poor localization. Prior methods enhance spatial coherence by modifying intermediate attention. But, this coherence isn't consistently propagated to the final output due to subsequent operations such as projections. Additionally, intermediate attention lacks direct interaction with text representati...

---

## 233. Self-supervised structured object representation learning

**Authors:** Oussama Hadjerci, Antoine Letienne, Mohamed Abbas Hedjazi, Adel Hafiane

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.19864v1) | > Self-supervised learning (SSL) has emerged as a powerful technique for learning visual representations. While recent SSL approaches achieve strong results in global image understanding, they are limited in capturing the structured representation in scenes. In this work, we propose a self-supervised approach that progressively builds structured visual representations by combining semantic grouping,...

---

## 234. DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers

**Authors:** Michael Podsiadly, Brendon K Lay

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.17509v1) | > Training AI models to understand images without costly labeled data remains a challenge. We combine two techniques--DINO (teacher-student learning) and Barlow Twins (redundancy reduction)--to create a model that learns better with fewer labels and less compute. While both DINO and Barlow Twins have independently demonstrated strong performance in self-supervised learning, each comes with limitatio...

---

## 235. Beyond Imaging: Vision Transformer Digital Twin Surrogates for 3D+T Biological Tissue Dynamics

**Authors:** Kaan Berke Ugurlar, Joaquín de Navascués, Michael Taynnan Barros

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.15883v2) | > Understanding the dynamic organization and homeostasis of living tissues requires high-resolution, time-resolved imaging coupled with methods capable of extracting interpretable, predictive insights from complex datasets. Here, we present the Vision Transformer Digital Twin Surrogate Network (VT-DTSN), a deep learning framework for predictive modeling of 3D+T imaging data from biological tissue. B...

---

## 236. Ultrafast Nonequilibrium Enhancement of Electron-Phonon Interaction in 2H-MoTe$_2$

**Authors:** Nina Girotto Erhardt, Sotirios Fragkos, Dominique Descamps, Stéphane Petit, Michael Schüler

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.12239v1) | > Understanding nonequilibrium electron-phonon interactions at the microscopic level and on ultrafast timescales is a central goal of modern condensed matter physics. Combining time- and angle-resolved extreme ultraviolet photoemission spectroscopy with constrained density functional perturbation theory, we demonstrate that photoexcited carrier density can serve as a tuning knob to enhance electron-...

---

## 237. Splat Feature Solver

**Authors:** Butian Xiong, Rong Liu, Kenneth Xu, Meida Chen, Andrew Feng

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.12216v1) | > Feature lifting has emerged as a crucial component in 3D scene understanding, enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP) onto splat-based 3D representations. The core challenge lies in optimally assigning rich general attributes to 3D primitives while addressing the inconsistency issues from multi-view images. We present a unified, kernel- and feature-agnostic for...

---

## 238. TimeSenCLIP: A Time Series Vision-Language Model for Remote Sensing Using Single-Pixel

**Authors:** Pallavi Jain, Diego Marcos, Dino Ienco, Roberto Interdonato, Tristan Berchoux

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.11919v2) | > Vision-language models (VLMs) have shown significant promise in remote sensing applications, particularly for land-use and land-cover (LULC) mapping via zero-shot classification and retrieval. However, current approaches face several key challenges, such as the dependence on caption-based supervision, which is often not available or very limited in terms of the covered semantics, and the fact of b...

---

## 239. DeepFleet: Multi-Agent Foundation Models for Mobile Robots

**Authors:** Ameya Agaskar, Sriram Siva, William Pickering, Kyle O'Brien, Charles Kekeh

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.08574v2) | > We introduce DeepFleet, a suite of foundation models designed to support coordination and planning for large-scale mobile robot fleets. These models are trained on fleet movement data, including robot positions, goals, and interactions, from hundreds of thousands of robots in Amazon warehouses worldwide. DeepFleet consists of four architectures that each embody a distinct inductive bias and collec...

---

## 240. Bounding Distributional Shifts in World Modeling through Novelty Detection

**Authors:** Eric Jing, Abdeslam Boularias

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.06096v1) | > Recent work on visual world models shows significant promise in latent state dynamics obtained from pre-trained image backbones. However, most of the current approaches are sensitive to training quality, requiring near-complete coverage of the action and state space during training to prevent divergence during inference. To make a model-based planning algorithm more robust to the quality of the le...

---

## 241. Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation

**Authors:** Jinxing Zhou, Yanghao Zhou, Mingfei Han, Tong Wang, Xiaojun Chang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.04418v1) | > Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects in audible videos based on given reference expressions. Prior works typically rely on learning latent embeddings via multimodal fusion to prompt a tunable SAM/SAM2 decoder for segmentation, which requires strong pixel-level supervision and lacks interpretability. From a novel perspective of explicit reference understandin...

---

## 242. MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis

**Authors:** Ning Zhu, Xiaochuan Ma, Shaoting Zhang, Guotai Wang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.03441v2) | > Cold-Start Active Learning (CSAL) aims to select informative samples for annotation without prior knowledge, which is important for improving annotation efficiency and model performance under a limited annotation budget in medical image analysis. Most existing CSAL methods rely on Self-Supervised Learning (SSL) on the target dataset for feature extraction, which is inefficient and limited by insuf...

---

## 243. Towards Robust Semantic Correspondence: A Benchmark and Insights

**Authors:** Wenyue Chong

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2508.00272v1) | > Semantic correspondence aims to identify semantically meaningful relationships between different images and is a fundamental challenge in computer vision. It forms the foundation for numerous tasks such as 3D reconstruction, object tracking, and image editing. With the progress of large-scale vision models, semantic correspondence has achieved remarkable performance in controlled and high-quality ...

---

## 244. Foundations and Models in Modern Computer Vision: Key Building Blocks in Landmark Architectures

**Authors:** Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.23357v2) | > This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analysis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Visio...

---

## 245. Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations

**Authors:** Nils Hütten, Florian Hölken, Hasan Tercan, Tobias Meisen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.21723v1) | > In recent years, Explainable AI has gained traction as an approach to enhancing model interpretability and transparency, particularly in complex models such as detection transformers. Despite rapid advancements, a substantial research gap remains in understanding the distinct roles of internal components - knowledge that is essential for improving transparency and efficiency. Inspired by neuroscie...

---

## 246. Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception

**Authors:** Marcel Simon, Tae-Ho Kim, Seul-Ki Yeom

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.19272v1) | > Self-supervised image encoders such as DINO have recently gained significant interest for learning robust visual features without labels. However, most SSL methods train on static images and miss the temporal cues inherent in videos. We introduce a video-distilled single-image encoder trained to predict the next-frame representation from the current frame. This simple objective injects 3D spatial ...

---

## 247. Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection

**Authors:** Francesco Dalmonte, Emirhan Bayar, Emre Akbas, Mariana-Iuliana Georgescu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.18481v1) | > Anomaly detection in medical images is an important yet challenging task due to the diversity of possible anomalies and the practical impossibility of collecting comprehensively annotated data sets. In this work, we tackle unsupervised medical anomaly detection proposing a modernized autoencoder-based framework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained vision foundation...

---

## 248. Vision Transformer attention alignment with human visual perception in aesthetic object evaluation

**Authors:** Miguel Carrasco, César González-Martín, José Aranda, Luis Oliveros

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.17616v1) | > Visual attention mechanisms play a crucial role in human perception and aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have demonstrated remarkable capabilities in computer vision tasks, yet their alignment with human visual attention patterns remains underexplored, particularly in aesthetic contexts. This study investigates the correlation between human visual attention and V...

---

## 249. Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation

**Authors:** Naeem Paeedeh, Mahardhika Pratama, Imam Mustafa Kamal, Wolfgang Mayer, Jimmy Cao

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.15243v2) | > Despite the progress in cross-domain few-shot learning, a model pre-trained with DINO combined with a prototypical classifier outperforms the latest SOTA methods. A crucial limitation that needs to be overcome is that updating too many parameters of the transformers leads to overfitting due to the scarcity of labeled samples. To address this challenge, we propose a new concept, coalescent projecti...

---

## 250. PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations

**Authors:** Yu Wei, Jiahui Zhang, Xiaoqin Zhang, Ling Shao, Shijian Lu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.13891v2) | > COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing attention due to its remarkable performance in reconstructing high-quality 3D scenes from unposed images or videos. However, it often struggles to handle scenes with complex camera trajectories as featured by drastic rotation and translation across adjacent camera views, leading to degraded estimation of camera poses and f...

---

## 251. Electron-phonon-dominated charge-density-wave fluctuations in TiSe$_2$ accessed by ultrafast nonequilibrium dynamics

**Authors:** Sotirios Fragkos, Hibiki Orio, Nina Girotto Erhardt, Akib Jabed, Sarath Sasi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.12430v3) | > The complex phase diagram of 1T-TiSe2 consists of a charge density wave (CDW) below 200 K, and CDW fluctuations of still unknown origin at higher temperatures. Here, we use time-resolved extreme ultraviolet momentum microscopy and density functional perturbation theory to uncover the formation mechanism of CDW fluctuations and their spectral features at 295 K. We investigated the transient dynamic...

---

## 252. Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants

**Authors:** Sybelle Goedicke-Fritz, Michelle Bous, Annika Engel, Matthias Flotho, Pascal Hirsch

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.12269v3) | > Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD progno...

---

## 253. Classifying the complexity of models of arithmetic

**Authors:** David Gonzalez, Mateusz Łełyk, Dino Rossegger, Patryk Szlufik

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.12025v1) | > We classify the possible Scott complexities for models of Peano arithmetic. We construct models of particular complexities by first giving a complete Scott analysis of colored linear orderings and constructing models of Peano arithmetic from these colored orderings. We also provide tight connections of certain Scott complexities with notions from the classical theory of models of Peano arithmetic,...

---

## 254. Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?

**Authors:** Hanxue Gu, Yaqian Chen, Nicholas Konz, Qihang Li, Maciej A. Mazurowski

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.11569v2) | > Foundation models, pre-trained on large image datasets and capable of capturing rich feature representations, have recently shown potential for zero-shot image registration. However, their performance has mostly been tested in the context of rigid or less complex structures, such as the brain or abdominal organs, and it remains unclear whether these models can handle more challenging, deformable a...

---

## 255. Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting

**Authors:** Changlu Chen, Yanbin Liu, Chaoxi Niu, Ling Chen, Tianqing Zhu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.11558v1) | > Foundation models have achieved remarkable success in natural language processing and computer vision, demonstrating strong capabilities in modeling complex patterns. While recent efforts have explored adapting large language models (LLMs) for time-series forecasting, LLMs primarily capture one-dimensional sequential dependencies and struggle to model the richer spatio-temporal (ST) correlations e...

---

## 256. NexViTAD: Few-shot Unsupervised Cross-Domain Defect Detection via Vision Foundation Models and Multi-Task Learning

**Authors:** Tianwei Mu, Feiyu Duan, Bo Zhou, Dan Xue, Manhong Huang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.07579v1) | > This paper presents a novel few-shot cross-domain anomaly detection framework, Nexus Vision Transformer for Anomaly Detection (NexViTAD), based on vision foundation models, which effectively addresses domain-shift challenges in industrial anomaly detection through innovative shared subspace projection mechanisms and multi-task learning (MTL) module. The main innovations include: (1) a hierarchical...

---

## 257. Unconventional Materials for Light Dark Matter Detection

**Authors:** Yonit Hochberg, Dino Novko, Rotem Ovadia, Antonio Politano

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.07164v1) | > We propose the use of several unconventional materials as detectors for dark matter with mass beneath the MeV scale. These include the transition-metal dichalcogenide TiSe$_2$ hosting a low-energy plasmon in the charge-density-wave phase, Sr$_2$RuO$_4$ containing a low-energy acoustic demon mode, and hole-doped diamond with tunable optical and acoustic plasmon frequencies. We perform first-princip...

---

## 258. Does Data Scaling Lead to Visual Compositional Generalization?

**Authors:** Arnas Uselis, Andrea Dittadi, Seong Joon Oh

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.07102v1) | > Compositional understanding is crucial for human intelligence, yet it remains unclear whether contemporary vision models exhibit it. The dominant machine learning paradigm is built on the premise that scaling data and model sizes will improve out-of-distribution performance, including compositional generalization. We test this premise through controlled experiments that systematically vary data sc...

---

## 259. Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement

**Authors:** Qiyuan Dai, Hanzhuo Huang, Yu Wu, Sibei Yang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.06928v1) | > Generalized Category Discovery (GCD) aims to recognize unlabeled images from known and novel classes by distinguishing novel classes from known ones, while also transferring knowledge from another set of labeled images with known classes. Existing GCD methods rely on self-supervised vision transformers such as DINO for representation learning. However, focusing solely on the global representation ...

---

## 260. IMPACT: Industrial Machine Perception via Acoustic Cognitive Transformer

**Authors:** Changheon Han, Yuseop Sim, Hoin Jung, Jiho Lee, Hojun Lee

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.06481v1) | > Acoustic signals from industrial machines offer valuable insights for anomaly detection, predictive maintenance, and operational efficiency enhancement. However, existing task-specific, supervised learning methods often scale poorly and fail to generalize across diverse industrial scenarios, whose acoustic characteristics are distinct from general audio. Furthermore, the scarcity of accessible, la...

---

## 261. Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion

**Authors:** Aleksandar Jevtić, Christoph Reich, Felix Wimbauer, Oliver Hahn, Christian Rupprecht

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.06230v2) | > Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training...

---

## 262. SingLoRA: Low Rank Adaptation Using a Single Matrix

**Authors:** David Bensaïd, Noam Rotstein, Roy Velich, Daniel Bensaïd, Ron Kimmel

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.05566v1) | > Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient fine-tuning of large pretrained models. LoRA augments the pre-trained weights of a model by adding the product of two smaller matrices that together form a low-rank matrix update. Recent research has shown that scale disparities between these two matrices often cause unstable training dynamics, leading to suboptimal performa...

---

## 263. SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability

**Authors:** Ali Nasiri-Sarvi, Hassan Rivaz, Mahdi S. Hosseini

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.06265v1) | > Understanding how different AI models encode the same high-level concepts, such as objects or attributes, remains challenging because each model typically produces its own isolated representation. Existing interpretability methods like Sparse Autoencoders (SAEs) produce latent concepts individually for each model, resulting in incompatible concept spaces and limiting cross-model interpretability. ...

---

## 264. Neural-Driven Image Editing

**Authors:** Pengfei Zhou, Jie Xia, Xiaopeng Peng, Wangbo Zhao, Zilong Ye

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.05397v3) | > Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffu...

---

## 265. Evolving HPC services to enable ML workloads on HPE Cray EX

**Authors:** Stefano Schuppli, Fawzi Mohamed, Henrique Mendonça, Nina Mujkanovic, Elia Palme

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2507.01880v1) | > The Alps Research Infrastructure leverages GH200 technology at scale, featuring 10,752 GPUs. Accessing Alps provides a significant computational advantage for researchers in Artificial Intelligence (AI) and Machine Learning (ML). While Alps serves a broad range of scientific communities, traditional HPC services alone are not sufficient to meet the dynamic needs of the ML community. This paper pre...

---

## 266. Grounding DINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models

**Authors:** Hamza Rasaee, Taha Koleilat, Hassan Rivaz

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.23903v3) | [DOI](https://doi.org/10.1109/TUFFC.2025.3605285)

> Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 (Segment Anything Model2) to enable object segmentation across multiple ultrasound organs. A total of ...

---

## 267. Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?

**Authors:** Annika Mütze, Sadia Ilyas, Christian Dörpelkus, Matthias Rottmann

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.23751v1) | > Open-vocabulary object detectors such as Grounding DINO are trained on vast and diverse data, achieving remarkable performance on challenging datasets. Due to that, it is unclear where to find their limitations, which is of major concern when using in safety-critical applications. Real-world data does not provide sufficient control, required for a rigorous evaluation of model generalization. In co...

---

## 268. When Test-Time Adaptation Meets Self-Supervised Models

**Authors:** Jisu Han, Jihee Park, Dongyoon Han, Wonjun Hwang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.23529v1) | > Training on test-time data enables deep learning models to adapt to dynamic environmental changes, enhancing their practical applicability. Online adaptation from source to target domains is promising but it remains highly reliant on the performance of source pretrained model. In this paper, we investigate whether test-time adaptation (TTA) methods can continuously improve models trained via self-...

---

## 269. Direct proton transfer on $^{46}$Ar supports the presence of a charge density bubble linked to a novel nuclear structure below $^{48}$Ca

**Authors:** Daniele Brugnara, Andrea Gottardo, Marlene Assiè, Carlo Barbieri, Daniele Mengoni

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.23228v2) | > The $^{46}$Ar($^3$He,d)$^{47}$K reaction was performed in inverse kinematics using a radioactive $^{46}$Ar beam produced by the SPIRAL1 facility at GANIL and a cryogenic $^{3}$He target. The AGATA-MUGAST-VAMOS setup allowed the coincident measurement of the $γ$ rays, deuterons and recoiling $^{47}$K isotopes produced by the reaction. The relative cross sections towards the proton-addition states i...

---

## 270. From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents

**Authors:** Sergio Torres Aguilar

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.20326v1) | > Robust Document Layout Analysis (DLA) is critical for the automated processing and understanding of historical documents with complex page organizations. This paper benchmarks five state-of-the-art object detection architectures on three annotated datasets representing a spectrum of codicological complexity: The e-NDP, a corpus of Parisian medieval registers (1326-1504); CATMuS, a diverse multicla...

---

## 271. OpenWildlife: Open-Vocabulary Multi-Species Wildlife Detector for Geographically-Diverse Aerial Imagery

**Authors:** Muhammed Patel, Javier Noa Turnes, Jayden Hsiao, Linlin Xu, David Clausi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.19204v1) | > We introduce OpenWildlife (OW), an open-vocabulary wildlife detector designed for multi-species identification in diverse aerial imagery. While existing automated methods perform well in specific settings, they often struggle to generalize across different species and environments due to limited taxonomic coverage and rigid model architectures. In contrast, OW leverages language-aware embeddings a...

---

## 272. Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction

**Authors:** Ankit Sanjyal

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.18208v1) | > Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction from sparse image collections. Recent work has explored integrating pre-trained vision features, particularly from DINO, to enhance few-shot reconstruction capabilities. However, the effectiveness of such approaches remains unclear, especially in extreme few-shot scenarios. In this paper, we present a systematic evaluation o...

---

## 273. Fine-Tuned Vision Transformers Capture Complex Wheat Spike Morphology for Volume Estimation from RGB Images

**Authors:** Olivia Zumsteg, Nico Graf, Aaron Haeusler, Norbert Kirchgessner, Nicola Storni

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.18060v2) | > Estimating three-dimensional morphological traits such as volume from two-dimensional RGB images presents inherent challenges due to the loss of depth information, projection distortions, and occlusions under field conditions. In this work, we explore multiple approaches for non-destructive volume estimation of wheat spikes using RGB images and structured-light 3D scans as ground truth references....

---

## 274. Gaussian Processes and Reproducing Kernels: Connections and Equivalences

**Authors:** Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, Bharath K. Sriperumbudur

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.17366v1) | > This monograph studies the relations between two approaches using positive definite kernels: probabilistic methods using Gaussian processes, and non-probabilistic methods using reproducing kernel Hilbert spaces (RKHS). They are widely studied and used in machine learning, statistics, and numerical analysis. Connections and equivalences between them are reviewed for fundamental topics such as regre...

---

## 275. AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario

**Authors:** Ciro Beneduce, Massimiliano Luca, Bruno Lepri

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.16898v1) | > Image generation models are revolutionizing many domains, and urban analysis and design is no exception. While such models are widely adopted, there is a limited literature exploring their geographic knowledge, along with the biases they embed. In this work, we generated 150 synthetic images for each state in the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two state-of-the-art ...

---

## 276. Rethinking Random Masking in Self-Distillation on ViT

**Authors:** Jihyeon Seong, Hyunkyung Han

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.10582v3) | > Vision Transformers (ViTs) have demonstrated remarkable performance across a wide range of vision tasks. In particular, self-distillation frameworks such as DINO have contributed significantly to these advances. Within such frameworks, random masking is often utilized to improve training efficiency and introduce regularization. However, recent studies have raised concerns that indiscriminate rando...

---

## 277. Improving Out-of-Distribution Detection via Dynamic Covariance Calibration

**Authors:** Kaiyu Guo, Zijian Wang, Tan Pan, Brian C. Lovell, Mahsa Baktashmotlagh

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.09399v3) | > Out-of-Distribution (OOD) detection is essential for the trustworthiness of AI systems. Methods using prior information (i.e., subspace-based methods) have shown effective performance by extracting information geometry to detect OOD data with a more appropriate distance metric. However, these methods fail to address the geometry distorted by ill-distributed samples, due to the limitation of static...

---

## 278. Inherently Faithful Attention Maps for Vision Transformers

**Authors:** Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.08915v3) | > We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regio...

---

## 279. DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models

**Authors:** Xunjie He, Christina Dao Wen Lee, Meiling Wang, Chengran Yuan, Zefan Huang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.07375v1) | > Collaborative perception plays a crucial role in enhancing environmental understanding by expanding the perceptual range and improving robustness against sensor failures, which primarily involves collaborative 3D detection and tracking tasks. The former focuses on object recognition in individual frames, while the latter captures continuous instance tracklets over time. However, existing works in ...

---

## 280. Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models

**Authors:** Yannis Spyridis, Vasileios Argyriou

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.06569v1) | > Automated sorting is crucial for improving the efficiency and scalability of textile recycling, but accurately identifying material composition and detecting contaminants from sensor data remains challenging. This paper investigates the use of standard RGB imagery, a cost-effective sensing modality, for key pre-processing tasks in an automated system. We present computer vision components designed...

---

## 281. FRAME: Pre-Training Video Feature Representations via Anticipation and Memory

**Authors:** Sethuraman TV, Savya Khosla, Vignesh Srinivasakumar, Jiahui Huang, Seoung Wug Oh

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.05543v1) | > Dense video prediction tasks, such as object tracking and semantic segmentation, require video encoders that generate temporally consistent, spatially dense features for every frame. However, existing approaches fall short: image encoders like DINO or CLIP lack temporal awareness, while video models such as VideoMAE underperform compared to image encoders on dense prediction tasks. We address this...

---

## 282. Programmable wrinkling for functionally-graded auxetic circular membranes

**Authors:** Sairam Pamulaparthi Venkata, Valentina Balbi, Michel Destradea, Dino Accoto, Giuseppe Zurlo

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.04148v1) | [DOI](https://doi.org/10.1016/j.eml.2023.102045)

> Materials with negative Poisson's ratio, also known as auxetic materials, display exotic properties such as expansion in all directions under uni-axial tension. For their unique properties, these materials find a broad range of applications in robotic, structural, aerospace, and biomedical engineering.
  In this work we study the wrinkling behavior of thin and soft auxetic membranes, subjected to ...

---

## 283. Talk2SAM: Text-Guided Semantic Enhancement for Complex-Shaped Object Segmentation

**Authors:** Luka Vetoshkin, Dmitry Yudin

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.05396v1) | > Segmenting objects with complex shapes, such as wires, bicycles, or structural grids, remains a significant challenge for current segmentation models, including the Segment Anything Model (SAM) and its high-quality variant SAM-HQ. These models often struggle with thin structures and fine boundaries, leading to poor segmentation quality. We propose Talk2SAM, a novel approach that integrates textual...

---

## 284. Attacking Attention of Foundation Models Disrupts Downstream Tasks

**Authors:** Hondamunige Prasanna Silva, Federico Becattini, Lorenzo Seidenari

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.05394v3) | > Foundation models represent the most prominent and recent paradigm shift in artificial intelligence. Foundation models are large models, trained on broad data that deliver high accuracy in many downstream tasks, often without fine-tuning. For this reason, models such as CLIP , DINO or Vision Transfomers (ViT), are becoming the bedrock of many industrial AI-powered applications. However, the relian...

---

## 285. EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment

**Authors:** Mikolaj Walczak, Romina Aalishah, Wyatt Mackey, Brittany Story, David L. Boothe

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2506.03046v1) | > Deep reinforcement learning agents are often fragile while humans remain adaptive and flexible to varying scenarios. To bridge this gap, we present EDEN, a biologically inspired navigation framework that integrates learned entorhinal-like grid cell representations and reinforcement learning to enable autonomous navigation. Inspired by the mammalian entorhinal-hippocampal system, EDEN allows agents...

---

## 286. Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation

**Authors:** Roger Ferrod, Cássio F. Dantas, Luigi Di Caro, Dino Ienco

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2505.24361v1) | > Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as robotics, autonomous driving and remote sensing. The combination of these multi-modal data enhances environmental perception by providing 3D spatial context, which is absent in standard RGB images. Although RGBD multi-modal data can be available to train computer vision models, accessing all sensor modalities during the ...

---

## 287. UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments

**Authors:** Wancai Zheng, Linlin Ou, Jiajie He, Libo Zhou, Xinyi Yu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2505.22335v1) | > Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous Localization and Mapping (SLAM) have significantly progressed in tracking and high-fidelity mapping. However, their sequential optimization framework and sensitivity to dynamic objects limit real-time performance and robustness in real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for dynamic environments t...

---

## 288. Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection

**Authors:** Guiping Cao, Wenjian Huang, Xiangyuan Lan, Jianguo Zhang, Dongmei Jiang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2505.21868v1) | > Small Object Detection (SOD) poses significant challenges due to limited information and the model's low class prediction score. While Transformer-based detectors have shown promising performance, their potential for SOD remains largely unexplored. In typical DETR-like frameworks, the CNN backbone network, specialized in aggregating local information, struggles to capture the necessary contextual ...

---

## 289. Geometry-Editable and Appearance-Preserving Object Compositon

**Authors:** Jianman Lin, Haojie Li, Chunmei Qing, Zhijing Yang, Liang Lin

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2505.20914v1) | > General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-l...

---

## 290. Hierarchical Text-Conditional Image Generation with CLIP Latents

**Authors:** A. Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen

**Year:** 2022 | **Venue:** arXiv.org | **Citations:** 8311 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2204.06125) | [DOI](https://doi.org/10.48550/arXiv.2204.06125)

> Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image repre...

---

## 291. Long-CLIP: Unlocking the Long-Text Capability of CLIP

**Authors:** Beichen Zhang, Pan Zhang, Xiao-wen Dong, Yuhang Zang, Jiaqi Wang

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 278 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.15378)

> Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective ...

---

## 292. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs

**Authors:** Christoph Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, Clayton Mullis

**Year:** 2021 | **Venue:** arXiv.org | **Citations:** 1720 | **Score:** 0.000

> Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratc...

---

## 293. CLIP-Adapter: Better Vision-Language Models with Feature Adapters

**Authors:** Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang

**Year:** 2021 | **Venue:** International Journal of Computer Vision | **Citations:** 1440 | **Score:** 0.000

[DOI](https://doi.org/10.1007/s11263-023-01891-x)

> Large-scale contrastive vision-language pretraining has shown significant progress in visual representation learning. Unlike traditional visual systems trained by a fixed set of discrete labels, a new paradigm was introduced in Radford et al. (International conference on machine learning, PMLR, 2021) to directly learn to align images with raw texts in an open-vocabulary setting. On downstream task...

---

## 294. EVA-CLIP: Improved Training Techniques for CLIP at Scale

**Authors:** Quan Sun, Yuxin Fang, Ledell Yu Wu, Xinlong Wang, Yue Cao

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 731 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2303.15389) | [DOI](https://doi.org/10.48550/arXiv.2303.15389)

> Contrastive language-image pre-training, CLIP for short, has gained increasing attention for its potential in various scenarios. In this paper, we propose EVA-CLIP, a series of models that significantly improve the efficiency and effectiveness of CLIP training. Our approach incorporates new techniques for representation learning, optimization, and augmentation, enabling EVA-CLIP to achieve superio...

---

## 295. PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents

**Authors:** Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang

**Year:** 2023 | **Venue:** International Conference on Medical Image Computing and Computer-Assisted Intervention | **Citations:** 246 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2303.07240) | [DOI](https://doi.org/10.48550/arXiv.2303.07240)

> Foundation models trained on large-scale dataset gain a recent surge in CV and NLP. In contrast, development in biomedical domain lags far behind due to data scarcity. To address this issue, we build and release PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, which is 8 times larger than before. PMC-OA covers diverse modalities or diseas...

---

## 296. Exploring CLIP for Assessing the Look and Feel of Images

**Authors:** Jianyi Wang, Kelvin C. K. Chan, Chen Change Loy

**Year:** 2022 | **Venue:** AAAI Conference on Artificial Intelligence | **Citations:** 952 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2207.12396) | [DOI](https://doi.org/10.48550/arXiv.2207.12396)

> Measuring the perception of visual content is a long-standing problem in computer vision. Many mathematical models have been developed to evaluate the look or quality of an image. Despite the effectiveness of such tools in quantifying degradations such as noise and blurriness levels, such quantification is loosely coupled with human language. When it comes to more abstract perception about the fee...

---

## 297. FG-CLIP: Fine-Grained Visual and Textual Alignment

**Authors:** Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang

**Year:** 2025 | **Venue:** International Conference on Machine Learning | **Citations:** 33 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2505.05071)

> Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal model...

---

## 298. Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP

**Authors:** Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, Liang-Chieh Chen

**Year:** 2023 | **Venue:** Neural Information Processing Systems | **Citations:** 206 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2308.02487) | [DOI](https://doi.org/10.48550/arXiv.2308.02487)

> Open-vocabulary segmentation is a challenging task requiring segmenting and recognizing objects from an open set of categories. One way to address this challenge is to leverage multi-modal models, such as CLIP, to provide image and text features in a shared embedding space, which bridges the gap between closed-vocabulary and open-vocabulary recognition. Hence, existing methods often adopt a two-st...

---

## 299. Forensics Adapter: Adapting CLIP for Generalizable Face Forgery Detection

**Authors:** Xinjie Cui, Yuezun Li, Ao Luo, Jiaran Zhou, Junyu Dong

**Year:** 2025 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 20 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52734.2025.01789)

> We describe the Forensics Adapter, an adapter network designed to transform CLIP into an effective and generalizable face forgery detector. Although CLIP is highly versatile, adapting it for face forgery detection is nontrivial as forgery-related knowledge is entangled with a wide range of unrelated knowledge. Existing methods treat CLIP merely as a feature extractor, lacking task-specific adaptat...

---

## 300. AA-CLIP: Enhancing Zero-Shot Anomaly Detection via Anomaly-Aware CLIP

**Authors:** Wenxin Ma, Xu Zhang, Qingsong Yao, Fenghe Tang, Chenxu Wu

**Year:** 2025 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 33 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52734.2025.00447)

> Anomaly detection (AD) identifies outliers for applications like defect and lesion detection. While CLIP shows promise for zero-shot AD tasks due to its strong generalization capabilities, its inherent Anomaly-Unawareness leads to limited discrimination between normal and abnormal features. To address this problem, we propose Anomaly-Aware CLIP (AA-CLIP), which enhances CLIP's anomaly discriminati...

---

## 301. AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection

**Authors:** Yunkang Cao, Jiangning Zhang, Luca Frittoli, Yuqi Cheng, Weiming Shen

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 135 | **Score:** 0.000

[DOI](https://doi.org/10.1007/978-3-031-72761-0_4)

> Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories. This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP. AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data. Two types of learnable prompts are pr...

---

## 302. Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP

**Authors:** Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao

**Year:** 2022 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 618 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2210.04150) | [DOI](https://doi.org/10.1109/CVPR52729.2023.00682)

> Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-tra...

---

## 303. Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification

**Authors:** Renrui Zhang, Zhang Wei, Rongyao Fang, Peng Gao, Kunchang Li

**Year:** 2022 | **Venue:** European Conference on Computer Vision | **Citations:** 457 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2207.09519) | [DOI](https://doi.org/10.48550/arXiv.2207.09519)

> Contrastive Vision-Language Pre-training, known as CLIP, has provided a new paradigm for learning visual representations using large-scale image-text pairs. It shows impressive performance on downstream tasks by zero-shot knowledge transfer. To further enhance CLIP's adaption capability, existing methods proposed to fine-tune additional learnable modules, which significantly improves the few-shot ...

---

## 304. CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection

**Authors:** Jie Liu, Yixiao Zhang, Jieneng Chen, Junfei Xiao, Yongyi Lu

**Year:** 2023 | **Venue:** IEEE International Conference on Computer Vision | **Citations:** 291 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2301.00785) | [DOI](https://doi.org/10.1109/ICCV51070.2023.01934)

> An increasing number of public datasets have shown a marked impact on automated organ segmentation and tumor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited investigation of diverse types of tumors, the resulting models are often limited to segmenting specific organs/tumors and ignore the semantics of anatomical structures, nor can they...

---

## 305. X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval

**Authors:** Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang

**Year:** 2022 | **Venue:** ACM Multimedia | **Citations:** 403 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2207.07285) | [DOI](https://doi.org/10.1145/3503161.3547910)

> Video-text retrieval has been a crucial and fundamental task in multi-modal research. The development of video-text retrieval has been considerably promoted by large-scale multi-modal contrastive pre-training, which primarily focuses on coarse-grained or fine-grained contrast. However, cross-grained contrast, which is the contrast between coarse-grained representations and fine-grained representat...

---

## 306. Know "No" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP

**Authors:** Junsung Park, Jungbeom Lee, Jongyoon Song, Sangwon Yu, Dahuin Jung

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 14 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2501.10913)

> While CLIP has significantly advanced multimodal understanding by bridging vision and language, the inability to grasp negation - such as failing to differentiate concepts like"parking"from"no parking"- poses substantial challenges. By analyzing the data used in the public CLIP model's pre-training, we posit this limitation stems from a lack of negation-inclusive data. To address this, we introduc...

---

## 307. Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene Understanding

**Authors:** Mohammed Elhenawy, Huthaifa I. Ashqar, A. Rakotonirainy, Taqwa I. Alhadidi, Ahmed Jaber

**Year:** 2025 | **Venue:** Electronics | **Citations:** 21 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2501.05566)

> Scene understanding is essential for enhancing driver safety, generating human-centric explanations for Automated Vehicle (AV) decisions, and leveraging Artificial Intelligence (AI) for retrospective driving video analysis. This study developed a dynamic scene retrieval system using Contrastive Language–Image Pretraining (CLIP) models, which can be optimized for real-time deployment on edge device...

---

## 308. Meta CLIP 2: A Worldwide Scaling Recipe

**Authors:** Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 16 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2507.22062)

> Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method ...

---

## 309. CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels

**Authors:** Siyuan Li, Li Sun, Qingli Li

**Year:** 2022 | **Venue:** AAAI Conference on Artificial Intelligence | **Citations:** 287 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2211.13977) | [DOI](https://doi.org/10.48550/arXiv.2211.13977)

> Pre-trained vision-language models like CLIP have recently shown superior performances on various downstream tasks, including image classification and segmentation. However, in fine-grained image re-identification (ReID), the labels are indexes, lacking concrete text descriptions. Therefore, it remains to be determined how such models could be applied to these tasks. This paper first finds out tha...

---

## 310. Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models

**Authors:** Christian Schlarmann, N. Singh, Francesco Croce, Matthias Hein

**Year:** 2024 | **Venue:** International Conference on Machine Learning | **Citations:** 86 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.12336)

> Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation model...

---

## 311. Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)

**Authors:** Usha Bhalla, Alexander X. Oesterling, Suraj Srinivas, F. Calmon, Himabindu Lakkaraju

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 83 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.10376)

> CLIP embeddings have demonstrated remarkable performance across a wide range of multimodal applications. However, these high-dimensional, dense vector representations are not easily interpretable, limiting our understanding of the rich structure of CLIP and its use in downstream applications that require transparency. In this work, we show that the semantic structure of CLIP's latent space can be ...

---

## 312. EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters

**Authors:** Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 80 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.04252)

> Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models. We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classificati...

---

## 313. ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation

**Authors:** Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu

**Year:** 2024 | **Venue:** Neurocomputing | **Citations:** 78 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2401.12665)

> Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to gener...

---

## 314. ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation

**Authors:** Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 71 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2408.04883)

> Open-vocabulary semantic segmentation requires models to effectively integrate visual representations with open-vocabulary semantic labels. While Contrastive Language-Image Pre-training (CLIP) models shine in recognizing visual concepts from text, they often struggle with segment coherence due to their limited localization ability. In contrast, Vision Foundation Models (VFMs) excel at acquiring sp...

---

## 315. Improving CLIP Training with Language Rewrites

**Authors:** Lijie Fan, Dilip Krishnan, Phillip Isola, D. Katabi, Yonglong Tian

**Year:** 2023 | **Venue:** Neural Information Processing Systems | **Citations:** 249 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2305.20088) | [DOI](https://doi.org/10.48550/arXiv.2305.20088)

> Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image...

---

## 316. What does CLIP know about a red circle? Visual prompt engineering for VLMs

**Authors:** Aleksandar Shtedritski, C. Rupprecht, A. Vedaldi

**Year:** 2023 | **Venue:** IEEE International Conference on Computer Vision | **Citations:** 231 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICCV51070.2023.01101)

> Large-scale Vision-Language Models, such as CLIP, learn powerful image-text representations that have found numerous applications, from zero-shot classification to text-to-image generation. Despite that, their capabilities for solving novel discriminative tasks via prompting fall behind those of large language models, such as GPT-3. Here we explore the idea of visual prompt engineering for solving...

---

## 317. CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval

**Authors:** Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei

**Year:** 2021 | **Venue:** Neurocomputing | **Citations:** 1004 | **Score:** 0.000

[DOI](https://doi.org/10.1016/j.neucom.2022.07.028)

> ...

---

## 318. PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?

**Authors:** Sedigheh Eslami, C. Meinel, Gerard de Melo

**Year:** 2023 | **Venue:** Findings | **Citations:** 223 | **Score:** 0.000

[PDF](https://aclanthology.org/2023.findings-eacl.88.pdf) | [DOI](https://doi.org/10.18653/v1/2023.findings-eacl.88)

> Contrastive Language–Image Pre-training (CLIP) has shown remarkable success in learning with cross-modal supervision from extensive amounts of image–text pairs collected online. Thus far, the effectiveness of CLIP has been investigated primarily in general-domain multimodal problems. In this work, we evaluate the effectiveness of CLIP for the task of Medical Visual Question Answering (MedVQA). We ...

---

## 319. ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference

**Authors:** Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 68 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.12442)

> Despite the success of large-scale pretrained Vision-Language Models (VLMs) especially CLIP in various open-vocabulary tasks, their application to semantic segmentation remains challenging, producing noisy segmentation maps with mis-segmented regions. In this paper, we carefully re-investigate the architecture of CLIP, and identify residual connections as the primary source of noise that degrades ...

---

## 320. Extract Free Dense Labels from CLIP

**Authors:** Chong Zhou, Chen Change Loy, Bo Dai

**Year:** 2021 | **Venue:** European Conference on Computer Vision | **Citations:** 649 | **Score:** 0.000

[DOI](https://doi.org/10.1007/978-3-031-19815-1_40)

> Contrastive Language-Image Pre-training (CLIP) has made a remarkable breakthrough in open-vocabulary zero-shot image recognition. Many recent studies leverage the pre-trained CLIP models for image-level classification and manipulation. In this paper, we wish examine the intrinsic potential of CLIP for pixel-level dense prediction, specifically in semantic segmentation. To this end, with minimal mo...

---

## 321. CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP

**Authors:** Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma

**Year:** 2023 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 216 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2301.04926) | [DOI](https://doi.org/10.1109/CVPR52729.2023.00678)

> Contrastive Language-Image Pre-training (CLIP) achieves promising results in 2D zero-shot and few-shot learning. Despite the impressive performance in 2D, applying CLIP to help the learning in 3D scene understanding has yet to be explored. In this paper, we make the first attempt to investigate how CLIP knowledge benefits 3D scene understanding. We propose CLIP2Scene, a simple yet effective framew...

---

## 322. CLIP Behaves like a Bag-of-Words Model Cross-modally but not Uni-modally

**Authors:** Darina Koishigarina, Arnas Uselis, S. Oh

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 13 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2502.03566)

> CLIP (Contrastive Language-Image Pretraining) has become a popular choice for various downstream tasks. However, recent studies have questioned its ability to represent compositional concepts effectively. These works suggest that CLIP often acts like a bag-of-words (BoW) model, interpreting images and text as sets of individual concepts without grasping the structural relationships. In particular,...

---

## 323. Demystifying CLIP Data

**Authors:** Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao (Bernie) Huang, Russell Howes

**Year:** 2023 | **Venue:** International Conference on Learning Representations | **Citations:** 205 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2309.16671) | [DOI](https://doi.org/10.48550/arXiv.2309.16671)

> Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been...

---

## 324. SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?

**Authors:** Hasan Hammoud, Hani Itani, Fabio Pizzati, Philip H. S. Torr, Adel Bibi

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 56 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.01832)

> We present SynthCLIP, a CLIP model trained on entirely synthetic text-image pairs. Leveraging recent text-to-image (TTI) networks and large language models (LLM), we generate synthetic datasets of images and corresponding captions at scale, with no human intervention. In this work, we provide an analysis on CLIP models trained on synthetic data. We provide insights on the data generation strategy,...

---

## 325. C2P-CLIP: Injecting Category Common Prompt in CLIP to Enhance Generalization in Deepfake Detection

**Authors:** Chuangchuang Tan, Renshuai Tao, Huan Liu, Guanghua Gu, Baoyuan Wu

**Year:** 2024 | **Venue:** AAAI Conference on Artificial Intelligence | **Citations:** 58 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2408.09647)

> This work focuses on AIGC detection to develop universal detectors capable of identifying various types of forgery images. Recent studies have found large pre-trained models, such as CLIP, are effective for generalizable deepfake detection along with linear classifiers. However, two critical issues remain unresolved: 1) understanding why CLIP features are effective on deepfake detection through a ...

---

## 326. ClipCap: CLIP Prefix for Image Captioning

**Authors:** Ron Mokady, Amir Hertz

**Year:** 2021 | **Venue:** arXiv.org | **Citations:** 800 | **Score:** 0.000

> Image captioning is a fundamental task in vision-language understanding, where the model predicts a textual informative caption to a given input image. In this paper, we present a simple approach to address this task. We use CLIP encoding as a prefix to the caption, by employing a simple mapping network, and then fine-tunes a language model to generate the image captions. The recently proposed CLI...

---

## 327. VCP-CLIP: A visual context prompting model for zero-shot anomaly segmentation

**Authors:** Zhen Qu, Xian Tao, Mukesh Prasad, Fei Shen, Zhengtao Zhang

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 55 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.12276)

> Recently, large-scale vision-language models such as CLIP have demonstrated immense potential in zero-shot anomaly segmentation (ZSAS) task, utilizing a unified model to directly detect anomalies on any unseen product with painstakingly crafted text prompts. However, existing methods often assume that the product category to be inspected is known, thus setting product-specific text prompts, which ...

---

## 328. MotionCLIP: Exposing Human Motion Generation to CLIP Space

**Authors:** Guy Tevet, Brian Gordon, Amir Hertz, Amit H. Bermano, D. Cohen-Or

**Year:** 2022 | **Venue:** European Conference on Computer Vision | **Citations:** 462 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2203.08063) | [DOI](https://doi.org/10.48550/arXiv.2203.08063)

> We introduce MotionCLIP, a 3D human motion auto-encoder featuring a latent embedding that is disentangled, well behaved, and supports highly semantic textual descriptions. MotionCLIP gains its unique power by aligning its latent space with that of the Contrastive Language-Image Pre-training (CLIP) model. Aligning the human motion manifold to CLIP space implicitly infuses the extremely rich semanti...

---

## 329. VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance

**Authors:** Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan

**Year:** 2022 | **Venue:** European Conference on Computer Vision | **Citations:** 441 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2204.08583) | [DOI](https://doi.org/10.48550/arXiv.2204.08583)

> Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demo...

---

## 330. PointCLIP: Point Cloud Understanding by CLIP

**Authors:** Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao

**Year:** 2021 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 576 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2112.02413) | [DOI](https://doi.org/10.1109/CVPR52688.2022.00836)

> Recently, zero-shot and few-shot learning via Contrastive Vision-Language Pre-training (CLIP) have shown inspirational performance on 2D visual recognition, which learns to match images with their corresponding texts in open-vocabulary settings. However, it remains under explored that whether CLIP, pre-trained by large-scale image-text pairs in 2D, can be generalized to 3D recognition. In this pap...

---

## 331. GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization

**Authors:** V. Cepeda, Gaurav Kumar Nayak, Mubarak Shah

**Year:** 2023 | **Venue:** Neural Information Processing Systems | **Citations:** 190 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2309.16020) | [DOI](https://doi.org/10.48550/arXiv.2309.16020)

> Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches div...

---

## 332. Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling

**Authors:** Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li

**Year:** 2021 | **Venue:** arXiv.org | **Citations:** 523 | **Score:** 0.000

> Contrastive Vision-Language Pre-training, known as CLIP, has provided a new paradigm for learning visual representations by using large-scale contrastive image-text pairs. It shows impressive performance on zero-shot knowledge transfer to downstream tasks. To further enhance CLIP's few-shot capability, CLIP-Adapter proposed to fine-tune a lightweight residual feature adapter and significantly impr...

---

## 333. Audioclip: Extending Clip to Image, Text and Audio

**Authors:** A. Guzhov, Federico Raue, Jörn Hees, A. Dengel

**Year:** 2021 | **Venue:** IEEE International Conference on Acoustics, Speech, and Signal Processing | **Citations:** 482 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2106.13043) | [DOI](https://doi.org/10.1109/icassp43922.2022.9747631)

> The rapidly evolving field of sound classification has greatly benefited from the methods of other domains. Today, the trend is to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models.We present AudioCLIP – an extension of the CLIP model that handles audio in addition to text and images. Utilizing the AudioSet dataset, our proposed model inco...

---

## 334. CRIS: CLIP-Driven Referring Image Segmentation

**Authors:** Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yan Guo

**Year:** 2021 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 453 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2111.15174) | [DOI](https://doi.org/10.1109/CVPR52688.2022.01139)

> Referring image segmentation aims to segment a referent via a natural linguistic expression. Due to the distinct data properties between text and image, it is challenging for a network to well align text and pixel-level features. Existing approaches use pretrained models to facilitate learning, yet separately transfer the language/vision knowledge from pretrained models, ignoring the multi-modal c...

---

## 335. FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition

**Authors:** Xiaohui Huang, Hao Zhou, Kun Yao, Kai Han

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 48 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.03241)

> In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence ...

---

## 336. CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs

**Authors:** Yassine Ouali, Adrian Bulat, Brais Martínez, Georgios Tzimiropoulos

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 42 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2408.10433)

> Despite recent successes, LVLMs or Large Vision Language Models are prone to hallucinating details like objects and their properties or relations, limiting their real-world deployment. To address this and improve their robustness, we present CLIP-DPO, a preference optimization method that leverages contrastively pre-trained Vision-Language (VL) embedding models, such as CLIP, for DPO-based optimiz...

---

## 337. Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation

**Authors:** Tong Shao, Zhuotao Tian, Hang Zhao, Jingyong Su

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 42 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.08268)

> CLIP, as a vision-language model, has significantly advanced Open-Vocabulary Semantic Segmentation (OVSS) with its zero-shot capabilities. Despite its success, its application to OVSS faces challenges due to its initial image-level alignment training, which affects its performance in tasks requiring detailed local context. Our study delves into the impact of CLIP's [CLS] token on patch feature cor...

---

## 338. Raising the Bar of AI-generated Image Detection with CLIP

**Authors:** D. Cozzolino, G. Poggi, Riccardo Corvi, Matthias Nießner, L. Verdoliva

**Year:** 2023 | **Venue:** 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) | **Citations:** 143 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2312.00195) | [DOI](https://doi.org/10.1109/CVPRW63382.2024.00439)

> The aim of this work is to explore the potential of pre-trained vision-language models (VLMs) for universal detection of AI-generated images. We develop a lightweight detection strategy based on CLIP features and study its performance in a wide variety of challenging scenarios. We find that, contrary to previous beliefs, it is neither necessary nor convenient to use a large domain-specific dataset...

---

## 339. A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)

**Authors:** Weijie Tu, Weijian Deng, Tom Gedeon

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 52 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.07410)

> Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accurac...

---

## 340. Diffusion Feedback Helps CLIP See Better

**Authors:** Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 39 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.20171)

> Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the...

---

## 341. A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation

**Authors:** Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 40 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.04087)

> Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with lim...

---

## 342. LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP

**Authors:** Yunshi Huang, Fereshteh Shakeri, J. Dolz, Malik Boudiaf, H. Bahig

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 41 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2404.02285) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02244)

> In a recent, strongly emergent literature on few-shot CLIP adaptation, Linear Probe (LP) has been often reported as a weak baseline. This has motivated intensive research building convoluted prompt learning or feature adaptation strategies. In this work, we propose and exam-ine from convex-optimization perspectives a generalization of the standard LP baseline, in which the linear classifier weight...

---

## 343. Class-Incremental Learning with CLIP: Adaptive Representation Adjustment and Parameter Fusion

**Authors:** Linlan Huang, Xusheng Cao, Haori Lu, Xialei Liu

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 40 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.14143)

> Class-incremental learning is a challenging problem, where the goal is to train a model that can classify data from an increasing number of classes over time. With the advancement of vision-language pre-trained models such as CLIP, they demonstrate good generalization ability that allows them to excel in class-incremental learning with completely frozen parameters. However, further adaptation to d...

---

## 344. Jina CLIP: Your CLIP Model Is Also Your Text Retriever

**Authors:** Andreas Koukounas, Georgios Mastrapas, Michael Günther, Bo Wang, Scott Martens

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 40 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.20204)

> Contrastive Language-Image Pretraining (CLIP) is widely used to train models to align images and texts in a common embedding space by mapping them to fixed-sized vectors. These models are key to multimodal information retrieval and related tasks. However, CLIP models generally underperform in text-only tasks compared to specialized text models. This creates inefficiencies for information retrieval...

---

## 345. FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs

**Authors:** Haodong Chen, Haojian Huang, Junhao Dong, Mingzhe Zheng, Dian Shao

**Year:** 2024 | **Venue:** ACM Multimedia | **Citations:** 36 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2407.02157) | [DOI](https://doi.org/10.1145/3664647.3680827)

> Dynamic Facial Expression Recognition (DFER) is crucial for understanding human behavior. However, current methods exhibit limited performance mainly due to the insufficient utilization of facial dynamics, and the ambiguity of expression semantics, etc. To this end, we propose a novel framework, named Multi-modal Fine-grained CLIP for DFER with AdaptERs (FineCLIPER), incorporating the following no...

---

## 346. TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives

**Authors:** Maitreya Patel, Abhiram Kusumba, Sheng Cheng, C. Kim, Tejas Gokhale

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 36 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2411.02545)

> Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between text and visual modalities to learn representations. This makes the nature of the training data a significant factor in the efficacy of CLIP for downstream tasks. However, the lack of compositional diversity in contemporary image-text datasets limits the compositional reasoning ability of CLIP. We show tha...

---

## 347. CLIP-Mesh: Generating textured meshes from text using pretrained image-text models

**Authors:** N. Khalid, Tianhao Xie, Eugene Belilovsky, T. Popa

**Year:** 2022 | **Venue:** ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia | **Citations:** 343 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2203.13333) | [DOI](https://doi.org/10.1145/3550469.3555392)

> We present a technique for zero-shot generation of a 3D model using only a target text prompt. Without any 3D supervision our method deforms the control shape of a limit subdivided surface along with its texture map and normal map to obtain a 3D asset that corresponds to the input text prompt and can be easily deployed into games or modeling applications. We rely only on a pre-trained CLIP model t...

---

## 348. Fine-tuned CLIP Models are Efficient Video Learners

**Authors:** H. Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman H. Khan, F. Khan

**Year:** 2022 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 225 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2212.03640) | [DOI](https://doi.org/10.1109/CVPR52729.2023.00633)

> Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since training on a similar scale for videos is infeasible, recent approaches focus on the effective transfer of image-based CLIP to the video domain. In this pursuit, new parametric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts...

---

## 349. CLIP-Driven Semantic Discovery Network for Visible-Infrared Person Re-Identification

**Authors:** Xiaoyan Yu, Neng Dong, Liehuang Zhu, Hao Peng, Dapeng Tao

**Year:** 2024 | **Venue:** IEEE transactions on multimedia | **Citations:** 33 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2401.05806) | [DOI](https://doi.org/10.1109/TMM.2025.3535353)

> Visible-infrared person re-identification (VIReID) primarily deals with matching identities across person images from different modalities. Due to the modality gap between visible and infrared images, cross-modality identity matching poses significant challenges. Recognizing that high-level semantics of pedestrian appearance, such as gender, shape, and clothing style, remain consistent across moda...

---

## 350. Wav2CLIP: Learning Robust Audio Representations from Clip

**Authors:** Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, J. Bello

**Year:** 2021 | **Venue:** IEEE International Conference on Acoustics, Speech, and Signal Processing | **Citations:** 326 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2110.11499) | [DOI](https://doi.org/10.1109/ICASSP43922.2022.9747669)

> We propose Wav2CLIP, a robust audio representation learning method by distilling from Contrastive Language-Image Pre-training (CLIP). We systematically evaluate Wav2CLIP on a variety of audio tasks including classification, retrieval, and generation, and show that Wav2CLIP can outperform several publicly available pre-trained audio representation algorithms. Wav2CLIP projects audio into a shared e...

---

## 351. ViLT-CLIP: Video and Language Tuning CLIP with Multimodal Prompt Learning and Scenario-Guided Optimization

**Authors:** Hao Wang, Fang Liu, Licheng Jiao, Jiahao Wang, Zehua Hao

**Year:** 2024 | **Venue:** AAAI Conference on Artificial Intelligence | **Citations:** 46 | **Score:** 0.000

[PDF](https://ojs.aaai.org/index.php/AAAI/article/download/28347/28680) | [DOI](https://doi.org/10.1609/aaai.v38i6.28347)

> Pre-trained vision-language(V-L) models such as CLIP have demonstrated impressive Zero-Shot performance in many downstream tasks. Since adopting contrastive video-text pairs methods like CLIP to video tasks is limited by its high cost and scale, recent approaches focus on efficiently transferring the image-based CLIP to the video domain. A major finding is that fine-tuning the pre-trained model to...

---

## 352. GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents

**Authors:** Tenglong Ao, Zeyi Zhang, Libin Liu

**Year:** 2023 | **Venue:** ACM Transactions on Graphics | **Citations:** 192 | **Score:** 0.000

[DOI](https://doi.org/10.1145/3592097)

> The automatic generation of stylized co-speech gestures has recently received increasing attention. Previous systems typically allow style control via predefined text labels or example motion clips, which are often not flexible enough to convey user intent accurately. In this work, we present GestureDiffuCLIP, a neural network framework for synthesizing realistic, stylized co-speech gestures with ...

---

## 353. Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero-shot Medical Image Segmentation

**Authors:** Sidra Aleem, Fangyijie Wang, Mayug Maniparambil, Eric Arazo, Julia Dietlmeier

**Year:** 2024 | **Venue:** 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) | **Citations:** 30 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2404.06362) | [DOI](https://doi.org/10.1109/CVPRW63382.2024.00526)

> The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs). SAM, a prompt-driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero-shot recognition capabilities. However, their unified potential has not yet been explored in medical image segmentation. To adapt SAM, to medical imaging, existing methods primaril...

---

## 354. Transfer CLIP for Generalizable Image Denoising

**Authors:** Junting Cheng, Dong Liang, Shan Tan

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 37 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2403.15132) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02454)

> Image denoising is a fundamental task in computer vision. While prevailing deep learning-based supervised and self-supervised methods have excelled in eliminating in-distribution noise, their susceptibility to out-of-distribution (OOD) noise remains a significant challenge. The recent emergence of contrastive language-image pretraining (CLIP) model has showcased exceptional capabilities in open-wo...

---

## 355. CKDH: CLIP-Based Knowledge Distillation Hashing for Cross-Modal Retrieval

**Authors:** Jiaxing Li, W. Wong, Lin Jiang, Xiaozhao Fang, Shengli Xie

**Year:** 2024 | **Venue:** IEEE transactions on circuits and systems for video technology (Print) | **Citations:** 36 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TCSVT.2024.3350695)

> Recently, deep hashing-based cross-modal retrieval has attracted much attention of researchers, due to its advantages of fast retrieval efficiency and low storage overhead, etc. However, the existing deep hashing-based cross-modal retrieval methods typically 1) suffer from inadequately capturing the semantic relevance and coexistent information for cross-modal data, which may result in sub-optimal...

---

## 356. How Much Can CLIP Benefit Vision-and-Language Tasks?

**Authors:** Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach

**Year:** 2021 | **Venue:** International Conference on Learning Representations | **Citations:** 469 | **Score:** 0.000

> Most existing Vision-and-Language (V&L) models rely on pre-trained visual encoders, using a relatively small set of manually-annotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image Pre-training), trained on a massive amou...

---

## 357. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields

**Authors:** Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao

**Year:** 2021 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 443 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2112.05139) | [DOI](https://doi.org/10.1109/CVPR52688.2022.00381)

> We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural radiance fields (NeRF). By leveraging the joint language-image embedding space of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose a unified framework that allows manip-ulating NeRF in a user-friendly way, using either a short text prompt or an exemplar image. Specifically, to combine the novel ...

---

## 358. AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning

**Authors:** Yuwei Tang, Zhenyi Lin, Qilong Wang, Pengfei Zhu, Qinghua Hu

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 24 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2404.08958) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02201)

> Recently, pre-trained vision-language models (e.g., CLIP) have shown great potential in few-shot learning and attracted a lot of research interest. Although efforts have been made to improve few-shot ability of CLIP, key factors on the effectiveness of existing methods have not been well studied, limiting further exploration of CLIP's potential in few-shot learning. In this paper, we first introdu...

---

## 359. MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification

**Authors:** Siddhant Bikram Shah, Shuvam Shiwakoti, Maheep Chaudhary, Haohan Wang

**Year:** 2024 | **Venue:** Conference on Empirical Methods in Natural Language Processing | **Citations:** 25 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2409.14703)

> The complexity of text-embedded images presents a formidable challenge in machine learning given the need for multimodal understanding of multiple aspects of expression conveyed by them. While previous research in multimodal analysis has primarily focused on singular aspects such as hate speech and its subclasses, this study expands this focus to encompass multiple aspects of linguistics: hate, ta...

---

## 360. VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models

**Authors:** Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, Lianwen Jin

**Year:** 2024 | **Venue:** Conference on Empirical Methods in Natural Language Processing | **Citations:** 24 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2410.00741)

> Contrastive Language-Image Pre-training (CLIP) has been widely studied and applied in numerous applications. However, the emphasis on brief summary texts during pre-training prevents CLIP from understanding long descriptions. This issue is particularly acute regarding videos given that videos often contain abundant detailed contents. In this paper, we propose the VideoCLIP-XL (eXtra Length) model,...

---

## 361. Mammo-CLIP: A Vision Language Foundation Model to Enhance Data Efficiency and Robustness in Mammography

**Authors:** Shantanu Ghosh, Clare B. Poynton, Shyam Visweswaran, K. Batmanghelich

**Year:** 2024 | **Venue:** International Conference on Medical Image Computing and Computer-Assisted Intervention | **Citations:** 26 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.12255)

> The lack of large and diverse training data on Computer-Aided Diagnosis (CAD) in breast cancer detection has been one of the concerns that impedes the adoption of the system. Recently, pre-training with large-scale image text datasets via Vision-Language models (VLM) (\eg CLIP) partially addresses the issue of robustness and data efficiency in computer vision (CV). This paper proposes Mammo-CLIP, ...

---

## 362. RET-CLIP: A Retinal Image Foundation Model Pre-trained with Clinical Diagnostic Reports

**Authors:** Jiawei Du, Jia Guo, Weihang Zhang, Shengzhu Yang, Hanruo Liu

**Year:** 2024 | **Venue:** International Conference on Medical Image Computing and Computer-Assisted Intervention | **Citations:** 26 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.14137)

> The Vision-Language Foundation model is increasingly investigated in the fields of computer vision and natural language processing, yet its exploration in ophthalmology and broader medical applications remains limited. The challenge is the lack of labeled data for the training of foundation model. To handle this issue, a CLIP-style retinal image foundation model is developed in this paper. Our fou...

---

## 363. CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching

**Authors:** Xiaoshi Wu, Feng Zhu, Rui Zhao, Hongsheng Li

**Year:** 2023 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 171 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2303.13076) | [DOI](https://doi.org/10.1109/CVPR52729.2023.00679)

> Open-vocabulary detection (OVD) is an object detection task aiming at detecting objects from novel categories beyond the base categories on which the detector is trained. Recent OVD methods rely on large-scale visual-language pre-trained models, such as CLIP, for recognizing novel objects. We identify the two core obstacles that need to be tackled when incorporating these models into detector trai...

---

## 364. ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation

**Authors:** Ziqi Zhou, Bowen Zhang, Yinjie Lei, Lingqiao Liu, Yifan Liu

**Year:** 2022 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 250 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2212.03588) | [DOI](https://doi.org/10.1109/CVPR52729.2023.01075)

> Recently, CLIP has been applied to pixel-level zero-shot learning tasks via a two-stage scheme. The general idea is to first generate class-agnostic region proposals and then feed the cropped proposal regions to CLIP to utilize its image-level zero-shot classification capability. While effective, such a scheme requires two image encoders, one for proposal generation and one for CLIP, leading to a ...

---

## 365. Alpha-CLIP: A CLIP Model Focusing on Wherever you Want

**Authors:** Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang

**Year:** 2023 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 164 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2312.03818) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01237)

> Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. How-ever, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific re...

---

## 366. CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No

**Authors:** Hualiang Wang, Yi Li, Huifeng Yao, X. Li

**Year:** 2023 | **Venue:** IEEE International Conference on Computer Vision | **Citations:** 152 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICCV51070.2023.00173)

> Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ...

---

## 367. VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling

**Authors:** Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 35 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2408.01181)

> VAR is a new generation paradigm that employs 'next-scale prediction' as opposed to 'next-token prediction'. This innovative transformation enables auto-regressive (AR) transformers to rapidly learn visual distributions and achieve robust generalization. However, the original VAR model is constrained to class-conditioned synthesis, relying solely on textual captions for guidance. In this paper, we...

---

## 368. MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-Wise Pruning Error Metric

**Authors:** Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 35 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2403.07839) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02584)

> Vision-language pretrained models have achieved impressive performance on various downstream tasks. However, their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pretrained models and applying magnitude-based pruning on CLIP models leads to in-flexibility and inferior performance. Recent efforts for VLP compression ...

---

## 369. Teaching CLIP to Count to Ten

**Authors:** Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri

**Year:** 2023 | **Venue:** IEEE International Conference on Computer Vision | **Citations:** 163 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICCV51070.2023.00294)

> Large vision-language models (VLMs), such as CLIP, learn rich joint image-text representations, facilitating advances in numerous downstream tasks, including zero-shot classification and text-to-image generation. Nevertheless, existing VLMs exhibit a prominent well-documented limitation – they fail to encapsulate compositional concepts such as counting. We introduce a simple yet effective method t...

---

## 370. Interpreting CLIP's Image Representation via Text-Based Decomposition

**Authors:** Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt

**Year:** 2023 | **Venue:** International Conference on Learning Representations | **Citations:** 151 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2310.05916) | [DOI](https://doi.org/10.48550/arXiv.2310.05916)

> We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representa...

---

## 371. CLIP2Video: Mastering Video-Text Retrieval via Image CLIP

**Authors:** Han Fang, Pengfei Xiong, Luhui Xu, Yu Chen

**Year:** 2021 | **Venue:** arXiv.org | **Citations:** 342 | **Score:** 0.000

> We present CLIP2Video network to transfer the image-language pre-training model to video-text retrieval in an end-to-end manner. Leading approaches in the domain of video-and-language learning try to distill the spatio-temporal video features and multi-modal interaction between videos and languages from a large-scale video-text dataset. Different from them, we leverage pretrained image-language mo...

---

## 372. CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation

**Authors:** Aditya Sanghi, Hang Chu, J. Lambourne, Ye Wang, Chin-Yi Cheng

**Year:** 2021 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 344 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2110.02624) | [DOI](https://doi.org/10.1109/CVPR52688.2022.01805)

> Generating shapes using natural language can enable new ways of imagining and creating the things around us. While significant recent progress has been made in text-to-image generation, text-to-shape generation remains a challenging problem due to the unavailability of paired text and shape data at a large scale. We present a simple yet effective method for zero-shot text-to-shape gener-ation that...

---

## 373. Interpreting the Second-Order Effects of Neurons in CLIP

**Authors:** Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 32 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2406.04341)

> We interpret the function of individual neurons in CLIP by automatically describing them using text. Analyzing the direct effects (i.e. the flow from a neuron through the residual stream to the output) or the indirect effects (overall contribution) fails to capture the neurons' function in CLIP. Therefore, we present the"second-order lens", analyzing the effect flowing from a neuron through the la...

---

## 374. MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection

**Authors:** Ximiao Zhang, Min Xu, Dehui Qiu, Ruixin Yan, Ning Lang

**Year:** 2024 | **Venue:** International Conference on Medical Image Computing and Computer-Assisted Intervention | **Citations:** 32 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.11315)

> In the field of medical decision-making, precise anomaly detection in medical imaging plays a pivotal role in aiding clinicians. However, previous work is reliant on large-scale datasets for training anomaly detection models, which increases the development cost. This paper first focuses on the task of medical image anomaly detection in the few-shot setting, which is critically significant for the...

---

## 375. Transductive Zero-Shot and Few-Shot CLIP

**Authors:** Ségolène Martin, Yunshi Huang, Fereshteh Shakeri, J. Pesquet, Ismail Ben Ayed

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 32 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2405.18437) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02722)

> Transductive inference has been widely investigated in few-shot image classification, but completely overlooked in the recent, fast growing literature on adapting vision-langage models like CLIP. This paper addresses the transductive zero-shot and few-shot CLIP classification challenge, in which inference is performed jointly across a mini-batch of unlabeled query samples, rather than treating eac...

---

## 376. Frozen CLIP Models are Efficient Video Learners

**Authors:** Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo

**Year:** 2022 | **Venue:** European Conference on Computer Vision | **Citations:** 254 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2208.03550) | [DOI](https://doi.org/10.48550/arXiv.2208.03550)

> Video recognition has been dominated by the end-to-end learning paradigm -- first initializing a video recognition model with weights of a pretrained image model and then conducting end-to-end training on videos. This enables the video network to benefit from the pretrained image model. However, this requires substantial computation and memory resources for finetuning on videos and the alternative...

---

## 377. jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images

**Authors:** Andreas Koukounas, Georgios Mastrapas, Bo Wang, Mohammad Kalim Akram, Sedigheh Eslami

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 22 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2412.08802)

> Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual u...

---

## 378. CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?

**Authors:** Ibrahim M. Alabdulmohsin, Xiao Wang, A. Steiner, Priya Goyal, Alexander D'Amour

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 30 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.04547)

> We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and assoc...

---

## 379. Distilling CLIP with Dual Guidance for Learning Discriminative Human Body Shape Representation

**Authors:** Feng Liu, Minchul Kim, Zhiyuan Ren, Xiaoming Liu

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 30 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52733.2024.00032)

> ...

---

## 380. Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation

**Authors:** Bingfeng Zhang, Siyue Yu, Yunchao Wei, Yao Zhao, Jimin Xiao

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 29 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2406.11189) | [DOI](https://doi.org/10.1109/CVPR52733.2024.00364)

> Weakly supervised semantic segmentation has witnessed great achievements with image-level labels. Several recent approaches use the CLIP model to generate pseudo labels for training an individual segmentation model, while there is no attempt to apply the CLIP model as the backbone to directly segment objects with image-level labels. In this paper, we propose WeCLIP, a CLIP-based single-stage pipel...

---

## 381. FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs

**Authors:** Sepehr Dehdashtian, Lan Wang, Vishnu Naresh Boddeti

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 30 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.15593)

> Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious fea...

---

## 382. RWKV-CLIP: A Robust Vision-Language Representation Learner

**Authors:** Tiancheng Gu, Kaicheng Yang, Xiang An, Ziyong Feng, Dongnan Liu

**Year:** 2024 | **Venue:** Conference on Empirical Methods in Natural Language Processing | **Citations:** 29 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2406.06973)

> Contrastive Language-Image Pre-training (CLIP) has significantly improved performance in various vision-language tasks by expanding the dataset with image-text pairs obtained from the web. This paper further explores CLIP from the perspectives of data and model architecture. To mitigate the impact of the noise data and enhance the quality of large-scale image-text data crawled from the internet, w...

---

## 383. Online Zero-Shot Classification with CLIP

**Authors:** Qi Qian, Juhua Hu

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 21 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2408.13320)

> Vision-language pre-training such as CLIP enables zero-shot transfer that can classify images according to the candidate class names. While CLIP demonstrates an impressive zero-shot performance on diverse downstream tasks, the distribution from the target data has not been leveraged sufficiently. In this work, we study a novel online zero-shot transfer scenario, where each image arrives in a rando...

---

## 384. CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks

**Authors:** Yi Li, Hualiang Wang, Yiqun Duan, Xiaomeng Li

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 123 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2304.05653) | [DOI](https://doi.org/10.48550/arXiv.2304.05653)

> ...

---

## 385. SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding

**Authors:** Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar

**Year:** 2023 | **Venue:** 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) | **Citations:** 128 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2310.15308) | [DOI](https://doi.org/10.1109/CVPRW63382.2024.00367)

> The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficient...

---

## 386. Fewer Steps, Better Performance: Efficient Cross-Modal Clip Trimming for Video Moment Retrieval Using Language

**Authors:** Xiang Fang, Daizong Liu, Wanlong Fang, Pan Zhou, Zichuan Xu

**Year:** 2024 | **Venue:** AAAI Conference on Artificial Intelligence | **Citations:** 28 | **Score:** 0.000

[PDF](https://ojs.aaai.org/index.php/AAAI/article/download/27941/27902) | [DOI](https://doi.org/10.1609/aaai.v38i2.27941)

> Given an untrimmed video and a sentence query, video moment retrieval using language (VMR) aims to locate a target query-relevant moment. Since the untrimmed video is overlong, almost all existing VMR methods first sparsely down-sample each untrimmed video into multiple fixed-length video clips and then conduct multi-modal interactions with the query feature and expensive clip features for reasoni...

---

## 387. DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training

**Authors:** Wei Li, Linchao Zhu, Longyin Wen, Yi Yang

**Year:** 2023 | **Venue:** International Conference on Learning Representations | **Citations:** 119 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2303.03032) | [DOI](https://doi.org/10.48550/arXiv.2303.03032)

> Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an e...

---

## 388. MoDE: CLIP Data Experts via Clustering

**Authors:** Jiawei Ma, Po-Yao Huang, Saining Xie, Shang-Wen Li, Luke S. Zettlemoyer

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 25 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2404.16030) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02489)

> The success of contrastive language-image pretraining (CLIP) relies on the supervision from the pairing between images and captions, which tends to be noisy in web- crawled data. We present Mixture of Data Experts (MoDE) and learn a system of CLIP data experts via clustering. Each data expert is trained on one data cluster, being less sensitive to false negative noises in other clusters. At infere...

---

## 389. Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open Domain Generalization

**Authors:** Mainak Singha, Ankit Jha, Shirsha Bose, Ashwin Nair, Moloud Abdar

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 23 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2404.00710) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01264)

> We delve into Open Domain Generalization (ODG), marked by domain and category shifts between training's labeled source and testing's unlabeled target domains. Existing solutions to ODG face limitations due to constrained generalizations of traditional CNN backbones and errors in detecting target open samples in the absence of prior knowledge. Addressing these pitfalls, we introduce ODG-CLIP, harne...

---

## 390. Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding

**Authors:** Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16449v1) | > Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annot...

---

## 391. AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose

**Authors:** Jongmin Yu, Hyeontaek Oh, Zhongtian Sun, Angelica I Aviles-Rivero, Moongu Jeon

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16429v1) | > Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved r...

---

## 392. Safe Multitask Molecular Graph Networks for Vapor Pressure and Odor Threshold Prediction

**Authors:** Shuang Wu, Meijie Wang, Lun Yu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16426v1) | > We investigate two important tasks in odor-related property modeling: Vapor Pressure (VP) and Odor Threshold (OP). To evaluate the model's out-of-distribution (OOD) capability, we adopt the Bemis-Murcko scaffold split. In terms of features, we introduce the rich A20/E17 molecular graph features (20-dimensional atom features + 17-dimensional bond features) and systematically compare GINE and PNA ba...

---

## 393. HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval

**Authors:** Zequn Xie, Xin Liu, Boyun Zhang, Yuxiao Lin, Sihang Cai

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16155v1) | > The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framew...

---

## 394. The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars

**Authors:** Yarin Benyamin

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15914v1) | > In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models priori...

---

## 395. Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model

**Authors:** Chenghao Fan, Wen Heng, Bo Li, Sichen Liu, Yuxuan Song

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15892v2) | > Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training ...

---

## 396. Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework

**Authors:** Shubham Shukla, Kunal Sonalkar

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15711v1) | > Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For e...

---

## 397. DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection

**Authors:** Morteza Poudineh, Marc Lalonde

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15453v1) | > Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak dis...

---

## 398. Rethinking Video Generation Model for the Embodied World

**Authors:** Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15282v1) | > Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progr...

---

## 399. OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation

**Authors:** Letian Zhang, Sucheng Ren, Yanqing Liu, Xianhang Li, Zeyu Wang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15369v1) | > This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to rec...

---

## 400. WavLink: Compact Audio-Text Embeddings with a Global Whisper Token

**Authors:** Gokul Karthik Kumar, Ludovick Lepauloux, Hakim Hacid

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15118v2) | > Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We pres...

---

## 401. Enhancing Few-Shot Out-of-Distribution Detection via the Refinement of Foreground and Background

**Authors:** Tianyu Li, Songyue Cai, Zongqian Wu, Ping Hu, Xiaofeng Zhu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15065v1) | > CLIP-based foreground-background (FG-BG) decomposition methods have demonstrated remarkable effectiveness in improving few-shot out-of-distribution (OOD) detection performance. However, existing approaches still suffer from several limitations. For background regions obtained from decomposition, existing methods adopt a uniform suppression strategy for all patches, overlooking the varying contribu...

---

## 402. Deep Leakage with Generative Flow Matching Denoiser

**Authors:** Isaac Baglin, Xiatian Zhu, Simon Hadfield

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15049v1) | > Federated Learning (FL) has emerged as a powerful paradigm for decentralized model training, yet it remains vulnerable to deep leakage (DL) attacks that reconstruct private client data from shared model updates. While prior DL methods have demonstrated varying levels of success, they often suffer from instability, limited fidelity, or poor robustness under realistic FL settings. We introduce a new...

---

## 403. A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem

**Authors:** Mertcan Daysalilar, Fuat Uyguroglu, Gabriel Nicolosi, Adam Meyers

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.15038v1) | > The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, exis...

---

## 404. Contrastive Knowledge Distillation for Embedding Refinement in Personalized Speech Enhancement

**Authors:** Thomas Serre, Mathieu Fontaine, Éric Benhaim, Slim Essid

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16235v1) | > Personalized speech enhancement (PSE) has shown convincing results when it comes to extracting a known target voice among interfering ones. The corresponding systems usually incorporate a representation of the target voice within the enhancement system, which is extracted from an enrollment clip of the target voice with upstream models. Those models are generally heavy as the speaker embedding's q...

---

## 405. ReinPath: A Multimodal Reinforcement Learning Approach for Pathology

**Authors:** Kangcheng Zhou, Jun Jiang, Qing Zhang, Shuang Zheng, Qingli Li

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.14757v1) | > Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we in...

---

## 406. AdaTIR: Adaptive Tool-Integrated Reasoning via Difficulty-Aware Policy Optimization

**Authors:** Zhaiyu Fang, Ruipeng Sun

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.14696v1) | > Tool-Integrated Reasoning (TIR) has significantly enhanced the capabilities of Large Language Models (LLMs), yet current agents tend to exhibit cognitive offloading, redundantly invoking external tools even for simple tasks. In this paper, we suggest that true agentic intelligence requires not just tool invocation, but the adaptive wisdom to discern when to use them. We propose AdaTIR, a framework...

---

## 407. Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints

**Authors:** Rotem Gatenyo, Ohad Fried

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.14207v1) | > We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optim...

---

## 408. PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning

**Authors:** Jiaying Wu, Can Gao, Jinglu Hu, Hui Li, Xiaofeng Cao

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.14111v1) | > Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic f...

---

## 409. Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model

**Authors:** Haoran Xu, Yanlin Liu, Zizhao Tong, Jiaze Li, Kexue Fu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.14052v1) | > Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text sp...

---

## 410. CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models

**Authors:** Donghee Lee, Rui Cai, Zhe Zhao

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.13622v1) | > Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation ...

---

## 411. DCCVT: Differentiable Clipped Centroidal Voronoi Tessellation

**Authors:** Wylliam Cantin Charawi, Adrien Gruson, Jane Wu, Christian Desrosiers, Diego Thomas

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.13603v1) | > While Marching Cubes (MC) and Marching Tetrahedra (MTet) are widely adopted in 3D reconstruction pipelines due to their simplicity and efficiency, their differentiable variants remain suboptimal for mesh extraction. This often limits the quality of 3D meshes reconstructed from point clouds or images in learning-based frameworks. In contrast, clipped CVTs offer stronger theoretical guarantees and y...

---

## 412. Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation

**Authors:** Mohit Kakda, Mirudula Shri Muthukumaran, Uttapreksha Patel, Lawrence Swaminathan Xavier Prince

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.13440v1) | > Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements f...

---

## 413. Convergence of finite element right-hand-side computation from finite difference data

**Authors:** Stefan Schoder

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.14320v1) | > This work presents two integration methods for field transfer in computational aeroacoustics and in coupled field problems, using the finite element method to solve the acoustic field. Firstly, a high-order Gaussian quadrature computes the finite element right-hand side. In contrast, the (flow) field provided by the finite difference mesh is mapped by higher-order B-Splines or a Lagrangian functio...

---

## 414. Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations

**Authors:** Tim Lachmann, Alexandra Israelsson, Christina Tornberg, Teimuraz Saghinadze, Michal Balazia

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.13225v1) | > Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emot...

---

## 415. CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks

**Authors:** Mingshuang Luo, Ruibing Hou, Bo Chao, Hong Chang, Zimo Liu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.13133v1) | > Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Ad...

---

## 416. Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models

**Authors:** Runxuan Liu, Xianhao Ou, Xinyan Ma, Jiyuan Wang, Jiafeng Liang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12995v1) | > Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based op...

---

## 417. Proxy Robustness in Vision Language Models is Effortlessly Transferable

**Authors:** Xiaowei Fu, Fuxiang Huang, Lei Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12865v1) | > As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibit...

---

## 418. Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data

**Authors:** Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12809v1) | > Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight ...

---

## 419. Open Vocabulary Panoptic Segmentation With Retrieval Augmentation

**Authors:** Nafis Sadeq, Qingfeng Liu, Mostafa El-Khamy

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12779v1) | > Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to...

---

## 420. Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval

**Authors:** Zequn Xie, Boyun Zhang, Yuxiao Lin, Tao Jin

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12768v1) | > Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video sema...

---

## 421. DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition

**Authors:** Hanyu Zhu, Zhihao Zhan, Yuhang Ming, Liang Li, Dibo Hou

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12729v1) | > One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. Howeve...

---

## 422. Adversarial Defense in Vision-Language Models: An Overview

**Authors:** Xiaowei Fu, Lei Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12443v1) | > The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-fr...

---

## 423. GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer

**Authors:** Xinyuan Zhao, Xianrui Chen, Ahmad Chaddad

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12316v1) | > We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Exp...

---

## 424. A Similarity Network for Correlating Musical Structure to Military Strategy

**Authors:** Yiwen Zhang, Hui Zhang, Fanqin Meng

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12314v1) | > Music perception, a multi-sensory process based on the synesthesia effect, is an essential component of music aesthetic education. Understanding music structure helps both perception and aesthetic education. Music structure incorporates a range of information, the coordination of which forms the melody, just as different military actions cooperate to produce a military strategy. However, there are...

---

## 425. Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations

**Authors:** Shizhan Gong, Xiaofan Zhang, Qi Dou

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12303v1) | > Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept ...

---

## 426. CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training

**Authors:** Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12282v1) | > The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized ...

---

## 427. Federated Joint Learning for Domain and Class Generalization

**Authors:** Haoran Xu, Jiaze Li, Jianzhong Ju, Zhenbo Luo

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12253v1) | > Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and \textbf{C...

---

## 428. Offline Policy Learning with Weight Clipping and Heaviside Composite Optimization

**Authors:** Jingren Liu, Hanzhang Qin, Junyi Liu, Mabel C. Chou, Jong-Shi Pang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12117v1) | > Offline policy learning aims to use historical data to learn an optimal personalized decision rule. In the standard estimate-then-optimize framework, reweighting-based methods (e.g., inverse propensity weighting or doubly robust estimators) are widely used to produce unbiased estimates of policy values. However, when the propensity scores of some treatments are small, these reweighting-based metho...

---

## 429. Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification

**Authors:** Xiaomei Yang, Xizhan Gao, Antai Liu, Kang Wei, Fa Zhu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.12062v1) | > The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spat...

---

## 430. Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs

**Authors:** Donghuo Zeng, Hao Niu, Yanan Wang, Masato Taya

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11995v1) | > Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled "train" might also co...

---

## 431. DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset

**Authors:** Yiming Li, Chen Cai, Tianyi Liu, Dan Lin, Wenqian Wang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11990v1) | > In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associate...

---

## 432. Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity

**Authors:** Jun Liu, Leo Yu Zhang, Fengpeng Li, Isao Echizen, Jiantao Zhou

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.14300v1) | > Hard-label black-box settings, where only top-1 predicted labels are observable, pose a fundamentally constrained yet practically important feedback model for understanding model behavior. A central challenge in this regime is whether meaningful gradient information can be recovered from such discrete responses. In this work, we develop a unified theoretical perspective showing that a wide range o...

---

## 433. AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training

**Authors:** Zhiyuan Li, Yuan Wu, Yi Chang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11864v1) | > To stabilize the training of Large Language Models (LLMs), gradient clipping is a nearly ubiquitous heuristic used to alleviate exploding gradients. However, traditional global norm clipping erroneously presupposes gradient homogeneity across different functional modules, leading to an adverse "spill-over" effect where volatile parameters force unnecessary scaling on stable ones. To overcome this,...

---

## 434. CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation

**Authors:** Vanshali Sharma, Andrea Mia Bejar, Gorkem Durak, Ulas Bagci

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11488v1) | > In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clin...

---

## 435. Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding

**Authors:** Wenhui Tan, Ruihua Song, Jiaze Li, Jianzhong Ju, Zhenbo Luo

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11359v1) | > Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which gener...

---

## 436. Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification

**Authors:** Zhiqi Pang, Lingling Zhao, Yang Liu, Chunyu Wang, Gaurav Sharma

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11243v1) | > We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start wi...

---

## 437. UAV-Deployed OAM-BB84 QKD: Turbulence- and Misalignment-Resilient Decoy-State Finite-Key Security with AI-Assisted Calibration

**Authors:** Linxier Deng

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11117v1) | > We present a theoretical framework for quantum key distribution (QKD) using orbital angular momentum (OAM) encoded BB84 on an unmanned aerial vehicle (UAV) platform. A unified channel model captures Kolmogorov turbulence, pointing induced misalignment, and finite aperture clipping, enabling quantitative predictions of inter mode crosstalk and the resulting quantum bit error rate (QBER). Using a we...

---

## 438. MERGETUNE: Continued fine-tuning of vision-language models

**Authors:** Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.10497v2) | > Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, continued fine-tuning (CFT), which seeks to recover pretrained knowledge after a zero-shot model has already been ad...

---

## 439. DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset

**Authors:** Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.10305v1) | > Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. H...

---

## 440. Privacy Enhanced PEFT: Tensor Train Decomposition Improves Privacy Utility Tradeoffs under DP-SGD

**Authors:** Pradip Kunwar, Minh Vu, Maanak Gupta, Manish Bhattarai

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.10045v1) | > Fine-tuning large language models on sensitive data poses significant privacy risks, as membership inference attacks can reveal whether individual records were used during training. While Differential Privacy (DP) provides formal protection, applying DP to conventional Parameter-Efficient Fine-Tuning (PEFT) methods such as Low-Rank Adaptation (LoRA) often incurs substantial utility loss. In this w...

---

## 441. The Spatial Blindspot of Vision-Language Models

**Authors:** Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09954v2) | > Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awaren...

---

## 442. Predicting When to Trust Vision-Language Models for Spatial Reasoning

**Authors:** Muhammad Imran, Yugyung Lee

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.11644v1) | > Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-bas...

---

## 443. Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP

**Authors:** Anant Mehta, Xiyuan Wei, Xingyu Chen, Tianbao Yang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09859v1) | > CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which ...

---

## 444. Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering

**Authors:** Jieying Chen, Jeffrey Hu, Joan Lasenby, Ayush Tewari

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09697v1) | > Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for ca...

---

## 445. LiteEmbed: Adapting CLIP to Rare Classes

**Authors:** Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09661v1) | > Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subs...

---

## 446. OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding

**Authors:** Sheng-Yu Huang, Jaesung Choe, Yu-Chiang Frank Wang, Cheng Sun

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09575v1) | > We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and ...

---

## 447. Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity

**Authors:** Ritabrata Chakraborty, Hrishit Mitra, Shivakumara Palaiahnakote, Umapada Pal

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09497v1) | > Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This...

---

## 448. Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models

**Authors:** Yizhi Chen, Ahmed Hemani

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09451v1) | > We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, using standard INT8 operation, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. ...

---

## 449. Research on Piano Timbre Transformation System Based on Diffusion Model

**Authors:** Chun-Chieh Hsu, Tsai-Ling Hsu, Chen-Chen Yeh, Shao-Chien Lu, Cheng-Han Wu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09333v1) | > We propose a timbre conversion model based on the Diffusion architecture de-signed to precisely translate music played by various instruments into piano ver-sions. The model employs a Pitch Encoder and Loudness Encoder to extract pitch and loudness features of the music, which serve as conditional inputs to the Dif-fusion Model's decoder, generating high-quality piano timbres. Case analysis re-sul...

---

## 450. SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion

**Authors:** Jialu Li, Taiyan Zhou

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09213v1) | > Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-...

---

## 451. SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection

**Authors:** Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.09147v2) | > Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently ...

---

## 452. Integrating APK Image and Text Data for Enhanced Threat Detection: A Multimodal Deep Learning Approach to Android Malware

**Authors:** Md Mashrur Arifin, Maqsudur Rahman, Nasir U. Eisty

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.08959v1) | > As zero-day Android malware attacks grow more sophisticated, recent research highlights the effectiveness of using image-based representations of malware bytecode to detect previously unseen threats. However, existing studies often overlook how image type and resolution affect detection and ignore valuable textual data in Android Application Packages (APKs), such as permissions and metadata, limit...

---

## 453. Motion Attribution for Video Generation

**Authors:** Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.08828v1) | > Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive iso...

---

## 454. S3-CLIP: Video Super Resolution for Person-ReID

**Authors:** Tamas Endrei, Gyorgy Cserey

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.08807v1) | > Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-R...

---

## 455. VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations

**Authors:** Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.08557v1) | > Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pai...

---

## 456. MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP

**Authors:** Aditya Chaudhary, Sneha Barman, Mainak Singha, Ankit Jha, Girish Mishra

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.08420v1) | > In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse sp...

---

## 457. Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms

**Authors:** Nawazish Alia, Rachael Shawb, Karl Mason

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.08052v1) | > Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and dema...

---

## 458. FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures

**Authors:** Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.08026v2) | > Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of divers...

---

## 459. Clipped Affine Policy: Low-Complexity Near-Optimal Online Power Control for Energy Harvesting Communications over Fading Channels

**Authors:** Hao Wu, Shengtian Yang, Huiguo Gao, Diao Wang, Jun Chen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.07622v1) | > This paper investigates online power control for point-to-point energy harvesting communications over wireless fading channels. A linear-policy-based approximation is derived for the relative-value function in the Bellman equation of the power control problem. This approximation leads to two fundamental power control policies: optimistic and robust clipped affine policies, both taking the form of ...

---

## 460. OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image

**Authors:** Tessa Pulli, Jean-Baptiste Weibel, Peter Hönig, Matthias Hirschmanner, Markus Vincze

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.07333v1) | > 6D object pose estimation plays a crucial role in scene understanding for applications such as robotics and augmented reality. To support the needs of ever-changing object sets in such context, modern zero-shot object pose estimators were developed to not require object-specific training but only rely on CAD models. Such models are hard to obtain once deployed, and a continuously changing and grow...

---

## 461. Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models

**Authors:** Yuanyang Yin, Yufan Deng, Shenghai Yuan, Kaipeng Zhang, Xiao Yang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.07287v1) | > The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the...

---

## 462. Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges

**Authors:** Agnivo Gosai, Shuvodeep De, Karun Thankachan, Ramadan A. ZeinEldin, Ali W. Mohamed

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.07235v2) | > This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rott...

---

## 463. CLIMP: Contrastive Language-Image Mamba Pretraining

**Authors:** Nimrod Shabtay, Itamar Zimerman, Eli Schwartz, Raja Giryes

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.06891v1) | > Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential stru...

---

## 464. Federated Continual Learning for Privacy-Preserving Hospital Imaging Classification

**Authors:** Anay Sinhal, Arpana Sinhal, Amit Sinhal

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.06742v1) | > Deep learning models for radiology interpretation increasingly rely on multi-institutional data, yet privacy regulations and distribution shift across hospitals limit central data pooling. Federated learning (FL) allows hospitals to collaboratively train models without sharing raw images, but current FL algorithms typically assume a static data distribution. In practice, hospitals experience conti...

---

## 465. 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence

**Authors:** Hao Tang, Ting Huang, Zeyu Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.06496v1) | > Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the wea...

---

## 466. A High-Speed CGH Calculation Method for Mirror Images on Bézier Surfaces using Optical Path Length Minimization

**Authors:** Kodai Ono, Seok Kang, Yuji Sakamoto

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.06459v1) | > Rendering reflections in curved mirrors is crucial for enhancing the realism in computer-generated hologram (CGH), yet it poses a fundamental challenge due to the unique computational principles of CGH. Conventional methods using Bézier clipping are computationally prohibitive, and a previously proposed mirror surface subdivision method suffered from the computation time increasing with mirror cur...

---

## 467. More Power to the Particles: Analytic Geometry for Partial Optimal Transport-based Fluid simulation

**Authors:** Cyprien Plateau--Holleville, Bruno Lévy

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05765v1) | > We propose an analytic construction of the geometry required for free-surface fluid simulations and deformation mechanics based on partial optimal transport such as the Gallouët-Mérigot's scheme or the Power Particles method. Such methods previously relied on a discretization of the cells by leveraging a classical convex cell clipping algorithm. However, this results in a heavy computational cost ...

---

## 468. FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching

**Authors:** Hongyaoxing Gul, Lijuan Hu, Shuzi Niu, Fangfang Liu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05684v1) | > Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approx...

---

## 469. Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR

**Authors:** Zijun Min, Bingshuai Liu, Ante Wang, Long Zhang, Anxiang Zeng

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05607v1) | > Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assig...

---

## 470. SAPL: Semantic-Agnostic Prompt Learning in CLIP for Weakly Supervised Image Manipulation Localization

**Authors:** Xinghao Wang, Changtao Miao, Dianmo Sheng, Tao Gong, Qi Chu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.06222v1) | > Malicious image manipulation threatens public safety and requires efficient localization methods. Existing approaches depend on costly pixel-level annotations which make training expensive. Existing weakly supervised methods rely only on image-level binary labels and focus on global classification, often overlooking local edge cues that are critical for precise localization. We observe that featur...

---

## 471. SAS-VPReID: A Scale-Adaptive Framework with Shape Priors for Video-based Person Re-Identification at Extreme Far Distances

**Authors:** Qiwei Yang, Pingping Zhang, Yuhao Wang, Zijing Gong

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05535v1) | > Video-based Person Re-IDentification (VPReID) aims to retrieve the same person from videos captured by non-overlapping cameras. At extreme far distances, VPReID is highly challenging due to severe resolution degradation, drastic viewpoint variation and inevitable appearance noise. To address these issues, we propose a Scale-Adaptive framework with Shape Priors for VPReID, named SAS-VPReID. The fra...

---

## 472. Multi-Image Super Resolution Framework for Detection and Analysis of Plant Roots

**Authors:** Shubham Agarwal, Ofek Nourian, Michael Sidorov, Sharon Chemweno, Ofer Hadar

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05482v1) | [DOI](https://doi.org/10.1109/TAFE.2025.3636384)

> Understanding plant root systems is critical for advancing research in soil-plant interactions, nutrient uptake, and overall plant health. However, accurate imaging of roots in subterranean environments remains a persistent challenge due to adverse conditions such as occlusion, varying soil moisture, and inherently low contrast, which limit the effectiveness of conventional vision-based approaches...

---

## 473. Multi-task Cross-modal Learning for Chest X-ray Image Retrieval

**Authors:** Zhaohui Liang, Sivaramakrishnan Rajaraman, Niccolo Marini, Zhiyun Xue, Sameer Antani

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05399v1) | > CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvemen...

---

## 474. ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos

**Authors:** Rustin Soraki, Homanga Bharadhwaj, Ali Farhadi, Roozbeh Mottaghi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05237v1) | > Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF pos...

---

## 475. From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)

**Authors:** Suyash Mishra, Qiang Li, Srikanth Patil, Anubhav Girdhar

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05059v1) | > Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. T...

---

## 476. On the Hidden Objective Biases of Group-based Reinforcement Learning

**Authors:** Aleksandar Fontana, Marco Simoni, Giulio Rossolini, Andrea Saracino, Paolo Mori

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.05002v1) | > Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrog...

---

## 477. ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models

**Authors:** Kaiwen Luo, Liang Lin, Yibo Zhang, Moayad Aloqaily, Dexian Wang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.04876v1) | > Although Audio Large Language Models (ALLMs) have witnessed substantial advancements, their long audio understanding capabilities remain unexplored. A plethora of benchmarks have been proposed for general audio tasks, they predominantly focus on short-form clips, leaving without a consensus on evaluating ALLMs over extended durations. This paper proposes ChronosAudio, the first multi-task benchmar...

---

## 478. Quantile Vector Autoregression without Crossing

**Authors:** Tomohiro Ando, Tadao Hoshino, Ruey Tsay

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.04663v1) | > This paper considers estimation and model selection of quantile vector autoregression (QVAR). Conventional quantile regression often yields undesirable crossing quantile curves, violating the monotonicity of quantiles. To address this issue, we propose a simplex quantile vector autoregression (SQVAR) framework, which transforms the autoregressive (AR) structure of the original QVAR model into a si...

---

## 479. HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment

**Authors:** Wenzhi Chen, Bo Hu, Leida Li, Lihuo He, Wen Lu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.04614v1) | > With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an ad...

---

## 480. PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache

**Authors:** Kunyang Li, Mubarak Shah, Yuzhang Shang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.04359v1) | > A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the domin...

---

## 481. ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography

**Authors:** Vladimir Frants, Sos Agaian, Karen Panetta

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.04159v1) | > Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This...

---

## 482. Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training

**Authors:** Chi Liu, Xin Chen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03895v1) | > Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Cli...

---

## 483. HearSay Benchmark: Do Audio LLMs Leak What They Hear?

**Authors:** Jin Wang, Liang Lin, Kaiwen Luo, Weiliu Wang, Yitian Chen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03783v1) | > While Audio Large Language Models (ALLMs) have achieved remarkable progress in understanding and generation, their potential privacy implications remain largely unexplored. This paper takes the first step to investigate whether ALLMs inadvertently leak user privacy solely through acoustic voiceprints and introduces $\textit{HearSay}$, a comprehensive benchmark constructed from over 22,000 real-wor...

---

## 484. ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization

**Authors:** Shijie Zhang, Kevin Zhang, Zheyuan Gu, Xiang Guo, Rujun Guo

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03723v1) | > Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffe...

---

## 485. e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings

**Authors:** Haonan Chen, Sicheng Gao, Radu Timofte, Tetsuya Sakai, Zhicheng Dou

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03666v2) | > Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones....

---

## 486. VideoMemory: Toward Consistent Video Generation via Memory Integration

**Authors:** Jinsong Zhou, Yihua Du, Xinli Xu, Luozhou Wang, Zijie Zhuang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03655v1) | > Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative plann...

---

## 487. Detecting AI-Generated Images via Distributional Deviations from Real Images

**Authors:** Yakun Niu, Yingjian Chen, Lei Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03586v1) | > The rapid advancement of generative models has significantly enhanced the quality of AI-generated images, raising concerns about misinformation and the erosion of public trust. Detecting AI-generated images has thus become a critical challenge, particularly in terms of generalizing to unseen generative models. Existing methods using frozen pre-trained CLIP models show promise in generalization but...

---

## 488. Provably Convergent Decentralized Optimization over Directed Graphs under Generalized Smoothness

**Authors:** Yanan Bo, Yongqiang Wang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03566v1) | > Decentralized optimization has become a fundamental tool for large-scale learning systems; however, most existing methods rely on the classical Lipschitz smoothness assumption, which is often violated in problems with rapidly varying gradients. Motivated by this limitation, we study decentralized optimization under the generalized $(L_0, L_1)$-smoothness framework, in which the Hessian norm is all...

---

## 489. FIRE-VLM: A Vision-Language-Driven Reinforcement Learning Framework for UAV Wildfire Tracking in a Physics-Grounded Fire Digital Twin

**Authors:** Chris Webb, Mobin Habibpour, Mayamin Hamid Raha, Ali Reza Tavakkoli, Janice Coen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03449v1) | > Wildfire monitoring demands autonomous systems capable of reasoning under extreme visual degradation, rapidly evolving physical dynamics, and scarce real-world training data. Existing UAV navigation approaches rely on simplified simulators and supervised perception pipelines, and lack embodied agents interacting with physically realistic fire environments. We introduce FIRE-VLM, the first end-to-e...

---

## 490. DeepLeak: Privacy Enhancing Hardening of Model Explanations Against Membership Leakage

**Authors:** Firas Ben Hmida, Zain Sbeih, Philemon Hailemariam, Birhanu Eshete

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03429v1) | > Machine learning (ML) explainability is central to algorithmic transparency in high-stakes settings such as predictive diagnostics and loan approval. However, these same domains require rigorous privacy guaranties, creating tension between interpretability and privacy. Although prior work has shown that explanation methods can leak membership information, practitioners still lack systematic guidan...

---

## 491. RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models

**Authors:** Sha Luo, Yogesh Prabhu, Timothy Ossowski, Kaiping Chen, Junjie Hu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03369v2) | > With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the fu...

---

## 492. The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization

**Authors:** Ruixing Zhang, Zihan Liu, Leilei Sun, Tongyu Zhu, Weifeng Lv

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03227v1) | > Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization ...

---

## 493. Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion

**Authors:** Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03213v1) | > Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a gener...

---

## 494. Decentralized Autoregressive Generation

**Authors:** Stepan Maschan, Haoxuan Qu, Jun Liu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03184v2) | > We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of ...

---

## 495. Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning

**Authors:** Yu Luo, Shuo Han, Yihan Hu, Dong Li, Jianye Hao

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03320v1) | > On-policy reinforcement learning (RL), particularly Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), has become the dominant paradigm for fine-tuning large language models (LLMs). While policy ratio clipping stabilizes training, this heuristic hard constraint incurs a fundamental cost: it indiscriminately truncates gradients from high-return yet high-divergence act...

---

## 496. ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios

**Authors:** Yihan Wei, Shenghai Yuan, Tianchen Deng, Boyang Lou, Enwen Hu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.03011v1) | > Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-...

---

## 497. LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments

**Authors:** Sara Micol Ferraina, Michele Brienza, Francesco Argenziano, Emanuele Musumeci, Vincenzo Suriani

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.02905v2) | > Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environment...

---

## 498. Vclip: Face-based Speaker Generation by Face-voice Association Learning

**Authors:** Yao Shi, Yunfei Xu, Hongbin Suo, Yulong Wan, Haifeng Liu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.02753v1) | > This paper discusses the task of face-based speech synthesis, a kind of personalized speech synthesis where the synthesized voices are constrained to perceptually match with a reference face image. Due to the lack of TTS-quality audio-visual corpora, previous approaches suffer from either low synthesis quality or domain mismatch induced by a knowledge transfer scheme. This paper proposes a new app...

---

## 499. ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging

**Authors:** Omer Nacar, Serry Sibaee, Adel Ammar, Yasser Alhabashi, Nadia Samer Sibai

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.02209v1) | > The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Eval...

---

## 500. BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models

**Authors:** Sunny Gupta, Shounak Das, Amit Sethi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.02147v1) | > Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (Bi...

---

## 501. Forget Less by Learning Together through Concept Consolidation

**Authors:** Arjun Ramesh Kaushik, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Nalini Ratha, Venu Govindaraju

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.01963v1) | > Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In ...

---

## 502. Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation

**Authors:** Mingxia Zhan, Li Zhang, Beibei Wang, Yingjie Wang, Zenglin Shi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.01457v3) | > Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text e...

---

## 503. Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration

**Authors:** Wentao Bian, Fenglei Xu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.01456v1) | > In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in "Fuse-then-Refine" paradigms: the "Plasticity-Stability Dilemma." In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes betw...

---

## 504. Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding

**Authors:** Yixuan Lai, He Wang, Kun Zhou, Tianjia Shao

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.01352v1) | > Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and "average" faces when viewp...

---

## 505. Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors

**Authors:** Hao Lu, Xuhui Zhu, Wenjing Zhang, Yanan Li, Xiang Bai

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.01192v1) | > Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can u...

---

## 506. NarrativeTrack: Evaluating Video Language Models Beyond the Frame

**Authors:** Hyeonjeong Ha, Jinjin Ge, Bo Feng, Kaixin Ma, Gargi Chakraborty

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.01095v1) | > Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTra...

---

## 507. Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI

**Authors:** Tatsuaki Tsuruyama

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.01045v2) | > Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work,...

---

## 508. WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift

**Authors:** Julian D. Santamaria, Claudia Isaza, Jhony H. Giraldo

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00993v1) | > Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong per...

---

## 509. FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing

**Authors:** Sunny Gupta, Amit Sethi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00785v1) | > Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE back...

---

## 510. RoboReward: General-Purpose Vision-Language Reward Models for Robotics

**Authors:** Tony Lee, Andrew Wagenmaker, Karl Pertsch, Percy Liang, Sergey Levine

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00675v2) | > A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotics, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we ai...

---

## 511. Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers

**Authors:** Söhnke Benedikt Fischedick, Daniel Seichter, Benedict Stephan, Robin Schmidt, Horst-Michael Gross

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00359v1) | [DOI](https://doi.org/10.1109/IROS60139.2025.11245809)

> In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed p...

---

## 512. HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection

**Authors:** Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00327v1) | > Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD,...

---

## 513. S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding

**Authors:** He Wang, Longteng Guo, Pengkang Huo, Xuanxu Lin, Yichen Yuan

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00264v1) | > Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific pap...

---

## 514. TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models

**Authors:** Kohei Yamamoto, Tomohiro Kikuchi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00260v1) | > While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a l...

---

## 515. Unknown Aware AI-Generated Content Attribution

**Authors:** Ellie Thieu, Jifan Zhang, Haoyue Bai

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00218v1) | > The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and ima...

---

## 516. FANoS: Friction-Adaptive Nosé--Hoover Symplectic Momentum for Stiff Objectives

**Authors:** Nalin Dhiman

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00889v1) | > We study a physics-inspired optimizer, \emph{FANoS} (Friction-Adaptive Nosé--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nosé--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal...

---

## 517. VL-OrdinalFormer: Vision Language Guided Ordinal Transformers for Interpretable Knee Osteoarthritis Grading

**Authors:** Zahid Ullah, Jihie Kim

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00879v1) | > Knee osteoarthritis (KOA) is a leading cause of disability worldwide, and accurate severity assessment using the Kellgren Lawrence (KL) grading system is critical for clinical decision making. However, radiographic distinctions between early disease stages, particularly KL1 and KL2, are subtle and frequently lead to inter-observer variability among radiologists. To address these challenges, we pro...

---

## 518. Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression

**Authors:** Manikanta Kotthapalli, Banafsheh Rekabdar

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.24547v1) | > The exponential growth of video traffic has placed increasing demands on bandwidth and storage infrastructure, particularly for content delivery networks (CDNs) and edge devices. While traditional video codecs like H.264 and HEVC achieve high compression ratios, they are designed primarily for pixel-domain reconstruction and lack native support for machine learning-centric latent representations, ...

---

## 519. Robust reduced rank regression under heavy-tailed noise and missing data via non-convex penalization

**Authors:** The Tien Mai

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.24450v1) | > Reduced rank regression (RRR) is a fundamental tool for modeling multiple responses through low-dimensional latent structures, offering both interpretability and strong predictive performance in high-dimensional settings. Classical RRR methods, however, typically rely on squared loss and Gaussian noise assumptions, rendering them sensitive to heavy-tailed errors, outliers, and data contamination. ...

---

## 520. Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge

**Authors:** Igor Lodin, Sergii Filatov, Vira Filatova, Dmytro Filatov

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.00854v1) | > We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mas...

---

## 521. On Signal Peak Power Constraint of Over-the-Air Federated Learning

**Authors:** Lorenz Bielefeld, Paul Zheng, Oner Hanay, Yao Zhu, Yulin Hu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.23381v1) | > Federated learning (FL) has been considered a promising privacy preserving distributed edge learning framework. Over-the-air computation (AirComp) technique leveraging analog transmission enables the aggregation of local updates directly over-the-air by exploiting the superposition properties of wireless multiple-access channel, thereby drastically reducing the communication bottleneck issues of F...

---

## 522. ISOPO: Proximal policy gradients without pi-old

**Authors:** Nilin Abrahamsen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.23353v2) | > This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes...

---

## 523. Multi Agents Semantic Emotion Aligned Music to Image Generation with Music Derived Captions

**Authors:** Junchang Shi, Gang Li

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.23320v1) | > When people listen to music, they often experience rich visual imagery. We aim to externalize this inner imagery by generating images conditioned on music. We propose MESA MIG, a multi agent semantic and emotion aligned framework that first produces structured music captions and then refines them with cooperating agents specializing in scene, motion, style, color, and composition. In parallel, a V...

---

## 524. Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis

**Authors:** Zijian Liu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.23178v1) | > Optimization under heavy-tailed noise has become popular recently, since it better fits many modern machine learning tasks, as captured by empirical observations. Concretely, instead of a finite second moment on gradient noise, a bounded ${\frak p}$-th moment where ${\frak p}\in(1,2]$ has been recognized to be more realistic (say being upper bounded by $σ_{\frak l}^{\frak p}$ for some $σ_{\frak l}...

---

## 525. Trust Region Masking for Long-Horizon LLM Reinforcement Learning

**Authors:** Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.23075v1) | > Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discont...

---

## 526. Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion

**Authors:** Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.23035v2) | > Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses pri...

---

## 527. CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision

**Authors:** Behnam Raoufi, Hossein Sharify, Mohamad Mahdee Ramezanee, Khosrow Hajsadeghi, Saeed Bagheri Shouraki

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.22969v1) | > Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and alig...

---

## 528. MARPO: A Reflective Policy Optimization for Multi Agent Reinforcement Learning

**Authors:** Cuiling Wu, Yaozhong Gan, Junliang Xing, Ying Fu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.22832v1) | > We propose Multi Agent Reflective Policy Optimization (MARPO) to alleviate the issue of sample inefficiency in multi agent reinforcement learning. MARPO consists of two key components: a reflection mechanism that leverages subsequent trajectories to enhance sample efficiency, and an asymmetric clipping mechanism that is derived from the KL divergence and dynamically adjusts the clipping range to i...

---

## 529. SAM 3D for 3D Object Reconstruction from Remote Sensing Images

**Authors:** Junsheng Yao, Lichao Mou, Qingyu Li

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.22452v1) | > Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on...

---

## 530. OxygenREC: An Instruction-Following Generative Framework for E-commerce Recommendation

**Authors:** Xuegang Hao, Ming Zhang, Alex Li, Xiangyu Qian, Zhi Ma

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.22386v2) | > Traditional recommendation systems suffer from inconsistency in multi-stage optimization objectives. Generative Recommendation (GR) mitigates them through an end-to-end framework; however, existing methods still rely on matching mechanisms based on inductive patterns. Although responsive, they lack the ability to uncover complex user intents that require deductive reasoning based on world knowledg...

---

## 531. VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning

**Authors:** Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.22315v1) | > Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process....

---

## 532. Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models

**Authors:** Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.21860v1) | > Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leve...

---

## 533. FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection

**Authors:** Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.21695v1) | > The fast evolution of generative models has heightened the demand for reliable detection of AI-generated images. To tackle this challenge, we introduce FUSE, a hybrid system that combines spectral features extracted through Fast Fourier Transform with semantic features obtained from the CLIP's Vision encoder. The features are fused into a joint representation and trained progressively in two stage...

---

## 534. DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation

**Authors:** Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.21252v2) | > The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal...

---

## 535. TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation

**Authors:** Gaoren Lin, Huangxuan Zhao, Yuan Xiong, Lefei Zhang, Bo Du

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.21135v1) | > Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issue...

---

## 536. Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation

**Authors:** Zebin Jiang, Tianle Jin, Xiangtong Yao, Alois Knoll, Hu Cao

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.21065v1) | > Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping meth...

---

## 537. LongVideoAgent: Multi-Agent Reasoning with Long Videos

**Authors:** Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.20618v1) | > Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize questio...

---

## 538. Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition

**Authors:** Gorjan Radevski

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.20501v1) | > This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.
  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective de...

---

## 539. CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images

**Authors:** Yujia Fu, Zhiyu Dong, Tianwen Qian, Chenye Zheng, Danian Ji

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.20374v2) | > Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subj...

---

## 540. Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms

**Authors:** Ipek Sena Yilmaz, Onur G. Tuncer, Zeynep E. Aksoy, Zeynep Yağmur Baydemir

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.20323v3) | > Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal priv...

---

## 541. Variable selection in frailty mixture cure models via penalized likelihood estimation

**Authors:** Richard Tawiah, Shu Kay Ng, Geoffrey J. McLachlan

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.19944v1) | > Variable selection naturally arises as a useful subject when faced with data with massive predictor space. In addition to the massive dimensionality, the data may be characterized by intra-subject correlation, and cure fraction, which are ubiquitous in longitudinal studies with recurrent events defining the endpoint of interest. However, variable selection methods simultaneously adjusting for intr...

---

## 542. Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis

**Authors:** Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.19663v1) | > Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowl...

---

## 543. AWPO: Enhancing Tool-Use of Large Language Models through Adaptive Integration of Reasoning Rewards

**Authors:** Zihan Lin, Xiaohan Wang, Hexiong Yang, Jiajun Chai, Jie Cao

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.19126v3) | > While Reinforcement Learning (RL) shows promise in training tool-use Large Language Models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of reasoning rewards based on chain-of-thought quality for better tool utilization. Furthermore, naïvely combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization ...

---

## 544. Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation

**Authors:** Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Mian

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.19088v1) | > Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud,...

---

## 545. WaTeRFlow: Watermark Temporal Robustness via Flow Consistency

**Authors:** Utae Jeong, Sumin In, Hyunju Ryu, Jaewan Choi, Feng Yang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.19048v1) | > Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly ...

---

## 546. Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition

**Authors:** Xiaoyang Li, Mingming Lu, Ruiqi Wang, Hao Li, Zewei Le

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.19036v1) | > Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dyna...

---

## 547. Optimizer Dynamics at the Edge of Stability with Differential Privacy

**Authors:** Ayana Hussain, Ricky Fang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.19019v1) | > Deep learning models can reveal sensitive information about individual training examples, and while differential privacy (DP) provides guarantees restricting such leakage, it also alters optimization dynamics in poorly understood ways. We study the training dynamics of neural networks under DP by comparing Gradient Descent (GD), and Adam to their privacy-preserving variants. Prior work shows that ...

---

## 548. Learning Through Little Eyes: Attribute Discrimination Beyond Objects

**Authors:** Patrick Batsell, Tsutsui Satoshi, Bihan Wen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18951v1) | > Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear w...

---

## 549. LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer

**Authors:** Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18930v1) | > Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferr...

---

## 550. Tight Lower Bounds and Optimal Algorithms for Stochastic Nonconvex Optimization with Heavy-Tailed Noise

**Authors:** Adrien Fradin, Abdurakhmon Sadiev, Laurent Condat, Peter Richtárik

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18713v1) | > We study stochastic nonconvex optimization under heavy-tailed noise. In this setting, the stochastic gradients only have bounded $p$--th central moment ($p$--BCM) for some $p \in (1,2]$. Building on the foundational work of Arjevani et al. (2022) in stochastic optimization, we establish tight sample complexity lower bounds for all first-order methods under \emph{relaxed} mean-squared smoothness ($...

---

## 551. AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference

**Authors:** Longhuan Xu, Feng Yin, Cunjian Chen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18675v1) | > Text-to-image diffusion inference typically follows synchronized schedules, where the numerical integrator advances the latent state to the same timestep at which the denoiser is conditioned. We propose an asynchronous inference mechanism that decouples these two, allowing the denoiser to be conditioned at a different, learned timestep while keeping image update schedule unchanged. A lightweight t...

---

## 552. Enhancing Medical Large Vision-Language Models via Alignment Distillation

**Authors:** Aofei Chang, Ting Wang, Fenglong Ma

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18554v1) | > Medical Large Vision-Language Models (Med-LVLMs) have shown promising results in clinical applications, but often suffer from hallucinated outputs due to misaligned visual understanding. In this work, we identify two fundamental limitations contributing to this issue: insufficient visual representation learning and poor visual attention alignment. To address these problems, we propose MEDALIGN, a ...

---

## 553. Object-Centric Framework for Video Moment Retrieval

**Authors:** Zongyao Li, Yongkang Wong, Satoshi Yamazaki, Jianquan Liu, Mohan Kankanhalli

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18448v1) | > Most existing video moment retrieval methods rely on temporal sequences of frame- or clip-level features that primarily encode global visual and semantic information. However, such representations often fail to capture fine-grained object semantics and appearance, which are crucial for localizing moments described by object-oriented queries involving specific entities and their interactions. In pa...

---

## 554. AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning

**Authors:** Fei Song, Yi Li, Jiangmeng Li, Rui Wang, Changwen Zheng

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18411v1) | > Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt...

---

## 555. Offline Behavioral Data Selection

**Authors:** Shiye Lei, Zhihao Cheng, Dacheng Tao

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18246v1) | > Behavioral cloning is a widely adopted approach for offline policy learning from expert demonstrations. However, the large scale of offline behavioral datasets often results in computationally intensive training when used in downstream tasks. In this paper, we uncover the striking data saturation in offline behavioral data: policy performance rapidly saturates when trained on a small fraction of t...

---

## 556. Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability

**Authors:** Ge Yan, Tuomas Oikarinen, Tsui-Wei, Weng

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.18092v1) | > Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, ...

---

## 557. Map2Video: Street View Imagery Driven AI Video Generation

**Authors:** Hye-Young Jo, Mose Sakashita, Aditi Mishra, Ryo Suzuki, Koichiro Niinuma

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17883v1) | > AI video generation has lowered barriers to video creation, but current tools still struggle with inconsistency. Filmmakers often find that clips fail to match characters and backgrounds, making it difficult to build coherent sequences. A formative study with filmmakers highlighted challenges in shot composition, character motion, and camera control. We present Map2Video, a street view imagery-dri...

---

## 558. AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection

**Authors:** Yichen Jiang, Mohammed Talha Alam, Sohail Ahmed Khan, Duc-Tien Dang-Nguyen, Fakhri Karray

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17730v1) | > Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vis...

---

## 559. MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding

**Authors:** Oskar Kristoffersen, Alba R. Sánchez, Morten R. Hannemose, Anders B. Dahl, Dim P. Papadopoulos

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17492v1) | > Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant mod...

---

## 560. Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing

**Authors:** Xiaosi Gu, Ayaka Sakata, Tomoyuki Obuchi

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17426v1) | > We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equ...

---

## 561. Vision-Language Model Guided Image Restoration

**Authors:** Cuixin Yang, Rongkang Dong, Kin-Man Lam

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17292v1) | > Many image restoration (IR) tasks require both pixel-level fidelity and high-level semantic understanding to recover realistic photos with fine-grained details. However, previous approaches often struggle to effectively leverage both the visual and linguistic knowledge. Recent efforts have attempted to incorporate Vision-language models (VLMs), which excel at aligning visual and textual features, ...

---

## 562. ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching

**Authors:** Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17178v1) | > Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require a...

---

## 563. Can Synthetic Images Serve as Effective and Efficient Class Prototypes?

**Authors:** Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17160v2) | > Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, proce...

---

## 564. The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining

**Authors:** Jasmine Vu, Shivanand Sheshappanavar

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17121v1) | > Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when in...

---

## 565. EasyV2V: A High-quality Instruction-based Video Editing Framework

**Authors:** Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16920v1) | > While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, l...

---

## 566. Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward

**Authors:** Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16912v2) | > This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to t...

---

## 567. Animate Any Character in Any World

**Authors:** Yitong Wang, Fangyun Wei, Hongyang Zhang, Bo Dai, Yan Lu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.17796v1) | > Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, l...

---

## 568. OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction

**Authors:** Yuxin Ray Song, Jinzhou Li, Rao Fu, Devin Murphy, Kaichen Zhou

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16842v1) | > The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocent...

---

## 569. Non-Asymptotic Global Convergence of PPO-Clip

**Authors:** Yin Liu, Qiming Dai, Junyu Zhang, Zaiwen Wen

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16565v1) | > Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more gen...

---

## 570. TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models

**Authors:** Zhiwei Li, Yitian Pang, Weining Wang, Zhenan Sun, Qi Li

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16523v1) | > Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish ...

---

## 571. BrepLLM: Native Boundary Representation Understanding with Large Language Models

**Authors:** Liyuan Deng, Hao Guo, Yunpeng Bai, Yongkang Dai, Huaxi Huang

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16413v1) | > Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a tw...

---

## 572. MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing

**Authors:** Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.20655v1) | > As integrated circuit (IC) dimensions shrink below the lithographic wavelength, optical lithography faces growing challenges from diffraction and process variability. Model-based optical proximity correction (OPC) and inverse lithography technique (ILT) remain indispensable but computationally expensive, requiring repeated simulations that limit scalability. Although deep learning has been applied...

---

## 573. Open Ad-hoc Categorization with Contextualized Feature Learning

**Authors:** Zilin Wang, Sangwoo Mo, Stella X. Yu, Sima Behpour, Liu Ren

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.16202v1) | [DOI](https://doi.org/10.1109/cvpr52734.2025.01407)

> Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through se...

---

## 574. Spatia: Video Generation with Updatable Spatial Memory

**Authors:** Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.15716v1) | > Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spati...

---

## 575. Operator-Theoretic Joint Estimation of Aging-Aware State of Charge and Control-Informed State of Health

**Authors:** Rahmat K. Adesunkanmi, Adel Alaeddini, Mahesh Krishnamurthy

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.15622v1) | > Accurate estimation of a battery's state of charge and state of health is essential for safe and reliable battery management. Existing approaches often decouple these two states, lack stability guarantees, and exhibit limited generalization across operating conditions. This study introduces a unified operator-theoretic framework for aging-aware state of charge and control-informed state of health ...

---

## 576. CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning

**Authors:** Longchen Dai, Zixuan Shen, Zhiheng Zhou, Peipeng Yu, Zhihua Xia

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.15433v1) | > Face recognition systems store face templates for efficient matching. Once leaked, these templates pose a threat: inverting them can yield photorealistic surrogates that compromise privacy and enable impersonation. Although existing research has achieved relatively realistic face template inversion, the reconstructed facial images exhibit over-smoothed facial-part attributes (eyes, nose, mouth) an...

---

## 577. See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball

**Authors:** Arnau Barrera Roy, Albert Clapés Sintes

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.15386v1) | > Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work in...

---

## 578. Emotion Recognition in Signers

**Authors:** Kotaro Funakoshi, Yaoxiong Zhu

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.15376v1) | > Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and B...

---

## 579. SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation

**Authors:** Wangyu Wu, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.15310v1) | > Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and prop...

---

## 580. Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle

**Authors:** Sibi Parivendan, Kashfia Sailunaz, Suresh Neethirajan

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.14998v1) | > Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We pr...

---

## 581. TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation

**Authors:** Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.14938v1) | > We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k h...

---

## 582. Vibe Spaces for Creatively Connecting and Expressing Visual Concepts

**Authors:** Huzheng Yang, Katherine Xu, Andrew Lu, Michael D. Grossberg, Yutong Bai

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.14884v1) | > Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distan...

---

## 583. Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean

**Authors:** Chuan He

**Year:** 2025 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2512.14686v1) | > Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depend...

---

## 584. Learning to Prompt for Vision-Language Models

**Authors:** Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu

**Year:** 2021 | **Venue:** International Journal of Computer Vision | **Citations:** 3350 | **Score:** 0.000

[DOI](https://doi.org/10.1007/s11263-022-01653-1)

> Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task...

---

## 585. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning

**Authors:** Wenliang Dai, Junnan Li, Dongxu Li, A. Tiong, Junqi Zhao

**Year:** 2023 | **Venue:** Neural Information Processing Systems | **Citations:** 2914 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2305.06500) | [DOI](https://doi.org/10.48550/arXiv.2305.06500)

> Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tun...

---

## 586. Conditional Prompt Learning for Vision-Language Models

**Authors:** Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu

**Year:** 2022 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 1873 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2203.05557) | [DOI](https://doi.org/10.1109/CVPR52688.2022.01631)

> With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning—a recent trend in NLP—to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a pr...

---

## 587. Evaluating Object Hallucination in Large Vision-Language Models

**Authors:** Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao

**Year:** 2023 | **Venue:** Conference on Empirical Methods in Natural Language Processing | **Citations:** 1254 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2305.10355) | [DOI](https://doi.org/10.48550/arXiv.2305.10355)

> Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target ...

---

## 588. Are We on the Right Way for Evaluating Large Vision-Language Models?

**Authors:** Lin Chen, Jinsong Li, Xiao-wen Dong, Pan Zhang, Yuhang Zang

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 561 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.20330)

> Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomeno...

---

## 589. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models

**Authors:** Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny

**Year:** 2023 | **Venue:** International Conference on Learning Representations | **Citations:** 2726 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2304.10592) | [DOI](https://doi.org/10.48550/arXiv.2304.10592)

> The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 ...

---

## 590. An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models

**Authors:** Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 333 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.06764)

> In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we i...

---

## 591. VinVL: Revisiting Visual Representations in Vision-Language Models

**Authors:** Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang

**Year:** 2021 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 1039 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2101.00529) | [DOI](https://doi.org/10.1109/CVPR46437.2021.00553)

> This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple pub...

---

## 592. Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding

**Authors:** Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu

**Year:** 2023 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 448 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2311.16922) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01316)

> Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitig...

---

## 593. FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts

**Authors:** Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong

**Year:** 2023 | **Venue:** AAAI Conference on Artificial Intelligence | **Citations:** 281 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2311.05608)

> Large Vision-Language Models (LVLMs) signify a groundbreaking paradigm shift within the Artificial Intelligence (AI) community, extending beyond the capabilities of Large Language Models (LLMs) by assimilating additional modalities (e.g., images).
Despite this advancement, the safety of LVLMs remains adequately underexplored, with a potential overreliance on the safety assurances purported by thei...

---

## 594. Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models

**Authors:** Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, T. Goldstein

**Year:** 2022 | **Venue:** Neural Information Processing Systems | **Citations:** 438 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2209.07511) | [DOI](https://doi.org/10.48550/arXiv.2209.07511)

> Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In thi...

---

## 595. When and why vision-language models behave like bags-of-words, and what to do about it?

**Authors:** Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Y. Zou

**Year:** 2022 | **Venue:** International Conference on Learning Representations | **Citations:** 524 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2210.01936) | [DOI](https://doi.org/10.48550/arXiv.2210.01936)

> Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the ...

---

## 596. SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities

**Authors:** Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 549 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2401.12168) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01370)

> Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size difference. We hypo...

---

## 597. DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding

**Authors:** Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 404 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2412.10302)

> We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we incorporate a dynamic tiling vision encoding strategy designed for processing high-resolution images with different aspect ratios. For the language component, we leverage Deep...

---

## 598. Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models

**Authors:** Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen

**Year:** 2024 | **Venue:** IEEE Transactions on Pattern Analysis and Machine Intelligence | **Citations:** 325 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.18814)

> In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We propose a novel approach to narrow the gap by mining the potential of VLMs for better performance across vari...

---

## 599. What matters when building vision-language models?

**Authors:** Hugo Laurençon, Léo Tronchon, Matthieu Cord, Victor Sanh

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 277 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.02246)

> The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices im...

---

## 600. VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning

**Authors:** Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 169 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2504.08837)

> Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on ...

---

## 601. OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models

**Authors:** Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 548 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2308.01390) | [DOI](https://doi.org/10.48550/arXiv.2308.01390)

> We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperpar...

---

## 602. VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks

**Authors:** Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 101 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2410.05160)

> Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite its importance and pr...

---

## 603. Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models

**Authors:** Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofen Yang

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 85 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2503.13939)

> Vision-language models (VLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored. Medical vision-language tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effec...

---

## 604. SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models

**Authors:** Guiming Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang

**Year:** 2025 | **Venue:** Trans. Mach. Learn. Res. | **Citations:** 140 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2504.11468)

> This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing ``pseudo reasoning paths'' imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hes...

---

## 605. PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding

**Authors:** Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, V. Guizilini

**Year:** 2025 | **Venue:** International Conference on Learning Representations | **Citations:** 70 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2501.16411)

> Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce Phys...

---

## 606. LLaVA-CoT: Let Vision Language Models Reason Step-by-Step

**Authors:** Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 338 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2411.10440)

> Large language models have demonstrated substantial advancements in reasoning capabilities. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-CoT, a large VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought promp...

---

## 607. Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters

**Authors:** Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 177 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2403.11549) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02191)

> Continual learning can empower vision-language models to continuously acquire new knowledge, without the need for access to the entire historical dataset. However, mitigating the performance degradation in large-scale models is non-trivial due to (i) parameter shifts throughout life-long learning and (ii) significant computational burdens associated with full-model tuning. In this work, we present...

---

## 608. DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models

**Authors:** Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu

**Year:** 2024 | **Venue:** Conference on Robot Learning | **Citations:** 337 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.12289)

> A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of reasoning modules for scen...

---

## 609. MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning

**Authors:** Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu

**Year:** 2025 | **Venue:** International Conference on Medical Image Computing and Computer-Assisted Intervention | **Citations:** 109 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2502.19634)

> Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, ...

---

## 610. MoE-LLaVA: Mixture of Experts for Large Vision-Language Models

**Authors:** Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu

**Year:** 2024 | **Venue:** IEEE transactions on multimedia | **Citations:** 269 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2401.15947)

> Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs) effectively improves downstream task performances. However, existing scaling methods enable all model parameters to be active for each token in the calculation, which brings massive training and inferring costs. In this work, we propose a simple yet effective training strategy MoE-Tuning for LVLMs. This strategy innovati...

---

## 611. Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning

**Authors:** Huilin Deng, Ding Zou, Rui Ma, Hongcheng Luo, Yang Cao

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 51 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2503.07065)

> While state-of-the-art vision-language models (VLMs) have demonstrated remarkable capabilities in complex visual-text tasks, their success heavily relies on massive model scaling, limiting their practical deployment. Small-scale VLMs offer a more practical alternative but face significant challenges when trained with traditional supervised fine-tuning (SFT), particularly in two aspects: out-of-dom...

---

## 612. Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning

**Authors:** Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 53 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2503.18013)

> Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mi...

---

## 613. PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction

**Authors:** Long Xing, Qidong Huang, Xiao-wen Dong, Jiajie Lu, Pan Zhang

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 133 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2410.17247)

> In large vision-language models (LVLMs), images serve as inputs that carry a wealth of information. As the idiom"A picture is worth a thousand words"implies, representing a single image in current LVLMs can require hundreds or even thousands of tokens. This results in significant computational costs, which grow quadratically as input image resolution increases, thereby severely impacting the effic...

---

## 614. Efficient Test-Time Adaptation of Vision-Language Models

**Authors:** Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El-Saddik, Eric P. Xing

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 109 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2403.18293) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01343)

> Test-time adaptation with pre-trained vision-language models has attracted increasing attention for tackling distribution shifts during the test time. Though prior studies have achieved very promising performance, they in-volve intensive computation which is severely unaligned with test-time adaptation. We design TDA, a training-free dynamic adapter that enables effective and efficient test-time a...

---

## 615. VisionZip: Longer is Better but Not Necessary in Vision Language Models

**Authors:** Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 107 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52734.2025.01843)

> Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effec...

---

## 616. Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding

**Authors:** Xintong Wang, Jingheng Pan, Liang Ding, Christian Biemann

**Year:** 2024 | **Venue:** Annual Meeting of the Association for Computational Linguistics | **Citations:** 145 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.18715)

> Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Co...

---

## 617. Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models

**Authors:** Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li

**Year:** 2023 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 353 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2310.14566) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01363)

> We introduce “HALLUSIONBENCH11“Hallusion” is a portmanteau of “hallucination” and “illusion.”,” a comprehensive benchmark designed for the evaluation of image-context rea-soning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpre-tation ...

---

## 618. A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges

**Authors:** Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem

**Year:** 2025 | **Venue:** 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) | **Citations:** 56 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPRW67362.2025.00147)

> Multimodal Vision Language Models (VLMs) have emerged as a transformative topic at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP [180], Claude [10], and GPT-4V [228] demonstrate strong reasoning and understanding abilities on visual and textua...

---

## 619. ColPali: Efficient Document Retrieval with Vision Language Models

**Authors:** Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 90 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.01449)

> Documents are visually rich structures that convey information through text, but also figures, page layouts, tables, or even fonts. Since modern retrieval systems mainly rely on the textual information they extract from document pages to index documents -often through lengthy and brittle processes-, they struggle to exploit key visual cues efficiently. This limits their capabilities in many practi...

---

## 620. Vision-Language Models for Vision Tasks: A Survey

**Authors:** Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu

**Year:** 2023 | **Venue:** IEEE Transactions on Pattern Analysis and Machine Intelligence | **Citations:** 1027 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2304.00685) | [DOI](https://doi.org/10.1109/TPAMI.2024.3369699)

> Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlatio...

---

## 621. Vision-Language Models Do Not Understand Negation

**Authors:** Kumail Alhamoud, Shaden S Alshammari, Yonglong Tian, Guohao Li, Philip Torr

**Year:** 2025 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 36 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52734.2025.02757)

> Many practical vision-language applications require models that understand negation, e.g., when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) through large-scale training, their ability to comprehend negation remains underexplored. This study addresses the question: how well do current VLMs understand n...

---

## 622. Words or Vision: Do Vision-Language Models Have Blind Faith in Text?

**Authors:** Ailin Deng, Tri Cao, Zhirui Chen, Bryan Hooi

**Year:** 2025 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 31 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52734.2025.00366)

> Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs’ modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Lan...

---

## 623. ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models

**Authors:** Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu

**Year:** 2024 | **Venue:**  | **Citations:** 183 | **Score:** 0.000

> Large vision-language models (LVLMs) have shown premise in a broad range of vision-language tasks with their strong reasoning and generalization capabilities. However, they require considerable computational resources for training and deployment. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data....

---

## 624. On Evaluating Adversarial Robustness of Large Vision-Language Models

**Authors:** Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li

**Year:** 2023 | **Venue:** Neural Information Processing Systems | **Citations:** 265 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2305.16934) | [DOI](https://doi.org/10.48550/arXiv.2305.16934)

> Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnera...

---

## 625. Debiasing Vision-Language Models via Biased Prompts

**Authors:** Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, A. Torralba, S. Jegelka

**Year:** 2023 | **Venue:** arXiv.org | **Citations:** 150 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2302.00070) | [DOI](https://doi.org/10.48550/arXiv.2302.00070)

> Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach f...

---

## 626. SpatialBot: Precise Spatial Understanding with Vision Language Models

**Authors:** Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang

**Year:** 2024 | **Venue:** IEEE International Conference on Robotics and Automation | **Citations:** 133 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICRA55743.2025.11128671)

> Vision Language Models (VLMs) have achieved impressive performance in 2D image understanding; however, they still struggle with spatial understanding, which is fundamental to embodied AI. In this paper, we propose SpatialBot, a model designed to enhance spatial understanding by utilizing both RGB and depth images. To train VLMs for depth perception, we introduce the SpatialQA and SpatialQA $\bolds...

---

## 627. Building and better understanding vision-language models: insights and future directions

**Authors:** Hugo Laurençon, Andrés Marafioti, Victor Sanh, Léo Tronchon

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 132 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2408.12637)

> The field of vision-language models (VLMs), which take images and texts as inputs and output texts, is rapidly evolving and has yet to reach consensus on several key aspects of the development pipeline, including data, architecture, and training methods. This paper can be seen as a tutorial for building a VLM. We begin by providing a comprehensive overview of the current state-of-the-art approache...

---

## 628. GUICourse: From General Vision Language Models to Versatile GUI Agents

**Authors:** Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang

**Year:** 2024 | **Venue:** Annual Meeting of the Association for Computational Linguistics | **Citations:** 89 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2406.11317)

> Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (th...

---

## 629. Vision Language Models are Biased

**Authors:** An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, A. Nguyen

**Year:** 2025 | **Venue:** arXiv.org | **Citations:** 16 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2505.23941)

> Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that helps them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that sta...

---

## 630. MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI

**Authors:** Kaining Ying, Fanqing Meng, Jin Wang, Zhiqiang Li, Han Lin

**Year:** 2024 | **Venue:** International Conference on Machine Learning | **Citations:** 159 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2404.16006)

> Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to...

---

## 631. Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models

**Authors:** Zangwei Zheng, Mingyu Ma, Kai Wang, Ziheng Qin, Xiangyu Yue

**Year:** 2023 | **Venue:** IEEE International Conference on Computer Vision | **Citations:** 106 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2303.06628) | [DOI](https://doi.org/10.1109/ICCV51070.2023.01752)

> Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model’s zero-shot transfer ability significantly degrades due to catastrophic forgetting. Existing CL methods can mitigate fo...

---

## 632. Unveiling Encoder-Free Vision-Language Models

**Authors:** Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 66 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2406.11832)

> Existing vision-language models (VLMs) mostly rely on vision encoders to extract visual features followed by large language models (LLMs) for visual-language tasks. However, the vision encoders set a strong inductive bias in abstracting visual representation, e.g., resolution, aspect ratio, and semantic priors, which could impede the flexibility and efficiency of the VLMs. Training pure VLMs that ...

---

## 633. Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models

**Authors:** Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng

**Year:** 2024 | **Venue:** Annual Meeting of the Association for Computational Linguistics | **Citations:** 96 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.00231)

> Large vision-language models (LVLMs) excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as geometry shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce Multimodal ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scient...

---

## 634. Negative Label Guided OOD Detection with Pretrained Vision-Language Models

**Authors:** Xue Jiang, Feng Liu, Zhengfeng Fang, Hong Chen, Tongliang Liu

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 59 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.20078)

> Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection ...

---

## 635. RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics

**Authors:** Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 84 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52734.2025.01470)

> Spatial understanding is a crucial capability that enables robots to perceive their surroundings, reason about their environment, and interact with it meaningfully. In modern robotics, these capabilities are increasingly provided by vision-language models. However, these models face significant challenges in spatial reasoning tasks, as their training data are based on general-purpose image dataset...

---

## 636. Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision

**Authors:** Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan

**Year:** 2025 | **Venue:** Information Fusion | **Citations:** 38 | **Score:** 0.000

[DOI](https://doi.org/10.1016/j.inffus.2025.103652)

> ...

---

## 637. Prompt injection attacks on vision language models in oncology

**Authors:** J. Clusmann, Dyke Ferber, I. Wiest, Carolin V. Schneider, T. Brinker

**Year:** 2025 | **Venue:** Nature Communications | **Citations:** 34 | **Score:** 0.000

[PDF](https://doi.org/10.1038/s41467-024-55631-x) | [DOI](https://doi.org/10.1038/s41467-024-55631-x)

> Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be compromised by prompt injection attacks. These can be used...

---

## 638. MMA: Multi-Modal Adapter for Vision-Language Models

**Authors:** Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, Xiaohua Xie

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 68 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52733.2024.02249)

> ...

---

## 639. Enhancing Large Vision Language Models with Self-Training on Image Comprehension

**Authors:** Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 71 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.19716)

> Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the perception capability of the model to understand image inputs for different queries and conduct subsequent reasoning. Improving this capability requires high-quality vision-language data, which is costly and labor-intensive to acquire. Self-training approaches have b...

---

## 640. Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models

**Authors:** Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 59 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2408.02032)

> While Large Vision-Language Models (LVLMs) have rapidly advanced in recent years, the prevalent issue known as the `hallucination' problem has emerged as a significant bottleneck, hindering their real-world deployments. Existing methods mitigate this issue mainly from two perspectives: One approach leverages extra knowledge like robust instruction tuning LVLMs with curated datasets or employing au...

---

## 641. A Survey on Hallucination in Large Vision-Language Models

**Authors:** Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 251 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.00253)

> Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related h...

---

## 642. Vision-language models for medical report generation and visual question answering: a review

**Authors:** Iryna Hartsock, Ghulam Rasool

**Year:** 2024 | **Venue:** Frontiers Artif. Intell. | **Citations:** 166 | **Score:** 0.000

[DOI](https://doi.org/10.3389/frai.2024.1430984)

> Medical vision-language models (VLMs) combine computer vision (CV) and natural language processing (NLP) to analyze visual and textual medical data. Our paper reviews recent advancements in developing VLMs specialized for healthcare, focusing on publicly available models designed for medical report generation and visual question answering (VQA). We provide background on NLP and CV, explaining how ...

---

## 643. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning

**Authors:** Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 134 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.10292)

> Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic...

---

## 644. Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models

**Authors:** Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 96 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52734.2025.00018)

> Today’s most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs t...

---

## 645. IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding

**Authors:** Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye

**Year:** 2024 | **Venue:** 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) | **Citations:** 90 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPRW67362.2025.00150)

> Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) techniqu...

---

## 646. Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models

**Authors:** Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 116 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2406.14852)

> Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning -- a fundamental component of human cognition -- remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understandi...

---

## 647. PromptKD: Unsupervised Prompt Distillation for Vision-Language Models

**Authors:** Zheng Li, Xiang Li, Xinyi Fu, Xing Zhang, Weiqiang Wang

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 84 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2403.02781) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02513)

> Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. In this paper, we introduce an unsupervised domain prompt distillation f...

---

## 648. Manipulate-Anything: Automating Real-World Robots using Vision-Language Models

**Authors:** Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani

**Year:** 2024 | **Venue:** Conference on Robot Learning | **Citations:** 82 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2406.18915)

> Large-scale endeavors like and widespread community efforts such as Open-X-Embodiment have contributed to growing the scale of robot demonstration data. However, there is still an opportunity to improve the quality, quantity, and diversity of robot demonstration data. Although vision-language models have been shown to automatically generate demonstration data, their utility has been limited to env...

---

## 649. Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving

**Authors:** Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 79 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2410.22313)

> End-to-end autonomous driving demonstrates strong planning capabilities with large-scale data but still struggles in complex, rare scenarios due to limited commonsense. In contrast, Large Vision-Language Models (LVLMs) excel in scene understanding and reasoning. The path forward lies in merging the strengths of both approaches. Previous methods using LVLMs to predict trajectories or control signal...

---

## 650. A Survey of Attacks on Large Vision–Language Models: Resources, Advances, and Future Trends

**Authors:** Daizong Liu, Mingyu Yang, Xiaoye Qu, Pan Zhou, Wei Hu

**Year:** 2024 | **Venue:** IEEE Transactions on Neural Networks and Learning Systems | **Citations:** 73 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TNNLS.2025.3592935)

> With the significant development of large models in recent years, large vision–language models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding and reasoning tasks. Compared with traditional large language models (LLMs), LVLMs present great potential and challenges due to their closer proximity to the multiresource real-world applications and the co...

---

## 651. Benchmarking Vision Language Models for Cultural Understanding

**Authors:** Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste

**Year:** 2024 | **Venue:** Conference on Empirical Methods in Natural Language Processing | **Citations:** 80 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.10920)

> Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering ...

---

## 652. Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt

**Authors:** Zonghao Ying, Aishan Liu, Tianyuan Zhang, Zhengmin Yu, Siyuan Liang

**Year:** 2024 | **Venue:** IEEE Transactions on Information Forensics and Security | **Citations:** 77 | **Score:** 0.000

[DOI](https://doi.org/10.1109/TIFS.2025.3583249)

> In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for g...

---

## 653. RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models

**Authors:** Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li

**Year:** 2024 | **Venue:** Conference on Empirical Methods in Natural Language Processing | **Citations:** 80 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.05131)

> The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challen...

---

## 654. MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models

**Authors:** Peng Xia, Peng Xia, Kangyu Zhu, Haoran Li, Haoran Li

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 77 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2410.13085)

> Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retriev...

---

## 655. AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models

**Authors:** Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang

**Year:** 2023 | **Venue:** AAAI Conference on Artificial Intelligence | **Citations:** 238 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2308.15366) | [DOI](https://doi.org/10.48550/arXiv.2308.15366)

> Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have demonstrated the capability of understanding images and achieved remarkable performance in various visual tasks. Despite their strong abilities in recognizing common objects due to extensive training datasets, they lack specific domain knowledge and have a weaker understanding of localized details within objects, which hinders t...

---

## 656. The Neglected Tails in Vision-Language Models

**Authors:** Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 60 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2401.12425) | [DOI](https://doi.org/10.1109/CVPR52733.2024.01234)

> Vision-language models (VLMs) excel in zero-shot recognition but their performance varies greatly across different visual concepts. For example, although CLIP achieves impressive accuracy on ImageNet (60-80%), its performance drops below 10% for more than ten concepts like night snake, presumably due to their limited presence in the pretraining data. However, measuring the frequency of concepts in...

---

## 657. SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models

**Authors:** Yongting Zhang, Luyao Chen, Guodong Zheng, Yifeng Gao, Rui Zheng

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 60 | **Score:** 0.000

[DOI](https://doi.org/10.1109/CVPR52734.2025.01850)

> The emergence of Vision Language Models (VLMs) has brought unprecedented advances in understanding multi-modal information. The combination of textual and visual semantics in VLMs is highly complex and diverse, making the safety alignment of these models challenging. Furthermore, due to the limited study on the safety alignment of VLMs, there is a lack of large-scale, high-quality datasets. To add...

---

## 658. WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences

**Authors:** Yujie Lu, Dongfu Jiang, Wenhu Chen, W. Wang, Yejin Choi

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 57 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2406.11069)

> Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4...

---

## 659. LVLM-EHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models

**Authors:** Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu

**Year:** 2023 | **Venue:** IEEE Transactions on Pattern Analysis and Machine Intelligence | **Citations:** 231 | **Score:** 0.000

[PDF](https://doi.org/10.1109/tpami.2024.3507000) | [DOI](https://doi.org/10.1109/TPAMI.2024.3507000)

> Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building an LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of 13 representative LVLMs such as InstructBL...

---

## 660. CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection

**Authors:** Sohail Ahmed Khan, Duc-Tien Dang-Nguyen

**Year:** 2024 | **Venue:** International Conference on Multimedia Retrieval | **Citations:** 65 | **Score:** 0.000

[PDF](https://dl.acm.org/doi/pdf/10.1145/3652583.3658035) | [DOI](https://doi.org/10.1145/3652583.3658035)

> The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of p...

---

## 661. Calibrated Self-Rewarding Vision Language Models

**Authors:** Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 65 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.14622)

> Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. T...

---

## 662. Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images

**Authors:** Kuofeng Gao, Yang Bai, Jindong Gu, Shu-Tao Xia, Philip H. S. Torr

**Year:** 2024 | **Venue:** International Conference on Learning Representations | **Citations:** 66 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2401.11170)

> Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this p...

---

## 663. Examining Gender and Racial Bias in Large Vision–Language Models Using a Novel Dataset of Parallel Images

**Authors:** Kathleen C. Fraser, S. Kiritchenko

**Year:** 2024 | **Venue:** Conference of the European Chapter of the Association for Computational Linguistics | **Citations:** 64 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.05779)

> Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision–language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the percei...

---

## 664. Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement

**Authors:** Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou

**Year:** 2024 | **Venue:** North American Chapter of the Association for Computational Linguistics | **Citations:** 64 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2405.15973)

> Large vision-language models (LVLMs) have achieved impressive results in visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there remains significant room for improvement in aligning visual and language modalities. Existing methods often depend on external models or data, leading to uncontrollable and unstable alignment results. In this p...

---

## 665. NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples

**Authors:** Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 62 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2410.14669)

> Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to g...

---

## 666. Automated Evaluation of Large Vision-Language Models on Self-Driving Corner Cases

**Authors:** Yanze Li, Wenhua Zhang, Kai Chen, Yanxin Liu, Pengxiang Li

**Year:** 2024 | **Venue:** IEEE Workshop/Winter Conference on Applications of Computer Vision | **Citations:** 62 | **Score:** 0.000

[DOI](https://doi.org/10.1109/WACV61041.2025.00759)

> Large Vision-Language Models (LVLMs) have received widespread attentions for advancing the interpretable self-driving. Existing evaluations of LVLMs primarily focus on multi-faceted capabilities in natural circumstances, lacking automated and quantifiable assessment for self-driving, let alone the severe road corner cases. In this work, we propose CODA-LM, the very first benchmark for the automati...

---

## 667. Vision language models are blind

**Authors:** Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, A. Nguyen

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 91 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.06581)

> While large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini 1.5 Pro, score high on many vision-understanding benchmarks, they are still struggling with low-level vision tasks that are easy to humans. Specifically, on BlindTest, our suite of 7 very simple tasks, including identifying (a) whether two circles overlap; (b) how many times two lines intersect; (c) which letter i...

---

## 668. Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions

**Authors:** Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, Aman Chadha

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 68 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2404.07214)

> The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced...

---

## 669. CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models

**Authors:** Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou

**Year:** 2024 | **Venue:** Neural Information Processing Systems | **Citations:** 69 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2406.06007)

> Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehen...

---

## 670. SuS-X: Training-Free Name-Only Transfer of Vision-Language Models

**Authors:** Vishaal Udandarao, Ankush Gupta, Samuel Albanie

**Year:** 2022 | **Venue:** IEEE International Conference on Computer Vision | **Citations:** 143 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2211.16198) | [DOI](https://doi.org/10.1109/ICCV51070.2023.00257)

> Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval performance on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable....

---

## 671. Unsupervised Prompt Learning for Vision-Language Models

**Authors:** Hao Huang, Jack Chu, Fangyun Wei

**Year:** 2022 | **Venue:** arXiv.org | **Citations:** 157 | **Score:** 0.000

[PDF](http://arxiv.org/pdf/2204.03649) | [DOI](https://doi.org/10.48550/arXiv.2204.03649)

> Contrastive vision-language models like CLIP have shown great progress in transfer learning. In the inference stage, the proper text description, also known as prompt, needs to be carefully designed to correctly classify the given images. In order to avoid laborious prompt engineering, recent works such as CoOp, CLIP-Adapter and Tip-Adapter propose to adapt vision-language models for downstream im...

---

## 672. BRAVE: Broadening the visual encoding of vision-language models

**Authors:** Ouguzhan Fatih Kar, A. Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 58 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2404.07204)

> Vision-language models (VLMs) are typically composed of a vision encoder, e.g. CLIP, and a language model (LM) that interprets the encoded features to solve downstream tasks. Despite remarkable progress, VLMs are subject to several shortcomings due to the limited capabilities of vision encoders, e.g."blindness"to certain image features, visual hallucination, etc. To address these issues, we study ...

---

## 673. JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models

**Authors:** Haibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen

**Year:** 2024 | **Venue:** arXiv.org | **Citations:** 60 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.01599)

> The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignm...

---

## 674. Detecting and Preventing Hallucinations in Large Vision Language Models

**Authors:** Anisha Gunjal, Jihan Yin, Erhan Bas

**Year:** 2023 | **Venue:** AAAI Conference on Artificial Intelligence | **Citations:** 256 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2308.06394) | [DOI](https://doi.org/10.48550/arXiv.2308.06394)

> Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 ...

---

## 675. Guiding Long-Horizon Task and Motion Planning with Vision Language Models

**Authors:** Zhutian Yang, Caelan Reed Garrett, Dieter Fox, Tom'as Lozano-P'erez, L. Kaelbling

**Year:** 2024 | **Venue:** IEEE International Conference on Robotics and Automation | **Citations:** 55 | **Score:** 0.000

[DOI](https://doi.org/10.1109/ICRA55743.2025.11128705)

> Vision-Language Models (VLM) can generate plausible high-level plans when prompted with a goal, the context, an image of the scene, and any planning constraints. However, there is no guarantee that the predicted actions are geometrically and kinematically feasible for a particular robot embodiment. As a result, many prerequisite steps such as opening drawers to access objects are often omitted in ...

---

## 676. Analyzing and Mitigating Object Hallucination in Large Vision-Language Models

**Authors:** Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng

**Year:** 2023 | **Venue:** International Conference on Learning Representations | **Citations:** 266 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2310.00754) | [DOI](https://doi.org/10.48550/arXiv.2310.00754)

> Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To addre...

---

## 677. NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models

**Authors:** Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, Qi Wu

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 75 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2407.12366)

> Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-...

---

## 678. Visual In-Context Learning for Large Vision-Language Models

**Authors:** Yucheng Zhou, Xiang Li, Qianning Wang, Jianbing Shen

**Year:** 2024 | **Venue:** Annual Meeting of the Association for Computational Linguistics | **Citations:** 115 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2402.11574)

> In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our appr...

---

## 679. What's "up" with vision-language models? Investigating their struggle with spatial reasoning

**Authors:** Amita Kamath, Jack Hessel, Kai-Wei Chang

**Year:** 2023 | **Venue:** Conference on Empirical Methods in Natural Language Processing | **Citations:** 207 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2310.19785)

> Recent vision-language (VL) models are powerful, but can they reliably distinguish"right"from"left"? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their...

---

## 680. Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models

**Authors:** Yabin Zhang, Wen-Qing Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou

**Year:** 2024 | **Venue:** Computer Vision and Pattern Recognition | **Citations:** 51 | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2403.17589) | [DOI](https://doi.org/10.1109/CVPR52733.2024.02713)

> With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in re-cent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tai-l...

---

## 681. Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training

**Authors:** David Wan, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal

**Year:** 2024 | **Venue:** European Conference on Computer Vision | **Citations:** 50 | **Score:** 0.000

[DOI](https://doi.org/10.48550/arXiv.2403.02325)

> Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a"visual prompt", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual ...

---

## 682. AnyView: Synthesizing Any Novel View in Dynamic Scenes

**Authors:** Basile Van Hoorick, Dian Chen, Shun Iwase, Pavel Tokmakov, Muhammad Zubair Irshad

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16982v1) | > Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data ...

---

## 683. A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs

**Authors:** Dayal Singh Kalra, Jean-Christophe Gagnon-Audet, Andrey Gromov, Ishita Mediratta, Kelvin Niu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16979v1) | > Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($λ_{\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, di...

---

## 684. Black Carbon scavenging in liquid Arctic clouds: the role of size and mixing state

**Authors:** Barbara Bertozzi, Robin L. Modini, Radovan Krejci, Gabriel Pereira Freitas, Rosaria E. Pileci

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16978v1) | > Black carbon (BC) contributes to Arctic warming by absorbing sunlight and darkening snow. Its atmospheric lifetime critically determines its concentration and climate impact, yet the processes controlling its removal remain poorly constrained in the Arctic. From 18 months of single-particle measurements at the Zeppelin Observatory (Svalbard), we analysed 37 liquid cloud events (~200 hours) to inve...

---

## 685. Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection

**Authors:** Estela Sánchez-Carballo, Francisco M. Melgarejo-Meseguer, José Luis Rojo-Álvarez

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16976v1) | > Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques...

---

## 686. VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents

**Authors:** Zirui Wang, Junyi Zhang, Jiaxin Ge, Long Lian, Letian Fu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16973v1) | > Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over di...

---

## 687. Auto-Regressive Masked Diffusion Models

**Authors:** Mahdi Karami, Ali Ghodsi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16971v1) | > Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel ...

---

## 688. Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians

**Authors:** Bernes Lorier Atabonfack, Ahmed Tahiru Issah, Mohammed Hardi Abdul Baaki, Clemence Ingabire, Tolulope Olusuyi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16967v1) | > In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delaye...

---

## 689. AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems

**Authors:** Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16964v1) | > The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark data...

---

## 690. Calibrating redshift distributions at $z>2$ with Lyman-$α$ forest cross-correlations

**Authors:** Qianjun Hang, Laura Casas, William d'Assignies, Wynne Turner, Andreu Font-Ribera

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16962v1) | > We explore the feasibility of using Lyman-$α$ (Ly$α$) forests to calibrate the ensemble redshift distribution of the high-redshift tail ($2<z<3$) of photometric galaxies. We use \texttt{CoLoRe} simulations to create mock DESI 5-year Ly$α$ forests and Rubin Observatory LSST 10-year photometric galaxies up to $z=3$, and measure the galaxy redshift distribution via their angular cross-correlations. D...

---

## 691. Engineering discrete local dynamics in globally driven dual-species atom arrays

**Authors:** Francesco Cesa, Andrea Di Fini, David Aram Korbany, Roberto Tricarico, Hannes Bernien

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16961v1) | > We introduce a method for engineering discrete local dynamics in globally-driven dual-species neutral atom experiments, allowing us to study emergent digital models through uniform analog controls. Leveraging the new opportunities offered by dual-species systems, such as species-alternated driving, our construction exploits simple Floquet protocols on static atom arrangements, and benefits of gene...

---

## 692. Probabilistic Graphical Models in Astronomy

**Authors:** Abigail Sheerin, Giuseppe Vinci

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16959v1) | > The field of astronomy is experiencing a data explosion driven by significant advances in observational instrumentation, and classical methods often fall short of addressing the complexity of modern astronomical datasets. Probabilistic graphical models offer powerful tools for uncovering the dependence structures and data-generating processes underlying a wide array of cosmic variables. By represe...

---

## 693. Thermodynamically consistent large-eddy simulation models

**Authors:** Thomas Dubos

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16957v1) | > Filtered budgets for anelastic turbulence and a general expression of the turbulent sensible heat flux are derived for a multicomponent fluid with an arbitrary equation of state. A family of subgrid-scale closures is then found under the constraint of consistency with (i) the first and second laws of thermodynamics and (ii) invariance with respect to irrelevant thermodynamic constants. A similar f...

---

## 694. DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers

**Authors:** Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16956v1) | > The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resum...

---

## 695. 3D Molecule Generation from Rigid Motifs via SE(3) Flows

**Authors:** Roman Poletukhin, Marcel Kollovieh, Eike Eberhard, Stephan Günnemann

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16955v1) | > Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation,...

---

## 696. Experimental investigation of nonclassicality in the simplest scenario via the degrees of freedom of light

**Authors:** João M. M. Gama, Guilherme T. C. Cruz, Massy Khoshbin, Lorenzo Catani, José A. O. Huguenin

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16952v1) | > In this work, we experimentally investigate the classical-light emulation of different notions of nonclassicality in the simplest scenario. We implement this prepare-and-measure scenario involving four preparations and two binary-outcome measurements using two distinct experimental setups that exploit different degrees of freedom of light: polarization and first-order Hermite-Gaussian transverse m...

---

## 697. Boundary critical phenomena in the quantum Ashkin-Teller model

**Authors:** Yifan Liu, Natalia Chepiga, Yoshiki Fukusumi, Masaki Oshikawa

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16951v1) | > We investigate the boundary critical phenomena of the one-dimensional quantum Ashkin-Teller model using boundary conformal field theory and density matrix renormalization group (DMRG) simulations. Based on the $\mathbb{Z}_2$-orbifold of the $c=1$ compactified boson boundary conformal field theory, we construct microscopic lattice boundary terms that renormalize to the stable conformal boundary con...

---

## 698. Evaluating Wi-Fi Performance for VR Streaming: A Study on Realistic HEVC Video Traffic

**Authors:** Ferran Maura, Francesc Wilhelmi, Boris Bellalta

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16950v1) | > Cloud-based Virtual Reality (VR) streaming presents significant challenges for 802.11 networks due to its high throughput and low latency requirements. When multiple VR users share a Wi-Fi network, the resulting uplink and downlink traffic can quickly saturate the channel. This paper investigates the capacity of 802.11 networks for supporting realistic VR streaming workloads across varying frame r...

---

## 699. Simulating Electron Dynamics with GPU-Accelerated Real-Time Tamm-Dancoff Approximation

**Authors:** Thomas Knoll, Benjamin G. Levine

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16949v1) | > Time-dependent electronic structure methods provide an efficient, accurate, and robust alternative to traditional time dependent methods for computing both linear and non-linear optical properties. With this in mind, we have developed the real-time Tamm-Dancoff approximation (RT-TDA). This is an approach to model electron dynamics by propagating the linear-response time-dependent density functiona...

---

## 700. Strategies for Span Labeling with Large Language Models

**Authors:** Danil Semin, Ondřej Dušek, Zdeněk Kasner

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16946v1) | > Large language models (LLMs) are increasingly used for text analysis tasks, such as named entity recognition or error detection. Unlike encoder-based models, however, generative architectures lack an explicit mechanism to refer to specific parts of their input. This leads to a variety of ad-hoc prompting strategies for span labeling, often with inconsistent results. In this paper, we categorize th...

---

## 701. A new class of colored Gaussian graphical models with explicit normalizing constants

**Authors:** Adam Chojecki, Piotr Graczyk, Hideyuki Ishi, Bartosz Kołodziejek

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16945v1) | > We study Bayesian model selection in colored Gaussian graphical models (CGGMs), which combine sparsity of conditional independencies with symmetry constraints encoded by vertex- and edge-colored graphs. A computational bottleneck in Bayesian inference for CGGMs is the evaluation of Diaconis-Ylvisaker normalizing constants, given by gamma-type integrals over cones of precision matrices with prescri...

---

## 702. Forecast on the generalised dark matter properties from a Euclid-like survey

**Authors:** Ziad Sakr, Jessica N. López-Sánchez

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16943v1) | > The Stage~IV \textit{Euclid} mission will deliver spectroscopic galaxy redshifts together with photometric positions and shapes, enabling cosmological analyses through spectroscopic galaxy clustering (GCsp), photometric galaxy clustering (GCph), weak-lensing cosmic shear (WL), and their cross-correlation (XC). In this work we forecast the constraining power of a Euclid-like survey on the Generalis...

---

## 703. Quantum Fisher information analysis for absorption measurements with undetected photons

**Authors:** Martin Houde, Franz Roeder, Christine Silberhorn, Benjamin Brecht, Nicolás Quesada

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16941v1) | > We theoretically compare the quantum Fisher information (QFI) for three configurations of absorption spectroscopy with undetected idler photons: an SU(1,1) interferometer with inter-source idler loss, an induced-coherence (IC) setup in which the idler partially seeds a second squeezer together with a vacuum ancilla, and a distributed-loss (DL) scheme with in-medium attenuation. We calculate the QF...

---

## 704. Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles

**Authors:** Anton Zamyatin, Patrick Indri, Sagar Malhotra, Thomas Gärtner

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16936v1) | > In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only unde...

---

## 705. Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias

**Authors:** Elias Schuhmacher, Andrianos Michail, Juri Opitz, Rico Sennrich, Simon Clematide

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16934v1) | > To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segme...

---

## 706. Reward-Forcing: Autoregressive Video Generation with Reward Feedback

**Authors:** Jingran Zhang, Ning Li, Yuanhao Ban, Andrew Bai, Justin Cui

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16933v1) | > While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically l...

---

## 707. Identifying heat-related diagnoses in emergency department visits among adults in Chicago: a heat-wide association study

**Authors:** Hyojung Jang, Peter M. Graffy, Benjamin W. Barrett, Daniel E. Horton, Jennifer L. Chan

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16932v1) | > Extreme heat is an escalating public health concern. Although prior studies have examined heat-health associations, their reliance on restricted diagnoses and diagnostic categories misses or misclassifies heat-related illness. We conducted a heat-wide association study to identify acute-care diagnoses associated with extreme heat in Chicago, Illinois. Using 916,904 acute-care visits -- including e...

---

## 708. Nishpaksh: TEC Standard-Compliant Framework for Fairness Auditing and Certification of AI Models

**Authors:** Shashank Prakash, Ranjitha Prasad, Avinash Agarwal

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16926v1) | > The growing reliance on Artificial Intelligence (AI) models in high-stakes decision-making systems, particularly within emerging telecom and 6G applications, underscores the urgent need for transparent and standardized fairness assessment frameworks. While global toolkits such as IBM AI Fairness 360 and Microsoft Fairlearn have advanced bias detection, they often lack alignment with region-specifi...

---

## 709. The shape function of the observed growth index

**Authors:** Ziad Sakr, Jinglan Zheng

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16921v1) | > The growth index $γ$ is a powerful trigger for detecting deviations from $Λ$CDM. However, its value is often determined by considering an asymptotic constant value that works for all redshift, or else following a chosen parameterisation. Here we formulate the growth index as function of three quantities that could be directly related to observables in redshift bins, $fσ_8(z_i)$, $f(z_i)$ and $H(z_...

---

## 710. CosmoSlider: An educational tool for cosmology

**Authors:** Andreas Nygaard, Steen Hannestad, Thomas Tram

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16919v1) | > Understanding how cosmological parameters influence the cosmic microwave background (CMB) power spectra is a central component of modern cosmology education, but interactive exploration is often limited by computational cost or technical complexity. We present CosmoSlider, a lightweight visualization tool that enables real-time exploration of CMB power spectra as multiple cosmological parameters a...

---

## 711. Drawing the line between explosion and collapse in electron-capture supernovae -- I. Impact of conductive flame speeds and ignition conditions on the explosion mechanism

**Authors:** Alexander Holas, Samuel W. Jones, Friedrich K. Roepke, Rüdiger Pakmor, Christina Fakiola

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16918v1) | > Electron-capture supernovae (ECSNe) are commonly thought to result in a collapse to a neutron star. Recent work has shown that a thermonuclear explosion is also a possible outcome. The division between the two regimes has not yet been mapped out. In this study, we investigate the conditions under which the transition from thermonuclear explosion to collapse occurs, and what physical mechanisms dri...

---

## 712. IRS Compensation of Hyper-Rayleigh Fading: How Many Elements Are Needed?

**Authors:** Aleksey S. Gvozdarev

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16915v1) | [DOI](https://doi.org/10.1109/LWC.2026.3656740)

> The letter introduces and studies the problem of defining the minimum number of Intelligent Reflecting Surface (IRS) elements needed to compensate for heavy fading conditions in multipath fading channels. The fading severity is quantified in terms of Hyper-Rayleigh Regimes (HRRs) (i.e., full-HRR (worst-case conditions), strong-, weak-, and no-HRR), and the channel model used (Inverse Power Lomax (...

---

## 713. LoL: Longer than Longer, Scaling Video Generation to Hour

**Authors:** Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16914v1) | > Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink f...

---

## 714. Recovering Communities in Structured Random Graphs

**Authors:** Michael Kapralov, Luca Trevisan, Weronika Wrzos-Kaminska

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16910v1) | > The problem of recovering planted community structure in random graphs has received a lot of attention in the literature on the stochastic block model, where the input is a random graph in which edges crossing between different communities appear with smaller probability than edges induced by communities. The communities themselves form a collection of vertex-disjoint sparse cuts in the expected g...

---

## 715. Preventing the Collapse of Peer Review Requires Verification-First AI

**Authors:** Lei You, Lele Cao, Iryna Gurevych

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16909v1) | > This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal...

---

## 716. The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning

**Authors:** Calarina Muslimani, Yunshu Du, Kenta Kawamoto, Kaushik Subramanian, Peter Stone

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16906v1) | > The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajector...

---

## 717. GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints

**Authors:** Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16905v1) | > Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility an...

---

## 718. Clinical Feasibility of Label-Free Digital Staining Using Mid-Infrared Microscopy at Subcellular Resolution

**Authors:** L. Duraffourg, H. Borges, M. Fernandes, M. Beurrier-Bousquet, J. Baraillon

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16904v1) | > We present a rapid, large-field bimodal imaging platform that integrates conventional brightfield microscopy with a lensless IR imaging scanner, enabling whole-slide IR image stack acquisition in minutes. Using a dedicated deep learning model, we implement an optical HE staining strategy based on subcellular morpho-spectral fingerprinting....

---

## 719. Dynamics of AGN feedback in the X-ray bright East and Southwest arms of M87, mapped by XRISM

**Authors:** A. Simionescu, C. Kilbourne, H. R. Russell, D. Ito, M. Charbonneau

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16901v1) | > As the central galaxy in the nearest cluster, M87 provides the best spatial resolution for disentangling the complex interactions between AGN jets and the surrounding environment. We investigate the velocity structure of the multitemperature X-ray gas in M87, particularly in the eastern and southwestern arms associated with past AGN outbursts, using high-resolution spectroscopy from XRISM/Resolve....

---

## 720. Embedding -based Crop Type Classification in the Groundnut Basin of Senegal

**Authors:** Madeline C. Lisaius, Srinivasan Keshav, Andrew Blake, Clement Atzberger

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16900v1) | > Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) t...

---

## 721. A more inclusive effective dark fluid equation of state parameter: constraints from SKA and Euclid like surveys

**Authors:** Ziad Sakr

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16899v1) | > We forecast constraints on an effective dark fluid equation of state parameter $w_{\rm eff}$ that encapsulates modified gravity theories that modifies both the Universe background expansion as well as its large scale structures growth. This is achieved through relating Friedmann equations' dark fluid pressure and density content, thus $w_{\rm eff}$, to modified gravity parameterized models by mean...

---

## 722. Universal relation between dipole polarizability of finite nuclei and neutron-star compactness

**Authors:** P. S. Koliogiannis, T. Ghosh, E. Yuksel, N. Paar

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16894v1) | > The nuclear equation of state, which determines the structure and properties of neutron stars, remains subject to substantial theoretical uncertainties, leading to model dependence in predicted observables. Universal relations have emerged as a powerful tool to mitigate this dependence by linking neutron star observables in a framework-independent manner. In this work, we introduce a new universal...

---

## 723. MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion

**Authors:** Chi Yu, Hongyu Yuan, Zhiyi Duan

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16886v1) | > Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferre...

---

## 724. GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss

**Authors:** Yangfan Xu, Lilian Zhang, Xiaofeng He, Pengdong Wu, Wenqi Wu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16885v1) | > Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unl...

---

## 725. Multigrade Neural Network Approximation

**Authors:** Shijun Zhang, Zuowei Shen, Yuesheng Xu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16884v1) | > We study multigrade deep learning (MGDL) as a principled framework for structured error refinement in deep neural networks. While the approximation power of neural networks is now relatively well understood, training very deep architectures remains challenging due to highly non-convex and often ill-conditioned optimization landscapes. In contrast, for relatively shallow networks, most notably one-...

---

## 726. Universal classical and quantum fluctuations in the large deviations of current of noisy quantum systems: The case of QSSEP and QSSIP

**Authors:** Mathias Albert, Denis Bernard, Tony Jin, Stefano Scopa, Shiyi Wei

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16883v1) | > We study the fluctuation statistics of integrated currents in noisy quantum diffusive systems, focusing on the Quantum Symmetric Simple Exclusion and Inclusion Processes (QSSEP/QSSIP). These one-dimensional fermionic (QSSEP) and bosonic (QSSIP) models feature stochastic nearest-neighbor hopping driven by Brownian noise, together with boundary injection and removal processes. They provide solvable ...

---

## 727. Performance of Differential Protection Applied to Collector Cables of Offshore Wind Farms with MMC-HVDC Transmission

**Authors:** Moisés J. B. B. Davi, Felipe V. Lopes, Vinícius A. Lacerda, Mário Oleskovicz, Oriol Gomis-Bellmunt

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16876v1) | > The ongoing global transition towards low-carbon energy has propelled the integration of offshore wind farms, which, when combined with Modular Multilevel Converter-based High-Voltage Direct Current (MMC-HVDC) transmission, present unique challenges for power system protection. In collector cables connecting wind turbines to offshore MMC, both ends are supplied by Inverter-Based Resources (IBRs), ...

---

## 728. No Validation, No Problem: Predicting Model Performance from a Single Gradient

**Authors:** Fangzheng Wu, Brian Summa

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16874v1) | > We propose a validation-free checkpointing signal from a single forward-backward pass: the Frobenius norm of the classifier-head gradient on one detached-feature batch, ||g||_F = ||dL/dW||_F. Across ImageNet-1k CNNs and Transformers, this proxy is strongly negative with Top-1 and positive with loss. Selecting the checkpoint with the minimum head gradient in a short tail window closes most of the g...

---

## 729. Provably Learning Attention with Queries

**Authors:** Satwik Bhattamishra, Kulin Shah, Michael Hahn, Varun Kanade

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16873v1) | > We study the problem of learning Transformer-based sequence models with black-box access to their outputs. In this setting, a learner may adaptively query the oracle with any sequence of vectors and observe the corresponding real-valued output. We begin with the simplest case, a single-head softmax-attention regressor. We show that for a model with width $d$, there is an elementary algorithm to le...

---

## 730. From Atom to Community: Structured and Evolving Agent Memory for User Behavior Modeling

**Authors:** Yuxin Liao, Le Wu, Min Hou, Yu Wang, Han Wu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16872v1) | > User behavior modeling lies at the heart of personalized applications like recommender systems. With LLM-based agents, user preference representation has evolved from latent embeddings to semantic memory. While existing memory mechanisms show promise in textual dialogues, modeling non-textual behaviors remains challenging, as preferences must be inferred from implicit signals like clicks without g...

---

## 731. On the stability of solutions to non-Newtonian Navier--Stokes--Fourier-like systems in the supercritical case

**Authors:** Anna Abbatiello, Miroslav Bulíček, Petr Kaplický

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16868v1) | > We consider a three-dimensional domain occupied by a homogeneous, incompressible, non-Newtonian, heat-conducting fluid with prescribed nonuniform temperature on the boundary and no-slip boundary conditions for the velocity. No external body forces are assumed. The constitutive relation for the Cauchy stress tensor is assumed in a general form that includes, in particular, the power-law and Ladyzhe...

---

## 732. Distributional Instruments: Identification and Estimation with Quantile Least Squares

**Authors:** Rowan Cherodian, Guy Tchuente

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16865v1) | > We study instrumental-variable designs where policy reforms strongly shift the distribution of an endogenous variable but only weakly move its mean. We formalize this by introducing distributional relevance: instruments may be purely distributional. Within a triangular model, distributional relevance suffices for nonparametric identification of average structural effects via a control function. We...

---

## 733. Interaction Induced Magnetotransport in a 2D Dirac-Heavy Hole Hybrid Band System

**Authors:** G. M. Gusev, A. D. Levin, V. A. Chitta, Z. D. Kvon, N. N. Mikhailov

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16864v1) | [DOI](https://doi.org/10.1038/s41598-025-32085-9)

> While electron-electron (e-e) interactions are known to influence resistivity in non-Galilean invariant two-dimensional (2D) systems, their effect on magnetotransport is not fully understood. Conventional models for simple bands often predict a vanishing magnetoresistivity from e-e interactions alone. In this work, we investigate magnetotransport in a gapless 6.3 nm HgTe quantum well, a hybrid 2D ...

---

## 734. Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation

**Authors:** Tims Pecerskis, Aivars Smirnovs

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16863v1) | [DOI](https://doi.org/10.5281/zenodo.18234923)

> This paper introduces the N-Way Self-Evaluating Deliberation (NSED) protocol, a Runtime Mixture-of-Models (MoM) architecture that constructs emergent composite models from a plurality of distinct expert agents. Unlike traditional Mixture-of-Experts (MoE) which rely on static gating networks, NSED employs a Dynamic Expertise Broker - a runtime optimization engine that treats model selection as a va...

---

## 735. Optical Tag-Based Neuronavigation and Augmentation System for Non-Invasive Brain Stimulation

**Authors:** Xuyi Hu, Ke Ma, Siwei Liu, Per Ola Kristensson, Stefan Goetz

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16862v1) | > Accurate neuronavigation is critical for effective transcranial magnetic stimulation (TMS), as stimulation outcomes depend directly on precise coil placement. Existing neuronavigation systems are often costly, complex, and prone to tracking errors. To address these limitations, we present a computer vision based neuronavigation system that enables real time tracking of the patient and TMS instrume...

---

## 736. Hidden Zeros in Massive Theories

**Authors:** Mariana Carrillo González, Freddie Ward

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16860v1) | > We investigate whether the hidden zeros and associated factorisations found for massless colour-ordered amplitudes persist under massive deformations. Using the kinematic mesh construction, we show that hidden zeros survive only for symmetry controlled mass generation. For massive $\text{Tr} Φ^3$ with a uniform mass, the zeros and their factorisation patterns are inherited after a massive shift of...

---

## 737. Statistical mechanics of a 2D material in a gas reservoir

**Authors:** Moon-ki Choi, Ellad B Tadmor

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16856v1) | > We derive and validate a partition function for low-dimensional systems interacting with a heat bath, addressing the general issue of thermodynamic modeling of nanoscale systems. In contrast to bulk systems in the canonical (NVT) ensemble where the partition function is solely determined by the Hamiltonian of the system and the temperature of the heat bath, our formulation demonstrates that accoun...

---

## 738. Stochastic Analysis of Fifth-Order KdV Soliton in Damping Regime and Reduction to Painlevé Second Equation

**Authors:** Irfan Mahmood, Adeena Iqbal, Sohail Mumtaz

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16854v1) | > This work presents a stochastic analysis of fifth-order KdV soliton momentum distribution in a damping regime. An explicit representation of the soliton momentum associated with amplitude variation is derived in terms of a random time function in the presence of dissipation. Statistical interpretations of soliton propagation modes, amplitude fluctuations, and amplification are analyzed within a $δ...

---

## 739. Reasoning Promotes Robustness in Theory of Mind Tasks

**Authors:** Ian B. de Haan, Peter van der Putten, Max van Duijn

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16853v1) | > Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such ...

---

## 740. Charting the Landscape of Oxygen Ion Conductors: A 60-Year Dataset with Interpretable Regression Models

**Authors:** Seong-Hoon Jang, Shin Kiyohara, Hitoshi Takamura, Yu Kumagai

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16852v1) | > Oxygen ion conductors are indispensable materials for such as solid oxide fuel cells, sensors, and membranes. Despite extensive research across diverse structural families, systematic data enabling comparative analysis remain scarce. Here, we present a curated dataset of oxygen ion conductors compiled from $84$ experimental reports spanning $60$ years, covering $483$ materials. Each record include...

---

## 741. Twisted bilayer graphene from first-principles: structural and electronic properties

**Authors:** Albert Zhu, Daniel Bennett, Daniel T. Larson, Mohammed M. Al Ezzi, Efstratios Manousakis

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16851v1) | > We present a comprehensive first-principles study of twisted bilayer graphene (tBLG) for a wide range of twist angles, with a focus on structural and electronic properties. By employing density functional theory (DFT) with an optimized local basis set, we simulate tBLG, obtaining fully relaxed commensurate structures for twist angles down to 0.987°. For all angles the lattice relaxation agrees wel...

---

## 742. Stochastic Modeling and Resource Dimensioning of Multi-Cellular Edge Intelligent Systems

**Authors:** Jaume Anguera Peris, Joakim Jaldén

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16848v1) | > Edge intelligence enables AI inference at the network edge, co-located with or near the radio access network, rather than in centralized clouds or on mobile devices. It targets low-latency, resource-constrained applications with large data volumes, requiring tight integration of wireless access and on-site computing. Yet system performance and cost-efficiency hinge on joint pre-deployment dimensio...

---

## 743. Hierarchical Distribution Matcher Design for Probabilistic Constellation Shaping Based on a Novel Semi-Analytical Optimization Approach

**Authors:** Pantea Nadimi Goki, Luca Potì

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16847v1) | > A novel design procedure for practical hierarchical distribution matchers (HiDMs) in probabilistically shaped constellation systems is presented. The proposed approach enables the determination of optimal parameters for any target distribution matcher rate. Specifically, lower bounds on energy loss, rate loss, and memory requirements are analytically estimated for HiDM architectures approximating ...

---

## 744. Cosmography with $Λ$-Szekeres Models

**Authors:** Morag Hills, Asta Heinesen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16844v1) | > The cosmological tensions present in the $Λ$ cold dark matter model that have emerged and strengthened over recent years motivate model independent approaches to analysing data. Cosmography is useful for interpreting data in cosmology without imposing assumptions about the field equations of gravity or the matter content in the Universe. Some cosmography methods, denoted covariant cosmography, go ...

---

## 745. The Origins of Planets for ArieL (OPAL) Key Science Project: the end-to-end planet formation campaign for the ESA space mission Ariel

**Authors:** Danae Polychroni, Diego Turrini, Romolo Politi, Sergio Fonte, Eugenio Schisano

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16841v1) | > The growing body of atmospheric observations of exoplanets from space and ground-based facilities showcases how the great diversity of the planetary population is not limited to their physical properties but extends to their compositions. The ESA space mission Ariel will observe and characterise hundreds of exoplanetary atmospheres to explore and understand the roots of this compositional diversit...

---

## 746. Spillovers and Co-movements in Multivariate Volatility: A Vector Multiplicative Error Model

**Authors:** Edoardo Otranto, Luca Scaffidi Domianello

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16837v1) | > Recent developments in financial time series focus on modeling volatility across multiple assets or indices in a multivariate framework, accounting for potential interactions such as spillover effects. Furthermore, the increasing integration of global financial markets provides a similar dynamics (referred to as comovement). In this context, we introduce a novel model for volatility vectors within...

---

## 747. ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models

**Authors:** Chenxi Ruan, Yu Xiao, Yihan Hou, Guosheng Hu, Wei Zeng

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16836v1) | > While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by...

---

## 748. Calibrated Probabilistic Interpolation for GEDI Biomass

**Authors:** Robin Young, Srinivasan Keshav

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16834v1) | > Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of het...

---

## 749. Directional Asymmetry in Edge BasedSpatial Models via a Skew Normal Prior

**Authors:** Danna L. Cruz-Reyes, Renato M. Assunção, Reinaldo B. Arellano-Valle, Rosangela H. Loschi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16829v1) | > We introduce a skewed edge based spatial prior, named RENeGe sk that extends the Gaussian RENeGe framework by incorporating directional asymmetry through a skew normal distribution. Skewness is defined on the edge graph and propagated to the node space, aligning asymmetric behavior with transitions across neighboring regions rather than with marginal node effects. The model is formulated within th...

---

## 750. Identification of Port-Hamiltonian Differential-Algebraic Equations from Input-Output Data

**Authors:** N. Hagelaars, G. J. E. van Otterdijk, S. Moradi, R. Tóth, N. O. Jaensson

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16827v1) | > Many models of physical systems, such as mechanical and electrical networks, exhibit algebraic constraints that arise from subsystem interconnections and underlying physical laws. Such systems are commonly formulated as differential-algebraic equations (DAEs), which describe both the dynamic evolution of system states and the algebraic relations that must hold among them. Within this class, port-H...

---

## 751. Trapped in the past? Disentangling fluid and crystallized intelligence of large language models using chess

**Authors:** Leonard S. Pleiss, Maximilian Schiffer, Robert K. von Weizsäcker

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16823v1) | > Large Language Models (LLMs) exhibit remarkable capabilities, yet it remains unclear to what extent these reflect sophisticated recall (crystallized intelligence) or reasoning ability (fluid intelligence). We introduce chess as a controlled testbed for disentangling these faculties. Leveraging the game's structure and scalable engine evaluations, we construct a taxonomy of positions varying in tra...

---

## 752. Directional-Shift Dirichlet ARMA Models for Compositional Time Series with Structural Break Intervention

**Authors:** Harrison Katz

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16821v1) | > Compositional time series, vectors of proportions summing to unity observed over time, frequently exhibit structural breaks due to external shocks, policy changes, or market disruptions. Standard methods either ignore such breaks or handle them through ad-hoc dummy variables that cannot extrapolate beyond the estimation sample. We develop a Bayesian Dirichlet ARMA model augmented with a directiona...

---

## 753. Existence of spot and lane stationary solutions for an ant active matter PDE model

**Authors:** Matthias Rakotomalala, Oscar de Wit

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16820v1) | > This paper studies the existence of multiple non-trivial stationary solutions of a partial differential equation (PDE) model introduced in [3], motivated by collective ant behavior. Previous work suggested the presence of two types of non-trivial stationary solutions for this PDE system: spot and lane solutions. In this paper, we establish the existence of these families of solutions along a bifur...

---

## 754. Flux-ratio anomalies in cusp quasars reveal dark matter beyond CDM

**Authors:** Siyuan Hou, Shucheng Xiang, Yue-Lin Sming Tsai, Daneng Yang, Yiping Shu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16818v1) | > Strongly lensed quasars in cusp configurations provide a uniquely sensitive probe of small-scale dark matter structure. Using the largest microlensing-free flux ratios for 17 quadruply imaged cusps, we combine these with extensive Monte Carlo simulations of mock lens realizations under cold dark matter (CDM), self-interacting dark matter (SIDM), and fuzzy dark matter (FDM) scenarios. Building on t...

---

## 755. PI2I: A Personalized Item-Based Collaborative Filtering Retrieval Framework

**Authors:** Shaoqing Wang, Yingcai Ma, Kairui Fu, Ziyang Wang, Dunxian Huang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16815v1) | > Efficiently selecting relevant content from vast candidate pools is a critical challenge in modern recommender systems. Traditional methods, such as item-to-item collaborative filtering (CF) and two-tower models, often fall short in capturing the complex user-item interactions due to uniform truncation strategies and overdue user-item crossing. To address these limitations, we propose Personalized...

---

## 756. The Scattering Algebra of Physical Space: Squared Massive Constructive Amplitudes

**Authors:** Moab Croft, Neil Christensen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16814v1) | [DOI](https://doi.org/10.1007/s00006-025-01435-1)

> The Algebra of Physical Space (APS) is used to explore the Constructive Standard Model (CSM) of particle physics. Namely, this paper connects the spinor formalism of the APS to massive amplitudes in the CSM. A novel equivalency between traditional CSM and APS-CSM formalisms is introduced, called the Scattering Algebra (SA), with example calculations confirming the consistency of results between bo...

---

## 757. A Fully Automated DM-BIM-BEM Pipeline Enabling Graph-Based Intelligence, Interoperability, and Performance-Driven Early Design

**Authors:** Jun Xiao, Qiong Wang, Yihui Li, Zhexuan Yu, Hao Zhou

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16813v1) | > Artificial intelligence in construction increasingly depends on structured representations such as Building Information Models and knowledge graphs, yet early-stage building designs are predominantly created as flexible boundary-representation (B-rep) models that lack explicit spatial, semantic, and performance structure. This paper presents a robust, fully automated framework that transforms unst...

---

## 758. Incorporating Eye-Tracking Signals Into Multimodal Deep Visual Models For Predicting User Aesthetic Experience In Residential Interiors

**Authors:** Chen-Ying Chien, Po-Chih Kuo

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16811v1) | > Understanding how people perceive and evaluate interior spaces is essential for designing environments that promote well-being. However, predicting aesthetic experiences remains difficult due to the subjective nature of perception and the complexity of visual responses. This study introduces a dual-branch CNN-LSTM framework that fuses visual features with eye-tracking signals to predict aesthetic ...

---

## 759. Zoology of Altermagnetic-type Non-collinear Magnets on the Maple Leaf Lattice

**Authors:** Pratyay Ghosh, Ronny Thomale

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16807v1) | > We define unconventional non-collinear magnetic ground states on the maple leaf lattice (MLL) distinguished by the selective breaking or preservation of time reversal ($\mathcal{T}$) and parity ($\mathcal{P}$). Depending on the nature of $\mathcal{P}\mathcal{T}$-breaking, linear spin-wave theory reveals momentum-dependent non-relativistic magnon spin splitting at different high symmetry points in ...

---

## 760. An Efficient Insect-inspired Approach for Visual Point-goal Navigation

**Authors:** Lu Yihe, Barbara Webb

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16806v1) | > In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths aro...

---

## 761. SoS: Analysis of Surface over Semantics in Multilingual Text-To-Image Generation

**Authors:** Carolin Holtermann, Florian Schneider, Anne Lauscher

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16803v1) | > Text-to-image (T2I) models are increasingly employed by users worldwide. However, prior research has pointed to the high sensitivity of T2I towards particular input languages - when faced with languages other than English (i.e., different surface forms of the same prompt), T2I models often produce culturally stereotypical depictions, prioritizing the surface over the prompt's semantics. Yet a comp...

---

## 762. Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis

**Authors:** Gaurav Negi, MA Waskow, Paul Buitelaar

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16800v1) | > Fine-grained opinion analysis of text provides a detailed understanding of expressed sentiments, including the addressed entity. Although this level of detail is sound, it requires considerable human effort and substantial cost to annotate opinions in datasets for training models, especially across diverse domains and real-world applications. We explore the feasibility of LLMs as automatic annotat...

---

## 763. Ultrafast Dipolar Electrostatic Modeling of Plasmonic Nanoparticles with Arbitrary Geometry

**Authors:** Paulo S. S. dos Santos, João P. Mendes, José M. M M. de Almeida, Luís C. C. Coelho

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16797v1) | > Accurate and fast calculations of localized surface plasmon resonances (LSPR) in metallic nanoparticles is essential for applications in sensing, nano-optics, and energy harvesting. Although full-wave numerical techniques such as the boundary element method (BEM) or the discrete dipole approximation (DDA) provide high accuracy, their computational cost often hinders rapid parametric studies. Here ...

---

## 764. Building a Robust Risk-Based Access Control System to Combat Ransomware's Capability to Encrypt: A Machine Learning Approach

**Authors:** Kenan Begovic, Abdulaziz Al-Ali, Qutaibah Malluhi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16795v1) | > Ransomware core capability, unauthorized encryption, demands controls that identify and block malicious cryptographic activity without disrupting legitimate use. We present a probabilistic, risk-based access control architecture that couples machine learning inference with mandatory access control to regulate encryption on Linux in real time. The system builds a specialized dataset from the native...

---

## 765. Improved Kelbg Potentials for $Z>1$ and Application to Carbon Plasmas

**Authors:** Heather D. Whitley, Michael S. Murillo, John I. Castor, Liam G. Stanton, Lorin X. Benedict

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16794v1) | > In this work, we present a general form for the electron-ion diffractive potential derived from the quantum pair density matrix and fit to the improved Kelbg potential for atomic numbers up to $Z = 54$. We apply classical molecular dynamics using the improved Kelbg potential for carbon with various forms of the Pauli potential to compute internal energies and pressures for hot, dense plasma condit...

---

## 766. A Novel Transfer Learning Approach for Mental Stability Classification from Voice Signal

**Authors:** Rafiul Islam, Md. Taimur Ahad

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16793v1) | > This study presents a novel transfer learning approach and data augmentation technique for mental stability classification using human voice signals and addresses the challenges associated with limited data availability. Convolutional neural networks (CNNs) have been employed to analyse spectrogram images generated from voice recordings. Three CNN architectures, VGG16, InceptionV3, and DenseNet121...

---

## 767. A Dynamic Parametric Simulator for Fetal Heart Sounds

**Authors:** Yingtong Zhou, Yiang Zhou, Zhengxian Qu, Kang Liu, Ting Tan

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16792v1) | > Research on fetal phonocardiogram (fPCG) is challenged by the limited number of abdominal recordings, substantial maternal interference, and marked transmissioninduced signal attenuation that complicate reproducible benchmarking. We present a reproducible dynamic parametric simulator that generates long abdominal fPCG sequences by combining cycle-level fetal S1/S2 event synthesis with a convolutio...

---

## 768. Mercury-Ar$χ$es: a high-performance n-body code for planet formation studies

**Authors:** Diego Turrini, Sergio Fonte, Romolo Politi, Danae Polychroni, Scigé J. Liu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16791v1) | > Forming planetary systems are populated by large numbers of gravitationally interacting planetary bodies, spanning from massive giant planets to small planetesimals akin to present-day asteroids and comets. All these planetary bodies are embedded in the gaseous embrace of their native protoplanetary disks, and their interactions with the disk gas play a central role in shaping their dynamical evol...

---

## 769. [HP99] 159 -- Properties of the first Supersoft X-ray Source with a Helium star donor

**Authors:** Hélène Szegedi, Philip A. Charles, David A. H. Buckley, Pieter J. Meintjes, Przemek Mróz

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16789v1) | > [HP99] 159 is remarkable as the first supersoft X-ray source (SSS) identified with an evolved helium star donor. With a likely orbital period of 1.164 d or 2.327 d, the origin of the SSS component is controversial, with the two current models being either steady He-burning on the white dwarf surface, or that it is a helium nova in the decaying phase. To help resolve this issue we present extensive...

---

## 770. Multisymplectic AKSZ sigma models

**Authors:** Thomas Basile, Maxim Grigoriev, Evgeny Skvortsov

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16785v1) | > The Alexandrov-Kontsevich-Schwarz-Zaboronsky (AKSZ) construction encodes all the data of a topological sigma-model in the finite-dimensional symplectic $Q$-manifold. Relaxing the nondegeneracy condition i.e. considering a presymplectic form instead, extends the construction to non-topological models. The gauge-invariant action functional of (presymplectic) AKSZ sigma model is written in terms of s...

---

## 771. Spectral embedding of inhomogeneous Poisson processes on multiplex networks

**Authors:** Joshua Corneck, Edward A. K. Cohen, Francesco Sanna Passino

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16784v1) | > In many real-world networks, data on the edges evolve in continuous time, naturally motivating representations based on point processes. Heterogeneity in edge types further gives rise to multiplex network point processes. In this work, we propose a model for multiplex network data observed in continuous-time. We establish two-to-infinity norm consistency and asymptotic normality for spectral-embed...

---

## 772. SLD: Segmentation-Based Landmark Detection for Spinal Ligaments

**Authors:** Lara Blomenkamp, Ivanna Kramer, Sabine Bauer, Theresa Schöche

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16782v1) | > In biomechanical modeling, the representation of ligament attachments is crucial for a realistic simulation of the forces acting between the vertebrae. These forces are typically modeled as vectors connecting ligament landmarks on adjacent vertebrae, making precise identification of these landmarks a key requirement for constructing reliable spine models. Existing automated detection methods are e...

---

## 773. Persuasion Tokens for Editing Factual Knowledge in LLMs

**Authors:** Paul Youssef, Jörg Schlötterer, Christin Seifert

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16781v1) | > In-context knowledge editing (IKE) is a promising technique for updating Large Language Models (LLMs) with new information. However, IKE relies on lengthy, fact-specific demonstrations which are costly to create and consume significant context window space. In this paper, we introduce persuasion tokens (P-Tokens) -- special tokens trained to replicate the effect of IKE demonstrations, enabling eff...

---

## 774. PocketDVDNet: Realtime Video Denoising for Real Camera Noise

**Authors:** Crispian Morris, Imogen Dexter, Fan Zhang, David R. Bull, Nantheera Anantrasirichai

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16780v1) | > Live video denoising under realistic, multi-component sensor noise remains challenging for applications such as autofocus, autonomous driving, and surveillance. We propose PocketDVDNet, a lightweight video denoiser developed using our model compression framework that combines sparsity-guided structured pruning, a physics-informed noise model, and knowledge distillation to achieve high-quality rest...

---

## 775. GTA: Generative Traffic Agents for Simulating Realistic Mobility Behavior

**Authors:** Simon Lämmer, Mark Colley, Patrick Ebel

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16778v1) | [DOI](https://doi.org/10.1145/3772318.3790772)

> People's transportation choices reflect complex trade-offs shaped by personal preferences, social norms, and technology acceptance. Predicting such behavior at scale is a critical challenge with major implications for urban planning and sustainable transport. Traditional methods use handcrafted assumptions and costly data collection, making them impractical for early-stage evaluations of new techn...

---

## 776. Z2 Lattice Gauge Theory on Non-trivial Topology and Its Quantum Simulation

**Authors:** Jiaqi Hu, Shu Tian, Xiaopeng Cui, Rebing Wu, Man-Hong Yung

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16776v1) | > Wegner duality is essential for Z2 lattice gauge theory, yet the duality on non-trivial topologies has remained implicit. We extend Wegner duality to arbitrary topology and dimension, obtaining a new class of Ising models, in which topology is encoded in non-local domain-wall patterns. Without the overhead of gauge constraints, simulating this model on an L*L torus requires only L*L qubits with tw...

---

## 777. LLM-powered Real-time Patent Citation Recommendation for Financial Technologies

**Authors:** Tianang Deng, Yu Deng, Tianchen Gao, Yonghong Hu, Rui Pan

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16775v1) | > Rapid financial innovation has been accompanied by a sharp increase in patenting activity, making timely and comprehensive prior-art discovery more difficult. This problem is especially evident in financial technologies, where innovations develop quickly, patent collections grow continuously, and citation recommendation systems must be updated as new applications arrive. Existing patent retrieval ...

---

## 778. E2E-AEC: Implementing an end-to-end neural network learning approach for acoustic echo cancellation

**Authors:** Yiheng Jiang, Biao Tian, Haoxu Wang, Shengkui Zhao, Bin Ma

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16774v1) | > We propose a novel neural network-based end-to-end acoustic echo cancellation (E2E-AEC) method capable of streaming inference, which operates effectively without reliance on traditional linear AEC (LAEC) techniques and time delay estimation. Our approach includes several key strategies: First, we introduce and refine progressive learning to gradually enhance echo suppression. Second, our model emp...

---

## 779. CASP: Few-Shot Class-Incremental Learning with CLS Token Attention Steering Prompts

**Authors:** Shuai Huang, Xuhan Lin, Yuwu Lu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16773v1) | > Few-shot class-incremental learning (FSCIL) presents a core challenge in continual learning, requiring models to rapidly adapt to new classes with very limited samples while mitigating catastrophic forgetting. Recent prompt-based methods, which integrate pretrained backbones with task-specific prompts, have made notable progress. However, under extreme few-shot incremental settings, the model's ab...

---

## 780. Improved measurements of the age of JWST galaxies at z=6-10

**Authors:** M. Lopez-Corredoira, C. M. Gutierrez

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16772v1) | > From James Webb Space Telescope (JWST) surveys, 31 galaxies with average redshift 7.3 are selected containing large Balmer break, Lyman-$α$ break (V-shaped SED versus $λ$). Apart from Hubble Space Telescope (HST) and JWST-NIRCam (Near-infrared camera) photometry for these galaxies, there are JWST-NIRSpec (Near-infrared spectrograph) spectra for 13 galaxies and mid-infrared photometry (mostly JWST-...

---

## 781. AutoRegressive Generation with B-rep Holistic Token Sequence Representation

**Authors:** Jiahao Li, Yunpeng Bai, Yongkang Dai, Hao Guo, Hongping Gan

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16771v1) | > Previous representation and generation approaches for the B-rep relied on graph-based representations that disentangle geometric and topological features through decoupled computational pipelines, thereby precluding the application of sequence-based generative frameworks, such as transformer architectures that have demonstrated remarkable performance. In this paper, we propose BrepARG, the first a...

---

## 782. From Noisy News Sentiment Scores to Interpretable Temporal Dynamics: A Bayesian State-Space Model

**Authors:** Ian Carbó Casals

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16769v1) | > Text-based sentiment indicators are widely used to monitor public and market mood, but weekly sentiment series are noisy by construction. A main reason is that the amount of relevant news changes over time and across categories. As a result, some weekly averages are based on many articles, while others rely on only a few. Existing approaches do not explicitly account for changes in data availabili...

---

## 783. Do LLM hallucination detectors suffer from low-resource effect?

**Authors:** Debtanu Datta, Mohan Kishore Chilukuri, Yash Kumar, Saptarshi Ghosh, Muhammad Bilal Zafar

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16766v1) | > LLMs, while outperforming humans in a wide range of tasks, can still fail in unanticipated ways. We focus on two pervasive failure modes: (i) hallucinations, where models produce incorrect information about the world, and (ii) the low-resource effect, where the models show impressive performance in high-resource languages like English but the performance degrades significantly in low-resource lang...

---

## 784. ReLU Networks for Model Predictive Control: Network Complexity and Performance Guarantees

**Authors:** Xingchen Li, Keyou You

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16764v1) | > Recent years have witnessed a resurgence in using ReLU neural networks (NNs) to represent model predictive control (MPC) policies. However, determining the required network complexity to ensure closed-loop performance remains a fundamental open problem. This involves a critical precision-complexity trade-off: undersized networks may fail to capture the MPC policy, while oversized ones may outweigh...

---

## 785. Flow Matching for Probabilistic Monocular 3D Human Pose Estimation

**Authors:** Cuong Le, Pavló Melnyk, Bastian Wandt, Mårten Wadenbäck

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16763v1) | > Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a si...

---

## 786. Thermodynamic geometry in hadron resonance gas model at real and imaginary baryon chemical potential and a simple sufficient condition for quark deconfinement

**Authors:** Riki Oshima, Hiroaki Kouno, Motoi Tachibana, Kouji Kashiwa

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16762v1) | > The thermodynamic geometry of the hadron resonance gas model with (without) excluded volume effects (EVE) of baryons is investigated. The case with imaginary mu, where mu is the baryon chemical potential, is investigated as well as the one with real mu. We calculate the scalar curvature R and use the R=0 criterion to investigate the phase structure in the mu^2-T plane where T is the temperature. T...

---

## 787. Shake-up and shake-off spectra in the electron capture decay of atomic $^7$Be

**Authors:** Mauro Guerra, Inwook Kim, Stephan Friedrich, Pedro Amaro, Adrien Andoche

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16756v1) | > The most stringent laboratory-based experimental limits on the existence of sub-MeV sterile neutrinos are currently set by decay spectroscopy of radioactive $^7$Be embedded into superconducting sensors. The systematic uncertainties are dominated by the modeling of the electron shake-up and shake-off spectra that are not based on state-of-the-art atomic theory and do not include electron correlatio...

---

## 788. Variability-Aware Detection and Repair of Compilation Errors Using Foundation Models in Configurable Systems

**Authors:** Rohit Gheyi, Lucas Albuquerque, Márcio Ribeiro, Eduardo Almeida, Danyllo Albuquerque

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16755v1) | > Modern software systems often rely on conditional compilation to support optional features and multiple deployment scenarios. In configurable systems, compilation errors may arise only under specific combinations of features, remaining hidden during development and testing. Such variability-induced errors are difficult to detect in practice, as traditional compilers analyze only a single configura...

---

## 789. Standardizing Longitudinal Radiology Report Evaluation via Large Language Model Annotation

**Authors:** Xinyi Wang, Grazziela Figueredo, Ruizhe Li, Xin Chen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16753v1) | > Longitudinal information in radiology reports refers to the sequential tracking of findings across multiple examinations over time, which is crucial for monitoring disease progression and guiding clinical decisions. Many recent automated radiology report generation methods are designed to capture longitudinal information; however, validating their performance is challenging. There is no proper too...

---

## 790. From Clicks to Consensus: Collective Consent Assemblies for Data Governance

**Authors:** Lin Kyi, Paul Gölz, Robin Berjon, Asia Biega

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16752v1) | > Obtaining meaningful and informed consent from users is essential for ensuring they maintain autonomy and control over their data. Notice and consent, the standard for collecting consent online, has been criticized. While other individualized solutions have been proposed, this paper argues that a collective approach to consent is worth exploring for several reasons. First, the data of different us...

---

## 791. Optimal Control of Hydro-Electric Power Plants with Uncontrolled Spillways

**Authors:** Maria do Rosario de Pinho, Maria Margarida A. Ferreira, Georgi Smirnov

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16748v1) | > In this paper, we study an optimal control problem for a cascade of hydroelectric power plants with reversible turbines and uncontrolled spillways. The system dynamics are governed by a linear control model subject to path constraints. The aim is to maximize the power production profit while respecting operational restrictions on reservoir water levels. The challenge is the presence of uncontrolla...

---

## 792. SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents

**Authors:** Yuhang Wang, Yuling Shi, Mo Yang, Rongrui Zhang, Shilin He

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16746v1) | > LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a ...

---

## 793. The Pauli-Villars-regularized Dirac vacuum in electromagnetic fields at positive temperature

**Authors:** William Borrelli, Umberto Morellini

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16743v1) | > In this paper we consider a model of the Dirac vacuum in classical electromagnetic fields at positive temperature. We adopt the Pauli-Villars regularisation technique in order to properly define the free energy of the vacuum, extending the previous work by the second named author on the purely magnetic case. This work is intended as a first step in understanding polarisation effects in the vacuum ...

---

## 794. Negative Pressure and Cavitation Dynamics in Plant-like Structures

**Authors:** Olivier Vincent

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16741v1) | [DOI](https://doi.org/10.1039/9781839161162-00119)

> It is well known that a solid (e.g. wood or rubber) can be put under tensile stress by pulling on it. Once a critical stress is overcome, the solid breaks, leaving an empty space. Similarly, due to internal cohesion, a liquid can withstand tension (i.e. negative pressure), up to a critical point where a large bubble spontaneously forms, releasing the tension and leaving a void (the bubble). This p...

---

## 795. Comments on "Challenges of cellwise outliers" by Jakob Raymaekers and Peter J. Rousseeuw

**Authors:** Claudio Agostinelli

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16739v1) | [DOI](https://doi.org/10.1016/j.ecosta.2024.02.003)

> The main aim of robust statistics is the development of methods able to cope with the presence of outliers. A new type of outliers, namely "cellwise", has garnered considerable attention. The state of the art for dealing with cellwise contamination in different models is presented in Raymaekers and Rousseeuw (2024). Outliers in time series can be treated as cellwise outliers, a further discussion ...

---

## 796. Fermi scale from quantum gravity scaling solution

**Authors:** Christof Wetterich

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16731v1) | > Fundamental scale invariance implies the scale invariant standard model.
  Both the Fermi scale and the Planck mass are given by fields, and their ratio is dictated by a dimensionless cosmon-Higgs coupling. For an ultraviolet fixed point of quantum gravity this coupling is an irrelevant parameter of the renormalization flow and becomes predictable. An analytic scaling solution for quantum gravity ...

---

## 797. A locking-free nodal-based polytopal method for linear elasticity

**Authors:** Jerome Droniou, Raman Kumar

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16728v1) | > This work presents a Discrete de Rham (DDR) numerical scheme for solving linear elasticity problems on general polyhedral meshes, with a focus on preventing volumetric locking in the quasi-incompressible regime. The method is formulated as a nodal-based approach using the lowest-order gradient space of the DDR complex, enriched with scalar face bubble degrees of freedom that effectively capture th...

---

## 798. Some Spatial Point Processes of Poisson Family

**Authors:** Pradeep Vishwakarma

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16726v1) | > Spatial Poisson point processes on finite-dimensional Euclidean space provide fundamental mathematical tools for modeling random spatial point patterns. In this paper, we introduce and analyze several Poisson-type spatial point processes. In particular, we propose and study a point process, namely, the generalized Poisson random field (GPRF), in which more than one point can be observed with posit...

---

## 799. LongCat-Flash-Thinking-2601 Technical Report

**Authors:** Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16725v1) | > We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, th...

---

## 800. Mitigating Bias in Automated Grading Systems for ESL Learners: A Contrastive Learning Approach

**Authors:** Kevin Fan, Eric Yun

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16724v1) | > As Automated Essay Scoring (AES) systems are increasingly used in high-stakes educational settings, concerns regarding algorithmic bias against English as a Second Language (ESL) learners have increased. Current Transformer-based regression models trained primarily on native-speaker corpora often learn spurious correlations between surface-level L2 linguistic features and essay quality. In this st...

---

## 801. Model Predictive Control for Coupled Adoption-Opinion Dynamics

**Authors:** Martina Alutto, Qiulin Xu, Fabrizio Dabbene, Hideaki Ishii, Chiara Ravazzi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16722v1) | > This paper investigates an optimal control problem for an adoption-opinion model that couples opinion dynamics with a compartmental adoption framework on a multilayer network to study the diffusion of sustainable behaviors. Adoption evolves through social contagion and perceived benefits, while opinions are shaped by social interactions and feedback from adoption levels. Individuals may also stop ...

---

## 802. Watching AI Think: User Perceptions of Visible Thinking in Chatbots

**Authors:** Samuel Rhys Cox, Jade Martin-Lise, Simo Hosio, Niels van Berkel

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16720v1) | > People increasingly turn to conversational agents such as ChatGPT to seek guidance for their personal problems. As these systems grow in capability, many now display elements of "thinking": short reflective statements that reveal a model's intentions or values before responding. While initially introduced to promote transparency, such visible thinking can also anthropomorphise the agent and shape ...

---

## 803. On a Coupled Adoption-Opinion Framework for Competing Innovations

**Authors:** Martina Alutto, Fabrizio Dabbene, Angela Fontan, Karl H. Johansson, Chiara Ravazzi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16719v1) | > In this paper, we propose a two-layer adoption-opinion model to study the diffusion of two competing technologies within a population whose opinions evolve under social influence and adoption-driven feedback. After adopting one technology, individuals may become dissatisfied and switch to the alternative. We prove the existence and uniqueness of the adoption-diffused equilibrium, showing that both...

---

## 804. Dynamic Expert-Guided Model Averaging for Causal Discovery

**Authors:** Adrick Tench, Thomas Demeester

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16715v1) | > Understanding causal relationships is critical for healthcare. Accurate causal models provide a means to enhance the interpretability of predictive models, and furthermore a basis for counterfactual and interventional reasoning and the estimation of treatment effects. However, would-be practitioners of causal discovery face a dizzying array of algorithms without a clear best choice. This abundance...

---

## 805. CER-HV: A CER-Based Human-in-the-Loop Framework for Cleaning Datasets Applied to Arabic-Script HTR

**Authors:** Sana Al-azzawi, Elisa Barney, Marcus Liwicki

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16713v1) | > Handwritten text recognition (HTR) for Arabic-script languages still lags behind Latin-script HTR, despite recent advances in model architectures, datasets, and benchmarks. We show that data quality is a significant limiting factor in many published datasets and propose CER-HV (CER-based Ranking with Human Verification) as a framework to detect and clean label errors. CER-HV combines a CER-based n...

---

## 806. A Feature Extraction Pipeline for Enhancing Lightweight Neural Networks in sEMG-based Joint Torque Estimation

**Authors:** Kartik Chari, Raid Dokhan, Anas Homsi, Niklas Kueper, Elsa Andrea Kirchner

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16712v1) | > Robot-assisted rehabilitation offers an effective approach, wherein exoskeletons adapt to users' needs and provide personalized assistance. However, to deliver such assistance, accurate prediction of the user's joint torques is essential. In this work, we propose a feature extraction pipeline using 8-channel surface electromyography (sEMG) signals to predict elbow and shoulder joint torques. For p...

---

## 807. Better Generalizing to Unseen Concepts: An Evaluation Framework and An LLM-Based Auto-Labeled Pipeline for Biomedical Concept Recognition

**Authors:** Shanshan Liu, Noriki Nishida, Fei Cheng, Narumi Tokunaga, Rumana Ferdous Munne

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16711v1) | > Generalization to unseen concepts is a central challenge due to the scarcity of human annotations in Mention-agnostic Biomedical Concept Recognition (MA-BCR). This work makes two key contributions to systematically address this issue. First, we propose an evaluation framework built on hierarchical concept indices and novel metrics to measure generalization. Second, we explore LLM-based Auto-Labele...

---

## 808. One H2 molecule per ten million H-atoms reveals sub-pc scale cold overdensities at z~4

**Authors:** P. Noterdaeme, S. Balashev, T. Berg, S. Cristiani, R. Cuellar

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16710v1) | > We present the detection and analysis of H2 absorption at z = 4.24 towards the bright quasar J0007-5705, observed with the Very Large Telescope as part of the ESPRESSO QUasar Absorption Line Survey (EQUALS). The high resolving power, R~120000, enables the identification of extremely weak H2 lines in several rotational levels at a total column density of N(H2)~2x10^14 cm^-2, among the lowest ever m...

---

## 809. Barotropic-Baroclinic Splitting for Multilayer Shallow Water Models with Exchanges

**Authors:** Nina Aguillon, Sophie Hörnschemeyer, Jacques Sainte-Marie

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16709v1) | > This work presents the numerical analysis of a barotropic-baroclinic splitting in a nonlinear multilayer framework with exchanges between the layers in terrain-following coordinates. The splitting is formulated as an exact operator splitting. The barotropic step handles free surface evolution and depth-averaged velocity via a well-balanced one-layer model, while the baroclinic step manages vertica...

---

## 810. Dirac-Bergmann algorithm and canonical quantization of $k$-essence cosmology

**Authors:** Andrés Lueiza, Andronikos Paliathanasis, Nikolaos Dimakis

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16703v1) | > We develop a general canonical quantization scheme for $k$-essence cosmology in scalar-tensor theory. Utilizing the Dirac-Bergmann algorithm, we construct the Hamiltonian associated with the cosmological field equations and identify the first- and second-class constraints. The introduction of appropriate canonically conjugate variables with respect to Dirac brackets, allows for the canonical quant...

---

## 811. Control of helix orientation in chiral magnets via lateral confinement

**Authors:** Maurice Colling, Mariia Stepanova, Mario Hentschel, Somasree Bhattacharjee, Erik Lysne

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16704v1) | > Helimagnetic materials offer a versatile platform for spin-based device concepts owing to their long-range, tunable spiral order. Here, we demonstrate controlled manipulation of the helimagnetic propagation vector q by geometrical confinement, using FeGe as a model DMI-driven chiral magnet. Micromagnetic simulations based on the nonlinear sigma model reveal that open boundaries give rise to a chir...

---

## 812. Supporting Stakeholder Requirements Expression with LLM Revisions: An Empirical Evaluation

**Authors:** Michael Mircea, Emre Gevrek, Elisa Schmid, Kurt Schneider

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16699v1) | > Stakeholders often struggle to accurately express their requirements due to articulation barriers arising from limited domain knowledge or from cognitive constraints. This can cause misalignment between expressed and intended requirements, complicating elicitation and validation. Traditional elicitation techniques, such as interviews and follow-up sessions, are time-consuming and risk distorting s...

---

## 813. AI-enhanced discovery and accelerated synthesis of metal phosphosulfides

**Authors:** Javier Sanz Rodrigo, Nicholas A. Kryger-Nelson, Lena A. Mittmann, Eugène Bertin, Ivano E. Castelli

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16693v1) | > Metal phosphosulfides have emerged as unique multifunctional materials, but they present unique synthesis challenges compared to more established material classes such as oxides and nitrides. As a consequence, experimental development and theoretical understanding of phosphosulfides have focused on individual compounds rather than on accelerated broad-range exploration. In this work, we first eval...

---

## 814. Creating a biologically more accurate spider robot to study active vibration sensing

**Authors:** Siyuan Sun, Eugene H. Lin, Nathan Brown, Hsin-Yi Hung, Andrew Gordus

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16691v1) | > Orb-weaving spiders detect prey on a web using vibration sensors at leg joints. They often dynamically crouch their legs during prey sensing, likely an active sensing strategy. However, how leg crouching enhances sensing is poorly understood, because measuring system vibrations in behaving animals is difficult. We use robophysical modeling to study this problem. Our previous spider robot had only ...

---

## 815. Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation

**Authors:** Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16686v1) | > This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-ste...

---

## 816. Asymptotic testing of covariance separability for matrix elliptical data

**Authors:** Joni Virta, Takeru Matsuda

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16684v1) | > We propose a new asymptotic test for the separability of a covariance matrix. The null distribution is valid in wide matrix elliptical model that includes, in particular, both matrix Gaussian and matrix $t$-distribution. The test is fast to compute and makes no assumptions about the component covariance matrices. An alternative, Wald-type version of the test is also proposed. Our simulations revea...

---

## 817. AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning

**Authors:** Suzhong Fu, Jingqi Dong, Xuan Ding, Rui Sun, Yiming Yang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16685v1) | > Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoni...

---

## 818. Computation-Accuracy Trade-Off in Service-Oriented Model-Based Control

**Authors:** Hazem Ibrahim, Julius Beerwerth, Lorenz Dörschel, Bassam Alrifaee

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16682v1) | > Representing a control system as a Service-Oriented Architecture (SOA)-referred to as Service-Oriented Model-Based Control (SOMC)-enables runtime-flexible composition of control loop elements. This paper presents a framework that optimizes the computation-accuracy trade-off by formulating service orchestration as an A$^\star$search problem, complemented by Contextual Bayesian Optimization (BO) to ...

---

## 819. From Transactions to Exploits: Automated PoC Synthesis for Real-World DeFi Attacks

**Authors:** Xing Su, Hao Wu, Hanzhong Liang, Yunlin Jiang, Yuxi Cheng

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16681v1) | > Blockchain systems are increasingly targeted by on-chain attacks that exploit contract vulnerabilities to extract value rapidly and stealthily, making systematic analysis and reproduction highly challenging. In practice, reproducing such attacks requires manually crafting proofs-of-concept (PoCs), a labor-intensive process that demands substantial expertise and scales poorly. In this work, we pres...

---

## 820. Classical Regularization in Variational Quantum Eigensolvers

**Authors:** Yury Chernyak, Ijaz Ahamed Mohammad, Martin Plesch

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16679v1) | > While quantum computers are a very promising tool for the far future, in their current state of the art they remain limited both in size and quality. This has given rise to hybrid quantum-classical algorithms, where the quantum device performs only a small but vital part of the overall computation. Among these, variational quantum algorithms (VQAs), which combine a classical optimization procedure...

---

## 821. Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation

**Authors:** Lucía Güitta-López, Lionel Güitta-López, Jaime Boal, Álvaro Jesús López-López

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16677v1) | [DOI](https://doi.org/10.1016/j.engappai.2025.111510)

> The sample efficiency challenge in Deep Reinforcement Learning (DRL) compromises its industrial adoption due to the high cost and time demands of real-world training. Virtual environments offer a cost-effective alternative for training DRL agents, but the transfer of learned policies to real setups is hindered by the sim-to-real gap. Achieving zero-shot transfer, where agents perform directly in r...

---

## 822. I Guess That's Why They Call it the Blues: Causal Analysis for Audio Classifiers

**Authors:** David A. Kelly, Hana Chockler

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16675v1) | > It is well-known that audio classifiers often rely on non-musically relevant features and spurious correlations to classify audio. Hence audio classifiers are easy to manipulate or confuse, resulting in wrong classifications. While inducing a misclassification is not hard, until now the set of features that the classifiers rely on was not well understood.
  In this paper we introduce a new method ...

---

## 823. Charging of a Quantum Battery by a Single-Photon Quantum Pulse

**Authors:** Elnaz Darsheshdar, Seyed Mostafa Moniri

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16671v1) | > We study a minimal model for charging a quantum battery consisting of a two-level system (TLS) acting as a charger, coupled to a harmonic oscillator that serves as the quantum battery. A single-photon quantum pulse of light excites the TLS, which subsequently transfers its excitation to the isolated battery. The TLS may also decay into the electromagnetic environment. We obtain analytical solution...

---

## 824. The Green Side of the Lua

**Authors:** André Brandão, Diogo Matos, Miguel Guimarães, Simão Cunha, João Saraiva

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16670v1) | > The United Nations' 2030 Agenda for Sustainable Development highlights the importance of energy-efficient software to reduce the global carbon footprint. Programming languages and execution models strongly influence software energy consumption, with interpreted languages generally being less efficient than compiled ones. Lua illustrates this trade-off: despite its popularity, it is less energy-eff...

---

## 825. PLawBench: A Rubric-Based Benchmark for Evaluating LLMs in Real-World Legal Practice

**Authors:** Yuzhen Shi, Huanghai Liu, Yiran Hu, Gaojie Song, Xinran Xu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16669v1) | > As large language models (LLMs) are increasingly applied to legal domain-specific tasks, evaluating their ability to perform legal work in real-world settings has become essential. However, existing legal benchmarks rely on simplified and highly standardized tasks, failing to capture the ambiguity, complexity, and reasoning demands of real legal practice. Moreover, prior evaluations often adopt co...

---

## 826. Inference from high-frequency data: A subsampling approach

**Authors:** Kim Christensen, Mark Podolskij, Nopporn Thamrongrat, Bezirgen Veliyev

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16668v1) | [DOI](https://doi.org/10.1016/j.jeconom.2016.07.010)

> In this paper, we show how to estimate the asymptotic (conditional) covariance matrix, which appears in central limit theorems in high-frequency estimation of asset return volatility. We provide a recipe for the estimation of this matrix by subsampling; an approach that computes rescaled copies of the original statistic based on local stretches of high-frequency data, and then it studies the sampl...

---

## 827. ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance

**Authors:** Zhuohao Li, Yinghao Li, Jian-Jian Jiang, Lang Zhou, Tianyu Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16667v1) | > Vision-Language-Action (VLA) models have advanced robotic manipulation by combining vision, language, and proprioception to predict actions. However, previous methods fuse proprioceptive signals directly with VLM-encoded vision-language features, resulting in state-dominant bias and false completions despite visible execution failures. We attribute this to modality imbalance, where policies over-r...

---

## 828. Efficient quantum machine learning with inverse-probability algebraic corrections

**Authors:** Jaemin Seo

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16665v1) | > Quantum neural networks (QNNs) provide expressive probabilistic models by leveraging quantum superposition and entanglement, yet their practical training remains challenging due to highly oscillatory loss landscapes and noise inherent to near-term quantum devices. Existing training approaches largely rely on gradient-based procedural optimization, which often suffers from slow convergence, sensiti...

---

## 829. OFDM-Based ISAC Imaging of Extended Targets via Inverse Virtual Aperture Processing

**Authors:** Michael Negosanti, Lorenzo Pucci, Andrea Giorgetti

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16664v1) | > This work investigates the performance of an integrated sensing and communication (ISAC) system exploiting inverse virtual aperture (IVA) for imaging moving extended targets in vehicular scenarios. A base station (BS) operates as a monostatic sensor using MIMO-OFDM waveforms. Echoes reflected by the target are processed through motion-compensation techniques to form an IVA range-Doppler (cross-ran...

---

## 830. A Categorical Approach to Semantic Interoperability across Building Lifecycle

**Authors:** Zoltan Nagy, Ryan Wisnesky, Kevin Carlson, Eswaran Subrahmanian, Gioele Zardini

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16663v1) | > Buildings generate heterogeneous data across their lifecycle, yet integrating these data remains a critical unsolved challenge. Despite three decades of standardization efforts, over 40 metadata schemas now span the building lifecycle, with fragmentation accelerating rather than resolving. Current approaches rely on point-to-point mappings that scale quadratically with the number of schemas, or un...

---

## 831. Low-Power On-Device Gesture Recognition with Einsum Networks

**Authors:** Sahar Golipoor, Lingyun Yao, Martin Andraud, Stephan Sigg

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16662v1) | > We design a gesture-recognition pipeline for networks of distributed, resource constrained devices utilising Einsum Networks. Einsum Networks are probabilistic circuits that feature a tractable inference, explainability, and energy efficiency. The system is validated in a scenario of low-power, body-worn, passive Radio Frequency Identification-based gesture recognition. Each constrained device inc...

---

## 832. Revisiting the Role of Natural Language Code Comments in Code Translation

**Authors:** Monika Gupta, Ajay Meena, Anamitra Roy Choudhury, Vijay Arya, Srikanta Bedathur

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16661v1) | > The advent of large language models (LLMs) has ushered in a new era in automated code translation across programming languages. Since most code-specific LLMs are pretrained on well-commented code from large repositories like GitHub, it is reasonable to hypothesize that natural language code comments could aid in improving translation quality. Despite their potential relevance, comments are largely...

---

## 833. Fast, faithful and photorealistic diffusion-based image super-resolution with enhanced Flow Map models

**Authors:** Maxence Noble, Gonzalo Iñaki Quintana, Benjamin Aubin, Clément Chadebec

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16660v1) | > Diffusion-based image super-resolution (SR) has recently attracted significant attention by leveraging the expressive power of large pre-trained text-to-image diffusion models (DMs). A central practical challenge is resolving the trade-off between reconstruction faithfulness and photorealism. To address inference efficiency, many recent works have explored knowledge distillation strategies specifi...

---

## 834. Provably Robust Bayesian Counterfactual Explanations under Model Changes

**Authors:** Jamie Duell, Xiuyi Fan

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16659v1) | > Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this paper, we introduce Probabilistically Safe CEs (PSCE), a method for generating counterfactual explanations that...

---

## 835. Generative Confidants: How do People Experience Trust in Emotional Support from Generative AI?

**Authors:** Riccardo Volpato, Simone Stumpf, Lisa DeBruine

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16656v1) | > People are increasingly turning to generative AI (e.g., ChatGPT, Gemini, Copilot) for emotional support and companionship. While trust is likely to play a central role in enabling these informal and unsupervised interactions, we still lack an understanding of how people develop and experience it in this context. Seeking to fill this gap, we recruited 24 frequent users of generative AI for emotiona...

---

## 836. Simulation of the carbon dioxide hydrate-water interfacial energy

**Authors:** Jesús Algabaa Esteban Acuña, José Manuel Míguez, Bruno Mendiboure, Iván M. Zerón, Felipe J. Blas

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16653v1) | [DOI](https://doi.org/10.1016/j.jcis.2022.05.029)

> Carbon dioxide hydrates are ice-like nonstoichiometric inclusion solid compounds with importance to global climate change, and gas transportation and storage. The thermodynamic and kinetic mechanisms that control carbon dioxide nucleation critically depend on hydrate-water interfacial free energy. Interfacial energies show large uncertainties due to the conditions at which experiments are performe...

---

## 837. Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training

**Authors:** Aurora Pia Ghiardelli, Guangzhi Tang, Tao Sun

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16652v1) | > We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Throug...

---

## 838. Select or Project? Evaluating Lower-dimensional Vectors for LLM Training Data Explanations

**Authors:** Lukas Hinterleitner, Loris Schoenegger, Benjamin Roth

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16651v1) | > Gradient-based methods for instance-based explanation for large language models (LLMs) are hindered by the immense dimensionality of model gradients. In practice, influence estimation is restricted to a subset of model parameters to make computation tractable, but this subset is often chosen ad hoc and rarely justified by systematic evaluation. This paper investigates if it is better to create low...

---

## 839. LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents

**Authors:** Amin Rakhsha, Thomas Hehn, Pietro Mazzaglia, Fabio Valerio Massoli, Arash Behboodi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16649v1) | > Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framewo...

---

## 840. Flavour-Changing Neutral Current Top Decays in the Three Higgs Doublet Model

**Authors:** Baradhwaj Coleppa, Benjamin Fuks, Akshat Khanna

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16647v1) | > We study flavour-changing neutral current decays of the top quark in the democratic Three Higgs Doublet Model featuring a $Z_3$-symmetric scalar potential and Natural Flavour Conservation. In this framework, while such processes are absent at tree-level, the extended scalar sector induces new one-loop contributions to rare top decays. We compute the branching ratios for processes of the form $t \t...

---

## 841. Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss

**Authors:** Minsu Gong, Nuri Ryu, Jungseul Ok, Sunghyun Cho

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16645v1) | > Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL...

---

## 842. Cosmological analysis of a viable $f(R)$ gravity model

**Authors:** Siqi He, Weiqiang Yang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16641v1) | > Since viable $f(R)$ gravity models must reconcile early-universe inflation with late-time acceleration, we specifically study the dynamical behavior of such a theory during the matter-dominated to dark-energy-dominated transition epoch. By using $y_{H}(z)$ versus $z$ and the Hubble parameter, we solved the field equations. After appropriately choosing appropriate parameter values , we plotted a se...

---

## 843. HapticMatch: An Exploration for Generative Material Haptic Simulation and Interaction

**Authors:** Mingxin Zhang, Yu Yao, Yasutoshi Makino, Hiroyuki Shinoda, Masashi Sugiyama

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16639v1) | > High-fidelity haptic feedback is essential for immersive virtual environments, yet authoring realistic tactile textures remains a significant bottleneck for designers. We introduce HapticMatch, a visual-to-tactile generation framework designed to democratize haptic content creation. We present a novel dataset containing precisely aligned pairs of micro-scale optical images, surface height maps, an...

---

## 844. A Unified Calibration Framework for High-Accuracy Articulated Robot Kinematics

**Authors:** Philip Tobuschat, Simon Duenser, Markus Bambach, Ivo Aschwanden

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16638v1) | > Researchers have identified various sources of tool positioning errors for articulated industrial robots and have proposed dedicated compensation strategies. However, these typically require individual, specialized experiments with separate models and identification procedures. This article presents a unified approach to the static calibration of industrial robots that identifies a robot model, in...

---

## 845. Conformal prediction for full and sparse polynomial chaos expansions

**Authors:** A. Hatstatt, X. Zhu, B. Sudret

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16636v1) | > Polynomial Chaos Expansions (PCEs) are widely recognized for their efficient computational performance in surrogate modeling. Yet, a robust framework to quantify local model errors is still lacking. While the local uncertainty of PCE prediction can be captured using bootstrap resampling, other methods offering more rigorous statistical guarantees are needed, especially in the context of small trai...

---

## 846. Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices

**Authors:** Julian Legler

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16635v1) | > Recent advancements enable fine-grained energy measurements in cloud-native environments (e.g., at container or process level) beyond traditional coarse-grained scopes. However, service-level energy measurement for microservice-based applications remains underexplored. Such measurements must include compute, network, and storage energy to avoid underestimating consumption in distributed setups. We...

---

## 847. Current-induced magnetization control in dipolar-coupled nanomagnet pairs and artificial spin ice

**Authors:** A. Pac, G. M. Macauley, J. A. Brock, A. Hrabec, A. Kurenkov

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16633v1) | > Exploiting current-induced spin-orbit torques (SOTs) to manipulate the magnetic state of dipolar-coupled nanomagnet systems with in-plane magnetic anisotropy, such as artificial spin ices, provides a route to local, electrically-programmable control of the magnetization, with relevance for applications including neuromorphic computing. Here, we demonstrate how the orientation of a nanomagnet relat...

---

## 848. Dual-Prototype Disentanglement: A Context-Aware Enhancement Framework for Time Series Forecasting

**Authors:** Haonan Yang, Jianchao Tang, Zhuo Li

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16632v1) | > Time series forecasting has witnessed significant progress with deep learning. While prevailing approaches enhance forecasting performance by modifying architectures or introducing novel enhancement strategies, they often fail to dynamically disentangle and leverage the complex, intertwined temporal patterns inherent in time series, thus resulting in the learning of static, averaged representation...

---

## 849. PanopMamba: Vision State Space Modeling for Nuclei Panoptic Segmentation

**Authors:** Ming Kang, Fung Fung Ting, Raphaël C. -W. Phan, Zongyuan Ge, Chee-Ming Ting

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16631v1) | > Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images. Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance. To address these issues, we propose PanopMamba, a novel hybrid enco...

---

## 850. $d$-wave FFLO state and charge-2e supersolidity in the $t$-$t'$-$J$ model under Zeeman fields

**Authors:** Xing-Zhou Qu, Dai-Wei Qu, Qiaoyi Li, Wei Li, Gang Su

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16630v1) | > Unconventional superconductivity under strong Zeeman fields--particularly beyond the Pauli paramagnetic limit--remains a central challenge in condensed matter physics. The exotic Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) state, in particular, remains in need of definitive study within fundamental electronic models. Here we employ state-of-the-art finite-temperature and ground-state tensor network ap...

---

## 851. Typologically Informed Parameter Aggregation

**Authors:** Stef Accou, Wessel Poelman

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16629v1) | > Massively multilingual language models enable cross-lingual generalization but underperform on low-resource and unseen languages. While adapter-based fine-tuning offers a parameter-efficient solution, training language-specific adapters at scale remains costly. We introduce Typologically Informed Parameter Aggregation (TIPA), a training-free method that constructs proxy language adapters by aggreg...

---

## 852. SCHIGAND: A Synthetic Facial Generation Mode Pipeline

**Authors:** Ananya Kadali, Sunnie Jehan-Morrison, Orasiki Wellington, Barney Evans, Precious Durojaiye

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16627v1) | > The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generatio...

---

## 853. MultiLexNorm++: A Unified Benchmark and a Generative Model for Lexical Normalization for Asian Languages

**Authors:** Weerayut Buaphet, Thanh-Nhi Nguyen, Risa Kondo, Tomoyuki Kajiwara, Yumin Kim

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16623v1) | > Social media data has been of interest to Natural Language Processing (NLP) practitioners for over a decade, because of its richness in information, but also challenges for automatic processing. Since language use is more informal, spontaneous, and adheres to many different sociolects, the performance of NLP models often deteriorates. One solution to this problem is to transform data to a standard...

---

## 854. E2Former-V2: On-the-Fly Equivariant Attention with Linear Activation Memory

**Authors:** Lin Huang, Chengxiang Huang, Ziang Wang, Yiyue Du, Chu Wang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16622v1) | > Equivariant Graph Neural Networks (EGNNs) have become a widely used approach for modeling 3D atomistic systems. However, mainstream architectures face critical scalability bottlenecks due to the explicit construction of geometric features or dense tensor products on \textit{every} edge. To overcome this, we introduce \textbf{E2Former-V2}, a scalable architecture that integrates algebraic sparsity ...

---

## 855. How Does Personalized Memory Shape LLM Behavior? Benchmarking Rational Preference Utilization in Personalized Assistants

**Authors:** Xueyang Feng, Weinan Gan, Xu Chen, Quanyu Dai, Yong Liu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16621v1) | > Large language model (LLM)-powered assistants have recently integrated memory mechanisms that record user preferences, leading to more personalized and user-aligned responses. However, irrelevant personalized memories are often introduced into the context, interfering with the LLM's intent understanding. To comprehensively investigate the dual effects of personalization, we develop RPEval, a bench...

---

## 856. PROST-LLM: Progressively Enhancing the Speech-to-Speech Translation Capability in LLMs

**Authors:** Jing Xu, Jiaqi Wang, Daxin Tan, Xiao Chen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16618v1) | > Although Large Language Models (LLMs) excel in many tasks, their application to Speech-to-Speech Translation (S2ST) is underexplored and hindered by data scarcity. To bridge this gap, we propose PROST-LLM (PROgressive Speech-to-speech Translation) to enhance the S2ST capabilities in LLMs progressively. First, we fine-tune the LLMs with the CVSS corpus, employing designed tri-task learning and chai...

---

## 857. Feller Property and Absorption of Diffusions for Multi-Species Metacommunities

**Authors:** Benoît Henry, Céline Wang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16616v1) | > We consider individuals of two species distributed over m patches, each with a hosting capacity \(d_i N\) , where \(d_i \in (0, 1]\). We assume that all the patches are linked by the dispersal of individuals. This work examines how the metacommunity evolves in these patches. The model incorporates Wright-Fisher intra-patch reproduction and a general exchange function representing dispersal. Under ...

---

## 858. AuroraEdge-V-2B: A Faster And Stronger Edge Visual Large Language Model

**Authors:** Xiang Chen

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16615v1) | > Recently, due to the advancement of multimodal technology, people are attempting to use visual large language models (VLLMs) in industrial production. Many deep learning models (DLMs) deployed in the production environment are gradually being replaced by VLLMs. Compared with DLMs, VLLMs have some advantages in industrial applications: (1) Their strong generalization ability enables them to perform...

---

## 859. Is the diurnal pattern sufficient to explain intraday variation in volatility? A nonparametric assessment

**Authors:** Kim Christensen, Ulrich Hounyo, Mark Podolskij

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16613v1) | [DOI](https://doi.org/10.1016/j.jeconom.2018.03.016)

> In this paper, we propose a nonparametric way to test the hypothesis that time-variation in intraday volatility is caused solely by a deterministic and recurrent diurnal pattern. We assume that noisy high-frequency data from a discretely sampled jump-diffusion process are available. The test is then based on asset returns, which are deflated by the seasonal component and therefore homoskedastic un...

---

## 860. Challenges in the Proper Metrological Verification of Smart Energy Meters

**Authors:** Antonio Bracale, Jakub Janowicz, Piotr Kuwałek, Grzegorz Wiczyński

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16612v1) | > The most common instruments currently measuring active/reactive energy and power quality indicators are smart energy meters. Unfortunately, the verification of such meters is currently performed under ideal conditions or with simple signal models, which do not recreate actual states occurring in the power grid and do not ensure the verification of the properties of their signal chains. This paper ...

---

## 861. A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling

**Authors:** Jingsong Xia, Siqi Wang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16608v1) | > Intelligent medical image analysis is essential for clinical decision support but is often limited by scarce annotations, constrained computational resources, and suboptimal model generalization. To address these challenges, we propose a lightweight medical image classification framework that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. MobileNetV2 is emp...

---

## 862. Omni-directional attention mechanism based on Mamba for speech separation

**Authors:** Ke Xue, Chang Sun, Rongfei Fan, Jing Wang, Han Hu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16603v1) | > Mamba, a selective state-space model (SSM), has emerged as an efficient alternative to Transformers for speech modeling, enabling long-sequence processing with linear complexity. While effective in speech separation, existing approaches, whether in the time or time-frequency domain, typically decompose the input along a single dimension into short one-dimensional sequences before processing them w...

---

## 863. Unsupervised Super-Resolution of Hyperspectral Remote Sensing Images Using Fully Synthetic Training

**Authors:** Xinxin Xu, Yann Gousseau, Christophe Kervazo, Saïd Ladjal

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16602v1) | > Considerable work has been dedicated to hyperspectral single image super-resolution to improve the spatial resolution of hyperspectral images and fully exploit their potential. However, most of these methods are supervised and require some data with ground truth for training, which is often non-available. To overcome this problem, we propose a new unsupervised training strategy for the super-resol...

---

## 864. Nuclear molecule of heavy nuclei

**Authors:** T. M. Shneidman, R. G. Nazmitdinov

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16600v1) | > The model of a nuclear molecule that composed of two heavy nuclei is proposed. To this aim the Hamiltonian of a dinuclear system is derived and diagonalized in the basis of bipolar spherical functions. Analytical expressions, describing excitations of highly deformed states of a nuclear molecule, are obtained. A remarkable agreement between numerical and analytical results is demonstrated at the d...

---

## 865. Attention-MoA: Enhancing Mixture-of-Agents via Inter-Agent Semantic Attention and Deep Residual Synthesis

**Authors:** Jianyu Wen, Yang Wei, Xiongxi Yu, Changxuan Xiao, Ke Zeng

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16596v1) | > As the development of Large Language Models (LLMs) shifts from parameter scaling to inference-time collaboration, the Mixture-of-Agents (MoA) framework has emerged as a general paradigm to harness collective intelligence by layering diverse models. While recent MoA variants have introduced dynamic routing and residual connections to improve efficiency, these methods often fail to facilitate deep s...

---

## 866. Variational approximate penalized credible regions for Bayesian grouped regression

**Authors:** Weichang Yu, Khue-Dung Dang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16585v1) | > We develop a fast and accurate grouped penalized credible region approach for variable selection and prediction in Bayesian high-dimensional linear regression. Most existing Bayesian methods either are subject to high computational costs due to long Markov Chain Monte Carlo runs or yield ambiguous variable selection results due to non-sparse solution output. The penalized credible region framework...

---

## 867. Numerical investigation of unsteady flow in a reversible pump-turbine

**Authors:** Chirag Trivedi

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16584v1) | > Hydropower is an important source of renewable energy that provides clean energy. Pump-turbine type hydraulic turbine is widely used to mitigate the intermittent energy demand and store a large-scale energy. Pump-turbine operates in reverse mode in pump mode to store energy. Flow conditions in turbine mode and pump mode operations is substantially different. This study investigates the unsteady fl...

---

## 868. X-Aligner: Composed Visual Retrieval without the Bells and Whistles

**Authors:** Yuqian Zheng, Mariana-Iuliana Georgescu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16582v1) | > Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a nove...

---

## 869. Necessary Optimality Conditions for Integrated Learning and Optimization Problem in Contextual Optimization

**Authors:** Yuan Tao, Huifu Xu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16581v1) | > Integrated learning and optimization (ILO) is a framework in contextual optimization which aims to train a predictive model for the probability distribution of the underlying problem data uncertainty, with the goal of enhancing the quality of downstream decisions. This framework represents a new class of stochastic bilevel programs, which are extensively utilized in the literature of operations re...

---

## 870. Solving Regularized Multifacility Location Problems with Unknown Number of Centers via Difference-of-Convex Optimization

**Authors:** W. Geremew, V. S. T. Long, N. M. Nam, A. Solano-Herrera

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16576v1) | > In this paper, we develop optimization methods for a new model of multifacility location problems defined by a Minkowski gauge with Laplace-type regularization terms. The model is analyzed from both theoretical and numerical perspectives. In particular, we establish the existence of optimal solutions and study qualitative properties of global minimizers. By combining Nesterov's smoothing technique...

---

## 871. The 2026 Skyrmionics Roadmap

**Authors:** Sabri Koraltan, Claas Abert, Manfred Albrecht, Maria Azhar, Christian Back

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16575v1) | > Magnetic skyrmions and related topological spin textures have emerged as a central topic in condensed-matter physics, combining fundamental significance with potential for transformative applications in spintronics, magnonics, and beyond. Over the past decade, advances in material platforms, imaging techniques, theoretical modeling, and device concepts have established skyrmionics as a rapidly exp...

---

## 872. Gravitational Lensing Effect from The Revised Deser-Woodard Nonlocal Gravity

**Authors:** Haida Li, Xiangdong Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16572v1) | > We investigate the gravitational lensing effects of a static spherically symmetric black hole (BH) within the framework of the revised Deser-Woodard (D-W) nonlocal gravity. By analyzing the deflection angle in both the weak and strong field limits, we derive several distinguishing features of the model. In the weak field limit, we report a leading-order correction to the deflection angle directly ...

---

## 873. Description of Charged\text{-}Particle Multiplicity Distributions in High\text{-}Energy Proton\text{-}Proton Collisions Based on a Two-Component Model and Examination of Parton Distribution Functions

**Authors:** Zhixiang Yang, Jianhong Ruan

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16569v1) | > High-energy proton-proton collisions at the LHC offer a stringent test of Quantum Chromodynamics (QCD) in the small-$x$, gluon-dominated regime. This study focus on a minimal, gluon-driven framework to describe the charged-particle multiplicities and their pseudorapidity densities in high energy collisions. The two-component model taken here includes the hard gluon-gluon fusion process and the sof...

---

## 874. Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach

**Authors:** Abdurahman Maarouf, Alket Bakiaj, Stefan Feuerriegel

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16568v1) | > Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely o...

---

## 875. Thick Lunar Crust Amplifies Gravitational-Wave Signal

**Authors:** Lei Zhang, Han Yan, Xian Chen, Jinhai Zhang

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16567v1) | > Gravitational waves (GWs) in the $10^{-3}-0.1$ Hz band encode unique signatures of the early universe and merging compact objects, but they are beyond the reach of existing observatories. Theoretical models suggest that the Moon could act as a resonant detector, but the unknown influence of its rugged surface and heterogeneous interior has cast doubt on this prospect. Here, we resolve this long-st...

---

## 876. Simulations of multi-phase gas in and around galaxies

**Authors:** Max Gronke, Evan Schneider

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16566v1) | > Multiphase gas -- ranging from cold molecular clouds ($\lesssim 100\,$K) to hot, diffuse plasma ($\gtrsim 10^6\,$K) is a defining feature of the interstellar, circumgalactic, intracluster, and intergalactic media. Accurately simulating its dynamics is critical to improving our understanding of galaxy formation and evolution, however, due to their multi-scale and multi-physics nature, multiphase sy...

---

## 877. Process-Tensor Tomography of SGD: Measuring Non-Markovian Memory via Back-Flow of Distinguishability

**Authors:** Vasileios Sevetlidis, George Pavlidis

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16563v1) | > This work proposes neural training as a \emph{process tensor}: a multi-time map that takes a sequence of controllable instruments (batch choices, augmentations, optimizer micro-steps) and returns an observable of the trained model. Building on this operational lens, we introduce a simple, model-agnostic witness of training memory based on \emph{back-flow of distinguishability}. In a controlled two...

---

## 878. Markov Stick-breaking Processes

**Authors:** María F. Gil-Leyva, Antonio Lijoi, Ramsés H. Mena, Igor Prünster

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16561v1) | > Stick-breaking has a long history and is one of the most popular procedures for constructing random discrete distributions in Statistics and Machine Learning. In particular, due to their intuitive construction and computational tractability they are ubiquitous in modern Bayesian nonparametric inference. Most widely used models, such as the Dirichlet and the Pitman-Yor processes, rely on iid or ind...

---

## 879. Predicting Networks Before They Happen: Experimentation on a Real-Time V2X Digital Twin

**Authors:** Roberto Pegurri, Habu Shintaro, Francesco Linsalata, Wang Kui, Tao Yu

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16559v1) | > Emerging safety-critical Vehicle-to-Everything (V2X) applications require networks to proactively adapt to rapid environmental changes rather than merely reacting to them. While Network Digital Twins (NDTs) offer a pathway to such predictive capabilities, existing solutions typically struggle to reconcile high-fidelity physical modeling with strict real-time constraints. This paper presents a nove...

---

## 880. Multi-wavelength Study of A Superflare on RS CVn-type Star HD22468 Triggered at Hard X-ray by SVOM

**Authors:** J. Wang, W. J. Xie, F. Cangemi, A. Coleiro, H. L. Li

**Year:** 2026 | **Venue:** arXiv | **Citations:** N/A | **Score:** 0.000

[PDF](https://arxiv.org/pdf/2601.16558v1) | > Detection of stellar flares at hard X-ray is still rare at the current stage. A transient was recently detected by the hard X-ray camera, ECLAIRs onboard the SVOM mission at 11:39:01.2UT on 2025, January 09. Simultaneous monitor in the optical band on the ground by SVOM/GWAC and follow-up spectroscopy enable us to confirm that the transient is caused by a superflare on HD~22468, a RS CVn-type star...

---

